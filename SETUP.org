#+TITLE: Reversible Meta-Synthesis Setup
#+AUTHOR: aygp-dr
#+DATE: 2025-03-29
#+PROPERTY: header-args :mkdirp yes

* Introduction

This repository implements the concepts from "Inductive Program Synthesis by Using a Reversible Meta-Interpreter" by Masayuki Numao and Masamichi Shimura (1990). The paper describes a method that combines analytical and empirical learning to accelerate program synthesis through explanation-based learning of similar programs.

Three implementations are provided:
1. Prolog - The original language used in the paper
2. Hy - A Lisp dialect embedded in Python
3. Scheme - A minimalist Lisp dialect

* Directory Structure

#+begin_src shell :tangle no
mkdir -p src/{prolog,hy,scheme}
mkdir -p examples/{prolog,hy,scheme}
mkdir -p tests/{prolog,hy,scheme}
mkdir -p doc
#+end_src

* Prolog Implementation

This is the primary implementation, closely following the original paper.

** Reversible Meta-Interpreter

The core of the paper is the reversible Prolog interpreter written in Prolog itself. This interpreter can both execute programs and synthesize them from examples.

#+begin_src prolog :tangle src/prolog/reversible_interpreter.pl
:- op(250, fx, '*'). % * is prefixed to variables.
:- op(250, fx, '/'). % / is prefixed to atoms.

% prolog(Clauses, Queries, Value): Answer Queries one by one by calling goalseq.
prolog(_, [], []).
prolog(Clauses, [Q | Qs], [NewEnv | RestEnv]) :-
    goalseq(Q, [], NewEnv, Clauses),
    prolog(Clauses, Qs, RestEnv).

% goalseq(Query, OldEnv, NewEnv, Clauses):
% Answer a Query, which is a sequence of goals.
% Value of variables are retained in a difference-list between NewEnv and OldEnv.
goalseq([], Env, Env, _Clauses).
goalseq([Goal | GRest], Env, NewEnv, Clauses) :-
    head(Goal, GG, Env, E),
    goal(GG, Clauses),
    goalseq(GRest, E, NewEnv, Clauses).

% goal(Goal, Clauses): Answer a Goal.
goal(Goal, Clauses) :-
    search(Clauses, (Head :- Body)),
    head(Head, Goal, [], NextEnv),
    goalseq(Body, NextEnv, _, Clauses).

% search(Clauses, Clause): Search a Clause in the data base Clauses.
search([Clause | _], Clause).
search([_ | Clauses], C) :- search(Clauses, C).

% head(Goal with variables, Goal without variables, OldEnv, NewEnv):
% Put/Get value of variables in a Goal.
head([Pred | XA], [Pred | YA], OEnv, NEnv) :-
    head1(XA, YA, OEnv, NEnv).

head1([X | XRest], [Y | YRest], OEnv, NEnv) :-
    bind(X, Y, OEnv, Env),
    head1(XRest, YRest, Env, NEnv).
head1([], [], Env, Env).

% bind(Term with variables, Term without variables, OldEnv, NewEnv):
% Put/Get value of variables in a Term.
bind(/X, /X, Env, Env).
bind([], [], Env, Env).
bind(*Var, Val, Env, NewEnv) :- fetch(*Var, Val, Env, NewEnv).
bind([X|XRest], [Val|ValRest], Env, NewEnv) :-
    bind(X, Val, Env, New1),
    bind(XRest, ValRest, New1, NewEnv).

% fetch(Variable, Value, OldEnv, NewEnv): Put/Get Value of a Variable.
fetch(Var, Val, Env, NewEnv) :- fetch1(Var, Val, Env, Env, NewEnv).

fetch1(Var, Val, [], Env, [(Var, Val) | Env]).
fetch1(Var, Val, [(Var, Val) | _Rest], Env, Env).
fetch1(Var1, Val, [(Var2, _) | EnvRest], Env, NewEnv) :-
    Var1 \== Var2,
    fetch1(Var1, Val, EnvRest, Env, NewEnv).
#+end_src

** Explanation-Based Learning

The paper uses Explanation-Based Learning (EBG) to accelerate program synthesis. Here's an implementation of the modified EBG algorithm described in the paper.

#+begin_src prolog :tangle src/prolog/ebg.pl
% EBG for learning generalizations from examples
ebg(Goal, Head, Body) :-
    functor(Goal, F, N),
    functor(Head, F, N),
    ebg1(Goal, Head, Body, []).

ebg1(A, C, [C|G], G) :- operational(A), !, A.
ebg1((A,B), (GenA,GenB), NG, OG) :-
    ebg1(A, GenA, NG, G),
    ebg1(B, GenB, G, OG).
ebg1(A, GenA, NG, OG) :-
    clause(GenA, GenB),
    copy((GenA :- GenB), (A :- B)),
    ebg1(B, GenB, NG, OG).
ebg1(true, _, G, G).

% Built-in predicates are operational
operational(\==(_,_)).

% Utility for copying term structure with new variables
copy(Old, New) :-
    assert('$marker'(Old)),
    retract('$marker'(NN)),
    New = NN.
#+end_src

** Composability

The paper introduces decomposition of explanations based on composability for more flexible program synthesis.

#+begin_src prolog :tangle src/prolog/composability.pl
% Composability definitions from the paper
composability(prolog(_,_,_), 1).
composability(goalseq(_,_,_,_), 3).
composability(goal(_,_), 1).
composability(search(_,_), 1).
composability(head(_,_,_,_), 4).
composability(head1(_,_,_,_), 4).
composability(bind(_,_,_,_), 4).
composability(fetch(_,_,_,_), 2).
composability(fetch1(_,_,_,_,_), 3).
#+end_src

** Executable Explanation

The executable explanation mechanism for decomposing and re-combining explanations.

#+begin_src prolog :tangle src/prolog/executable_explanation.pl
% Representation of an explanation in Prolog clauses
% Each etree/2 predicate represents a node in the explanation tree

% Example of an executable explanation for the append program
etree(1, append([], L, L)).
etree(2, append([H|T1], L2, [H|T3])) :- etree(3, append(T1, L2, T3)).

% Cut and paste operation for explanations based on composability
etree(Id, Goal) :-
    composability(Goal, C),
    DECOMP_FORCE >= C,  % Global variable to control decomposition depth
    clause(etree(SId, Goal), Body),
    nonvar(SId),
    Id \== SId,
    Body.
#+end_src

** Examples from the Paper

Here are some examples from the paper implemented in Prolog.

#+begin_src prolog :tangle examples/prolog/append_example.pl
:- consult('../../src/prolog/reversible_interpreter.pl').

% Example of using the reversible interpreter to run append
example_run_append :-
    prolog([([append, [], *l, *l] :- []),
           ([append, [*x | *l1], *l2, [*x|*l3]] :- [[append, *l1, *l2, *l3]])],
           [[[append, [/x], [/y], *ans]]],
           Value),
    write('Value: '), write(Value), nl.

% Example of program synthesis for append
example_synthesize_append :-
    prolog(Clauses,
           [[[append, [/a], [/b], [/a, /b]]]],
           [[]]),
    write('Synthesized Clauses: '), write(Clauses), nl.
#+end_src

#+begin_src prolog :tangle examples/prolog/app3_merge3_example.pl
:- consult('../../src/prolog/reversible_interpreter.pl').
:- consult('../../src/prolog/ebg.pl').
:- consult('../../src/prolog/composability.pl').
:- consult('../../src/prolog/executable_explanation.pl').

% Precedent program: app3
example_app3 :-
    prolog([([append, [], *x, *x] :- []),
            ([append, [*x|*l1], *l2, [*x|*l3]] :- [[append, *l1, *l2, *l3]]),
            ([app3, *x, *y, *z, *a] :- [[append, *x, *y, *aa], [append, *aa, *z, *a]])],
            [[[app3, [/a], [/b], [/c], [/a, /b, /c]]]],
            Value),
    write('Value: '), write(Value), nl.

% Synthesis of merge3 using clause-level chunks
example_synthesize_merge3 :-
    % Set global variable to decompose at clause level
    nb_setval(DECOMP_FORCE, 1),
    
    % First create explanation from app3
    prolog([([append, [], *x, *x] :- []),
            ([append, [*x|*l1], *l2, [*x|*l3]] :- [[append, *l1, *l2, *l3]]),
            ([app3, *x, *y, *z, *a] :- [[append, *x, *y, *aa], [append, *aa, *z, *a]])],
            [[[app3, [/a], [/b], [/c], [/a, /b, /c]]]],
            _),
    
    % Then synthesize merge3 using the explanation
    etree(1,
          prolog([([merge, [], *x, *x] :- []),
                  ([merge, *x, [], *x] :- []),
                  ([merge, [*x|*l1], [*y|*l2], [*x|*l3]] :- [[=<, *x, *y], [merge, *l1, [*y|*l2], *l3]]),
                  ([merge, [*x|*l1], [*y|*l2], [*y|*l3]] :- [[>, *x, *y], [merge, [*x|*l1], *l2, *l3]])
                  | Clauses],
                 [[[merge3, [/3], [/1], [/2], [/1, /2, /3]]]],
                 [[]])),
    write('Synthesized merge3 Clauses: '), write(Clauses), nl.
#+end_src

#+begin_src prolog :tangle examples/prolog/zip_rzip_example.pl
:- consult('../../src/prolog/reversible_interpreter.pl').
:- consult('../../src/prolog/ebg.pl').
:- consult('../../src/prolog/composability.pl').
:- consult('../../src/prolog/executable_explanation.pl').

% Precedent program: zip
example_zip :-
    prolog([([zip, [], *l, *l] :- []),
            ([zip, [*x | *l1], [*y | *l2], [*x, *y | *l3]] :- [[zip, *l1, *l2, *l3]])],
            [[[zip, [/x, /y], [/z, /u], *ans]]],
            Value),
    write('Value: '), write(Value), nl.

% Synthesis of rzip using clause-structure-level chunks
example_synthesize_rzip :-
    % Set global variable to decompose at clause-structure level
    nb_setval(DECOMP_FORCE, 2),
    
    % First create explanation from zip
    prolog([([zip, [], *l, *l] :- []),
            ([zip, [*x | *l1], [*y | *l2], [*x, *y | *l3]] :- [[zip, *l1, *l2, *l3]])],
            [[[zip, [/x, /y], [/z, /u], *ans]]],
            _),
    
    % Then synthesize rzip using the explanation
    etree(1,
          prolog(Clauses,
                 [[[rzip, [/1, /2], [/3, /4], [/3, /1, /4, /2]]]],
                 [[]])),
    write('Synthesized rzip Clauses: '), write(Clauses), nl.
#+end_src

* Hy Implementation

Hy is a Lisp dialect that runs on Python, allowing us to leverage Python's ecosystem while maintaining a Lisp syntax.

** Reversible Meta-Interpreter

#+begin_src hy :tangle src/hy/reversible_interpreter.hy
#!/usr/bin/env hy
(import [collections.abc [Mapping Sequence]])

(defclass Env []
  (defn __init__ [self &optional [bindings {}]]
    (setv self.bindings bindings))
    
  (defn lookup [self var]
    (get self.bindings var None))
    
  (defn extend [self var val]
    (Env (| self.bindings {var val}))))

(defn atom? [x]
  (not (isinstance x Sequence)))

(defn variable? [x]
  (and (isinstance x str) (.startswith x "*")))

(defn constant? [x]
  (and (isinstance x str) (.startswith x "/")))

(defn real-value [x]
  (if (constant? x)
      (cut x 1)
      x))

(defclass Interpreter []
  (defn __init__ [self]
    (setv self.clauses []))
    
  (defn add-clause [self head body]
    (.append self.clauses [head body]))
    
  (defn find-matching-clauses [self goal]
    (lfor clause self.clauses :if (self.match-head (first clause) (first goal)) clause))
    
  (defn match-head [self pattern term &optional [env (Env)]]
    (cond 
      [(variable? pattern) 
       (let [val (.lookup env pattern)]
         (if (is val None)
             (.extend env pattern term)
             (and (= val term) env)))]
      [(constant? pattern) 
       (if (= (real-value pattern) term) env None)]
      [(and (isinstance pattern Sequence) (isinstance term Sequence))
       (if (!= (len pattern) (len term))
           None
           (self.match-sequence (rest pattern) (rest term)
                              (self.match-head (first pattern) (first term) env)))]
      [(= pattern term) env]
      [True None]))
      
  (defn match-sequence [self patterns terms env]
    (if (or (is env None) (not patterns))
        env
        (self.match-sequence 
          (rest patterns) 
          (rest terms) 
          (self.match-head (first patterns) (first terms) env))))
    
  (defn eval-goal [self goal env]
    (setv matching-clauses (self.find-matching-clauses self goal))
    (if matching-clauses
        (let [[head body] (first matching-clauses)
              new-env (self.match-head head goal env)]
          (self.eval-body body new-env))
        None))
    
  (defn eval-body [self body env]
    (if (not body)
        env
        (let [new-env (self.eval-goal self (first body) env)]
          (if new-env
              (self.eval-body (rest body) new-env)
              None))))
              
  (defn query [self goal]
    (self.eval-goal self goal (Env)))
    
  (defn synthesize [self example-inputs example-outputs]
    "Synthesize a program from input-output examples"))

(defn reverse-interpreter [clauses queries]
  "Implementation of the reversible interpreter.
   In execution mode: given clauses and queries, returns results.
   In synthesis mode: given queries and expected results, returns clauses."
  (let [interp (Interpreter)]
    (for [[head body] clauses]
      (.add-clause interp head body))
    
    (lfor query queries
          (.eval-goal interp query (Env)))))
#+end_src

** Explanation-Based Learner

#+begin_src hy :tangle src/hy/ebg.hy
#!/usr/bin/env hy

(import [src.hy.reversible_interpreter [Interpreter variable? constant?]])

(defclass ExplanationNode []
  (defn __init__ [self goal children]
    (setv self.goal goal
          self.children children))
  
  (defn print-tree [self &optional [indent 0]]
    (print (.join "" (list (* indent "  "))) self.goal)
    (for [child self.children]
      (.print-tree child (+ indent 1)))))

(defclass ExplanationBuilder []
  (defn __init__ [self interpreter]
    (setv self.interpreter interpreter))
  
  (defn build-explanation [self goal]
    "Build an explanation tree for a given goal"
    (let [matching-clauses (.find-matching-clauses self.interpreter goal)]
      (if matching-clauses
          (let [[head body] (first matching-clauses)
                matched-env (.match-head self.interpreter head goal)]
            (ExplanationNode goal
                            (lfor subgoal body
                                  (.build-explanation self subgoal))))
          (ExplanationNode goal []))))
  
  (defn generalize-explanation [self expl-tree]
    "Generalize an explanation tree by replacing constants with variables"
    (defn generalize-term [term]
      (cond
        [(constant? term) (+ "*GEN" (cut term 1))]
        [(isinstance term str) term]
        [(isinstance term list)
         (list (map generalize-term term))]
        [True term]))
    
    (ExplanationNode 
      (generalize-term (.goal expl-tree))
      (lfor child (.children expl-tree)
            (.generalize-explanation self child)))))

(defn decompose-explanation [explanation composability decomp-force]
  "Decompose an explanation based on composability values"
  (defn should-decompose [goal]
    (and (in goal composability)
         (<= (get composability goal) decomp-force)))
  
  (defn decompose-node [node]
    (if (should-decompose (. node goal))
        (lfor child (. node children)
              (decompose-node child))
        [node]))
  
  (decompose-node explanation))
#+end_src

** Example Usage

#+begin_src hy :tangle examples/hy/append_example.hy
#!/usr/bin/env hy

(import [src.hy.reversible_interpreter [reverse-interpreter]])

(defn append-example []
  (print "Append Example (Execution mode):")
  (setv clauses 
    [[["append" [] "*l" "*l"] []]
     [["append" ["*x" "*l1"] "*l2" ["*x" "*l3"]] 
      [["append" "*l1" "*l2" "*l3"]]]])
     
  (setv queries [[["append" ["/x"] ["/y"] "*ans"]]])
  
  (setv results (reverse-interpreter clauses queries))
  (print "Results:" results))

(defn synthesize-append-example []
  (print "\nAppend Example (Synthesis mode):")
  ; In a real implementation, this would use the reversible interpreter in synthesis mode
  (print "Synthesized program that would append [/a] and [/b] to get [/a /b]:"))

(defmain [&rest args]
  (append-example)
  (synthesize-append-example))
#+end_src

* Scheme Implementation

The Scheme implementation provides a minimalist approach focusing on core concepts.

** Reversible Meta-Interpreter

#+begin_src scheme :tangle src/scheme/reversible-interpreter.scm
#!/usr/bin/env scheme
#|
Reversible Meta-Interpreter in Scheme
Based on the Prolog implementation from "Inductive Program Synthesis by Using 
a Reversible Meta-Interpreter" by Numao and Shimura
|#

; Environment handling
(define (make-env) '())
(define (extend-env var val env) (cons (cons var val) env))
(define (lookup-var var env)
  (let ((pair (assoc var env)))
    (if pair (cdr pair) #f)))

; Predicate utilities
(define (variable? x)
  (and (symbol? x) (char=? (string-ref (symbol->string x) 0) #\*)))

(define (constant? x)
  (and (symbol? x) (char=? (string-ref (symbol->string x) 0) #\/)))

(define (real-value x)
  (if (constant? x)
      (string->symbol (substring (symbol->string x) 1))
      x))

; The reversible interpreter
(define (prolog clauses queries)
  (map (lambda (query) (goalseq query (make-env) clauses)) queries))

(define (goalseq goals env clauses)
  (if (null? goals)
      env
      (let ((new-env (goal (car goals) env clauses)))
        (if new-env
            (goalseq (cdr goals) new-env clauses)
            #f))))

(define (goal g env clauses)
  (let loop ((cs clauses))
    (if (null? cs)
        #f
        (let* ((clause (car cs))
               (head (car clause))
               (body (cadr clause))
               (new-env (match-head head g env)))
          (if new-env
              (goalseq body new-env clauses)
              (loop (cdr cs)))))))

(define (match-head pattern term env)
  (cond
    ((variable? pattern)
     (let ((val (lookup-var pattern env)))
       (if val
           (if (equal? val term) env #f)
           (extend-env pattern term env))))
    ((constant? pattern)
     (if (equal? (real-value pattern) term) env #f))
    ((and (pair? pattern) (pair? term))
     (let ((new-env (match-head (car pattern) (car term) env)))
       (if new-env
           (match-head (cdr pattern) (cdr term) new-env)
           #f)))
    ((equal? pattern term) env)
    (else #f)))

; Synthesis mode
(define (synthesize-program queries results)
  (display "Program synthesis from examples"))

; Exports
(define exports
  (list (cons 'prolog prolog)
        (cons 'synthesize-program synthesize-program)))
#+end_src

** Explanation-Based Learning

#+begin_src scheme :tangle src/scheme/ebg.scm
#!/usr/bin/env scheme

; Explanation tree structure
(define (make-explanation-node goal children)
  (list 'expl-node goal children))

(define (explanation-node-goal node) (cadr node))
(define (explanation-node-children node) (caddr node))

; Building an explanation
(define (build-explanation goal clauses)
  (let ((matching-clauses (filter 
                            (lambda (clause) 
                              (can-match? (car clause) goal))
                            clauses)))
    (if (null? matching-clauses)
        (make-explanation-node goal '())
        (let* ((clause (car matching-clauses))
               (head (car clause))
               (body (cadr clause)))
          (make-explanation-node 
            goal
            (map (lambda (subgoal) 
                   (build-explanation subgoal clauses))
                 body))))))

(define (can-match? pattern term)
  (cond
    ((variable? pattern) #t)
    ((constant? pattern) 
     (equal? (real-value pattern) (real-value term)))
    ((and (pair? pattern) (pair? term))
     (and (can-match? (car pattern) (car term))
          (can-match? (cdr pattern) (cdr term))))
    (else (equal? pattern term))))

; Generalization
(define (generalize-explanation expl-tree)
  (let generalize-term ((term (explanation-node-goal expl-tree)))
    (cond
      ((constant? term) 
       (string->symbol 
         (string-append "*GEN" 
                        (substring (symbol->string term) 1))))
      ((symbol? term) term)
      ((pair? term)
       (cons (generalize-term (car term))
             (generalize-term (cdr term))))
      (else term))))

; Decomposition based on composability
(define (decompose-explanation expl composability decomp-force)
  (let ((goal (explanation-node-goal expl)))
    (if (and (assoc goal composability)
             (<= (cdr (assoc goal composability)) decomp-force))
        (apply append
               (map (lambda (child) 
                      (decompose-explanation 
                        child composability decomp-force))
                    (explanation-node-children expl)))
        (list expl))))

; Exports
(define exports
  (list (cons 'build-explanation build-explanation)
        (cons 'generalize-explanation generalize-explanation)
        (cons 'decompose-explanation decompose-explanation)))
#+end_src

** Example Usage

#+begin_src scheme :tangle examples/scheme/append-example.scm
#!/usr/bin/env scheme
(load "../../src/scheme/reversible-interpreter.scm")

; Example of using the reversible interpreter to execute the append program
(define append-clauses
  '(((append () *l *l) ())
    ((append (*x . *l1) *l2 (*x . *l3)) 
     ((append *l1 *l2 *l3)))))

(define append-queries
  '(((append (/x) (/y) *ans))))

(display "Running append program: ")
(display (prolog append-clauses append-queries))
(newline)

; Example of program synthesis
(display "Synthesizing append program from examples")
(newline)
(display (synthesize-program 
           '(((append (/a) (/b) (/a /b))))
           '()))
(newline)
#+end_src

* Installation and Dependencies

** Linux

#+begin_src shell :tangle install-linux.sh
#!/bin/bash
set -e

echo "Installing dependencies for reversible-meta-synthesis on Linux..."

# Prolog (SWI-Prolog)
if ! command -v swipl &> /dev/null; then
    echo "Installing SWI-Prolog..."
    sudo apt-get update
    sudo apt-get install -y swi-prolog
fi

# Hy (1.0.0)
if ! pip3 show hy | grep -q "Version: 1.0.0"; then
    echo "Installing Hy 1.0.0..."
    pip3 install 'hy==1.0.0'
fi

# Scheme (Guile)
if ! command -v guile &> /dev/null; then
    echo "Installing Guile Scheme..."
    sudo apt-get install -y guile-3.0
fi

echo "All dependencies installed successfully!"
#+end_src

** FreeBSD

#+begin_src shell :tangle install-freebsd.sh
#!/bin/sh
set -e

echo "Installing dependencies for reversible-meta-synthesis on FreeBSD..."

# Prolog (SWI-Prolog)
if ! which swipl >/dev/null 2>&1; then
    echo "Installing SWI-Prolog..."
    pkg install -y swi-prolog
fi

# Hy (1.0.0)
if ! pip show hy | grep -q "Version: 1.0.0"; then
    echo "Installing Hy 1.0.0..."
    pkg install -y py39-pip
    pip install 'hy==1.0.0'
fi

# Scheme (Guile)
if ! which guile >/dev/null 2>&1; then
    echo "Installing Guile Scheme..."
    pkg install -y guile3
fi

echo "All dependencies installed successfully!"
#+end_src

** macOS

#+begin_src shell :tangle install-macos.sh
#!/bin/bash
set -e

echo "Installing dependencies for reversible-meta-synthesis on macOS..."

# Check for Homebrew
if ! command -v brew &> /dev/null; then
    echo "Homebrew not found. Installing..."
    /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
fi

# Prolog (SWI-Prolog)
if ! command -v swipl &> /dev/null; then
    echo "Installing SWI-Prolog..."
    brew install swi-prolog
fi

# Hy (1.0.0)
if ! pip3 show hy | grep -q "Version: 1.0.0"; then
    echo "Installing Hy 1.0.0..."
    pip3 install 'hy==1.0.0'
fi

# Scheme (Guile)
if ! command -v guile &> /dev/null; then
    echo "Installing Guile Scheme..."
    brew install guile
fi

echo "All dependencies installed successfully!"
#+end_src

* Running Tests

#+begin_src org :tangle tests/README.org
#+TITLE: Tests for Reversible Meta-Synthesis

This directory contains tests for the different implementations:

* Prolog Tests

To run the Prolog tests:

#+begin_src shell
cd tests/prolog
swipl -s run_tests.pl
#+end_src

* Hy Tests

To run the Hy tests:

#+begin_src shell
cd tests/hy
hy run_tests.hy
#+end_src

* Scheme Tests

To run the Scheme tests:

#+begin_src shell
cd tests/scheme
guile run-tests.scm
#+end_src
#+end_src

* Project README

#+begin_src markdown :tangle README.md
# Reversible Meta-Synthesis

An implementation of "Inductive Program Synthesis by Using a Reversible Meta-Interpreter" by Masayuki Numao and Masamichi Shimura (1990).

## Overview

This repository implements the concepts from the seminal paper on using reversible meta-interpreters for inductive program synthesis. The key innovations in this approach include:

1. A reversible interpreter that can both execute programs and synthesize them from examples
2. Explanation-based learning to accelerate program synthesis
3. Decomposition of explanations based on composability for flexible knowledge transfer

## Implementations

Three implementations are provided:
- **Prolog**: The original language used in the paper, providing the most faithful implementation
- **Hy**: A Lisp dialect embedded in Python, bridging functional and imperative paradigms
- **Scheme**: A minimalist Lisp implementation focusing on core concepts

## Key Examples

The repository includes implementations of several examples from the paper:
- Basic append program synthesis
- Synthesizing merge3 from app3
- Synthesizing rzip from zip
- Synthesizing fullrev from reverse and flatten

## Getting Started

1. Clone this repository
2. Run the appropriate installation script for your platform:
   ```
   # Linux
   ./install-linux.sh
   
   # FreeBSD
   ./install-freebsd.sh
   
   # macOS
   ./install-macos.sh
   ```
3. Explore the examples in the `examples` directory

## Directory Structure

```
.
├── doc/                  # Documentation
├── examples/             # Example programs
│   ├── prolog/
│   ├── hy/
│   └── scheme/
├── src/                  # Source code
│   ├── prolog/
│   ├── hy/
│   └── scheme/
└── tests/                # Tests
    ├── prolog/
    ├── hy/
    └── scheme/
```

## References

Numao, M., & Shimura, M. (1990). Inductive Program Synthesis by Using a Reversible Meta-Interpreter. In M. Bruynooghe (Ed.), Proc. the Second Workshop on Meta-Programming in Logic, pp. 123-136, Leuven, Belgium.
#+end_src

* Prolog Tests

Let's add some tests for the Prolog implementation:

#+begin_src prolog :tangle tests/prolog/run_tests.pl
:- use_module(library(plunit)).

% Load the implementation files
:- consult('../../src/prolog/reversible_interpreter.pl').
:- consult('../../src/prolog/ebg.pl').
:- consult('../../src/prolog/composability.pl').
:- consult('../../src/prolog/executable_explanation.pl').

% Test suite for the reversible interpreter
:- begin_tests(reversible_interpreter).

test(basic_append_execution) :-
    prolog([([append, [], *l, *l] :- []),
            ([append, [*x | *l1], *l2, [*x|*l3]] :- [[append, *l1, *l2, *l3]])],
           [[[append, [/a, /b], [/c, /d], *ans]]],
           Value),
    Value = [[(*ans, [/a, /b, /c, /d])]].

test(basic_append_synthesis) :-
    prolog(Clauses,
           [[[append, [], [/a, /b], [/a, /b]]],
            [[append, [/c], [/d], [/c, /d]]]],
           [[], []]),
    member(([append, [], *l, *l] :- []), Clauses),
    member(([append, [*x | *l1], *l2, [*x|*l3]] :- [[append, *l1, *l2, *l3]]), Clauses).

:- end_tests(reversible_interpreter).

% Test suite for explanation-based learning
:- begin_tests(ebg).

test(ebg_basic) :-
    % Test that EBG can generalize a simple append example
    Goal = prolog([([append, [], *l, *l] :- []),
                  ([append, [*x | *l1], *l2, [*x|*l3]] :- [[append, *l1, *l2, *l3]])],
                 [[[append, [/a, /b], [/c, /d], *ans]]],
                 [[(*ans, [/a, /b, /c, /d])]]),
    ebg(Goal, Head, _Body),
    % Check that the generalized head contains the right predicate
    Head = prolog(_, _, _).

:- end_tests(ebg).

% Execute the tests
:- run_tests.
:- halt.
#+end_src

* Hy Tests

Let's add tests for the Hy implementation:

#+begin_src hy :tangle tests/hy/run_tests.hy
#!/usr/bin/env hy

(import [src.hy.reversible_interpreter [reverse-interpreter Interpreter]])
(import [src.hy.ebg [ExplanationBuilder]])
(import [unittest [TestCase main]])

(defclass ReversibleInterpreterTests [TestCase]
  (defn test-basic-append-execution [self]
    "Test that the interpreter can execute a basic append program"
    (setv clauses 
      [[["append" [] "*l" "*l"] []]
       [["append" ["*x" "*l1"] "*l2" ["*x" "*l3"]] 
        [["append" "*l1" "*l2" "*l3"]]]])
       
    (setv queries [[["append" ["/a" "/b"] ["/c" "/d"] "*ans"]]])
    
    (setv results (reverse-interpreter clauses queries))
    (.assertEqual self (first results) {"*ans" ["/a" "/b" "/c" "/d"]}))
  
  (defn test-variable-binding [self]
    "Test that variables are properly bound in the environment"
    (setv interp (Interpreter))
    (setv env (.match-head interp "*var" "value" None))
    (.assertEqual self (.lookup env "*var") "value")))

(defclass EBGTests [TestCase]
  (defn test-explanation-building [self]
    "Test that we can build and generalize explanations"
    (setv interp (Interpreter))
    (.add-clause interp ["append" [] "*l" "*l"] [])
    (.add-clause interp 
      ["append" ["*x" "*l1"] "*l2" ["*x" "*l3"]] 
      [["append" "*l1" "*l2" "*l3"]])
    
    (setv builder (ExplanationBuilder interp))
    (setv expl (.build-explanation builder ["append" ["/a"] ["/b"] "*ans"]))
    
    (.assertIsNotNone self expl)
    ; Test generalization
    (setv gen-expl (.generalize-explanation builder expl))
    (.assertNotEqual self 
                     (.goal expl) 
                     (.goal gen-expl)
                     "Generalization should replace constants with variables")))

(defmain [&rest args]
  (main))
#+end_src

* Scheme Tests

Let's add tests for the Scheme implementation:

#+begin_src scheme :tangle tests/scheme/run-tests.scm
#!/usr/bin/env guile
!#

(load "../../src/scheme/reversible-interpreter.scm")
(load "../../src/scheme/ebg.scm")

;; Simple test framework
(define (assert-equal expected actual message)
  (if (equal? expected actual)
      (begin
        (display "PASS: ")
        (display message)
        (newline))
      (begin
        (display "FAIL: ")
        (display message)
        (newline)
        (display "  Expected: ")
        (display expected)
        (newline)
        (display "  Actual:   ")
        (display actual)
        (newline))))

;; Test cases for the reversible interpreter
(define (test-reversible-interpreter)
  (display "\nTesting Reversible Interpreter:\n")
  
  ;; Test append execution
  (let* ((append-clauses
           '(((append () *l *l) ())
             ((append (*x . *l1) *l2 (*x . *l3)) 
              ((append *l1 *l2 *l3)))))
         (append-queries
           '(((append (/a /b) (/c /d) *ans))))
         (results (prolog append-clauses append-queries)))
    
    (assert-equal 
      '(((*ans /a /b /c /d)))
      results
      "Basic append execution")))

;; Test cases for EBG
(define (test-ebg)
  (display "\nTesting Explanation-Based Learning:\n")
  
  ;; Test explanation building
  (let* ((append-clauses
           '(((append () *l *l) ())
             ((append (*x . *l1) *l2 (*x . *l3)) 
              ((append *l1 *l2 *l3)))))
         (goal '(append (/a) (/b) (/a /b)))
         (explanation (build-explanation goal append-clauses)))
    
    (assert-equal 
      'expl-node
      (car explanation)
      "Building explanation tree")
    
    ;; Test explanation generalization
    (let ((generalized (generalize-explanation explanation)))
      (assert-equal 
        'expl-node
        (car generalized)
        "Generalizing explanation tree"))))

;; Run all tests
(define (run-all-tests)
  (test-reversible-interpreter)
  (test-ebg)
  (display "\nAll tests completed.\n"))

(run-all-tests)
#+end_src

* Mermaid Diagrams

Let's add some Mermaid diagrams to visualize the key concepts:

#+begin_src org :tangle doc/diagrams.org
#+TITLE: Reversible Meta-Synthesis Diagrams
#+AUTHOR: aygp-dr
#+DATE: 2025-03-29

* Reversible Interpreter Flow

This diagram illustrates the dual nature of the reversible interpreter:

#+begin_src mermaid :file doc/reversible_interpreter.png
graph TD
    subgraph "Execution Mode"
    A[Program] --> B[Interpreter]
    C[Data] --> B
    B --> D[Result]
    end
    
    subgraph "Synthesis Mode"
    E[Interpreter] --> F[Program]
    G[Data] --> E
    H[Desired Result] --> E
    end
#+end_src

* Explanation-Based Learning Process

This diagram shows how explanation-based learning accelerates program synthesis:

#+begin_src mermaid :file doc/ebg_process.png
flowchart TD
    A[Example Program] --> B[Build Explanation]
    B --> C[Generalize Explanation]
    C --> D[Decompose Explanation]
    D --> E[Apply to New Problem]
    E --> F[Synthesized Program]
    
    G[Composability Values] --> D
    H[DECOMP_FORCE] --> D
#+end_src

* Decomposition Levels

This diagram illustrates the different levels of explanation decomposition:

#+begin_src mermaid :file doc/decomposition_levels.png
graph TD
    A[Whole Explanation] --> B[Program-Level Chunks]
    B --> C[Clause-Level Chunks]
    C --> D[Clause-Structure-Level Chunks]
    D --> E[Term-Level Chunks]
    E --> F[Primitive-Level Chunks]
    
    B -- "DECOMP_FORCE = 0" --> B
    C -- "DECOMP_FORCE = 1" --> C
    D -- "DECOMP_FORCE = 2" --> D
    E -- "DECOMP_FORCE = 3" --> E
    F -- "DECOMP_FORCE = 4" --> F
#+end_src
#+end_src

* Usage Examples

Let's create a comprehensive example of using the system:

#+begin_src org :tangle doc/usage_examples.org
#+TITLE: Reversible Meta-Synthesis Usage Examples
#+AUTHOR: aygp-dr
#+DATE: 2025-03-29

* Prolog Usage

** Basic Execution and Synthesis

This example demonstrates basic usage of the reversible interpreter in both execution and synthesis modes:

#+begin_src prolog
% Load the reversible interpreter
:- consult('../src/prolog/reversible_interpreter.pl').

% Example 1: Execution mode - run the append program
example_execution :-
    prolog([([append, [], *l, *l] :- []),
            ([append, [*x | *l1], *l2, [*x|*l3]] :- [[append, *l1, *l2, *l3]])],
           [[[append, [/a, /b], [/c, /d], *ans]]],
           Value),
    write('Result: '), write(Value), nl.
    % Expected output: Result: [[(*ans,[/a,/b,/c,/d])]]

% Example 2: Synthesis mode - create an append program from examples
example_synthesis :-
    prolog(Clauses,
           [[[append, [], [/a, /b], [/a, /b]]],
            [[append, [/c], [/d], [/c, /d]]]],
           [[], []]),
    write('Synthesized program: '), nl,
    write_clauses(Clauses).

write_clauses([]).
write_clauses([Clause | Rest]) :-
    write('  '), write(Clause), nl,
    write_clauses(Rest).
#+end_src

** Explanation-Based Learning

This example demonstrates how to use explanation-based learning to accelerate synthesis:

#+begin_src prolog
% Load the necessary files
:- consult('../src/prolog/reversible_interpreter.pl').
:- consult('../src/prolog/ebg.pl').
:- consult('../src/prolog/composability.pl').
:- consult('../src/prolog/executable_explanation.pl').

% Build an explanation for the append program
build_append_explanation :-
    Goal = prolog([([append, [], *l, *l] :- []),
                   ([append, [*x | *l1], *l2, [*x|*l3]] :- [[append, *l1, *l2, *l3]])],
                  [[[append, [/a, /b], [/c, /d], *ans]]],
                  [[(*ans, [/a, /b, /c, /d])]]),
    ebg(Goal, Head, Body),
    write('Generalized head: '), write(Head), nl,
    write('Generalized body: '), nl,
    write_body(Body).

write_body([]).
write_body([Condition | Rest]) :-
    write('  '), write(Condition), nl,
    write_body(Rest).
#+end_src

** Synthesizing Complex Programs

This example demonstrates synthesizing a complex program using explanation decomposition:

#+begin_src prolog
% Load all necessary files
:- consult('../src/prolog/reversible_interpreter.pl').
:- consult('../src/prolog/ebg.pl').
:- consult('../src/prolog/composability.pl').
:- consult('../src/prolog/executable_explanation.pl').

% Synthesize merge3 from app3 using clause-level chunks
synthesize_merge3 :-
    % Set global variable to decompose at clause level
    nb_setval(DECOMP_FORCE, 1),
    
    % First create explanation from app3
    prolog([([append, [], *x, *x] :- []),
            ([append, [*x|*l1], *l2, [*x|*l3]] :- [[append, *l1, *l2, *l3]]),
            ([app3, *x, *y, *z, *a] :- [[append, *x, *y, *aa], [append, *aa, *z, *a]])],
            [[[app3, [/a], [/b], [/c], [/a, /b, /c]]]],
            _),
    
    % Then synthesize merge3 using the explanation
    etree(1,
          prolog([([merge, [], *x, *x] :- []),
                  ([merge, *x, [], *x] :- []),
                  ([merge, [*x|*l1], [*y|*l2], [*x|*l3]] :- [[=<, *x, *y], [merge, *l1, [*y|*l2], *l3]]),
                  ([merge, [*x|*l1], [*y|*l2], [*y|*l3]] :- [[>, *x, *y], [merge, [*x|*l1], *l2, *l3]])
                  | Clauses],
                 [[[merge3, [/3], [/1], [/2], [/1, /2, /3]]]],
                 [[]])),
    write('Synthesized merge3 program: '), nl,
    write_clauses(Clauses).
#+end_src

* Hy Usage

Here's how to use the Hy implementation:

#+begin_src hy
#!/usr/bin/env hy

(import [src.hy.reversible_interpreter [reverse-interpreter Interpreter]])
(import [src.hy.ebg [ExplanationBuilder]])

(defn basic-example []
  (print "Basic Reversible Interpreter Example")
  
  ; Define the append program
  (setv clauses 
    [[["append" [] "*l" "*l"] []]
     [["append" ["*x" "*l1"] "*l2" ["*x" "*l3"]] 
      [["append" "*l1" "*l2" "*l3"]]]])
     
  ; Execute the program
  (setv queries [[["append" ["/a" "/b"] ["/c" "/d"] "*ans"]]])
  (setv results (reverse-interpreter clauses queries))
  
  (print "Execution results:")
  (print results)
  
  ; In a real implementation, synthesis would be:
  ; (setv synthesized-program (synthesize-program input-examples output-examples))
  (print "\nProgram synthesis is also possible with the reversible interpreter"))

(defn explanation-example []
  (print "\nExplanation-Based Learning Example")
  
  ; Create an interpreter with the append program
  (setv interp (Interpreter))
  (.add-clause interp ["append" [] "*l" "*l"] [])
  (.add-clause interp 
    ["append" ["*x" "*l1"] "*l2" ["*x" "*l3"]] 
    [["append" "*l1" "*l2" "*l3"]])
  
  ; Build an explanation
  (setv builder (ExplanationBuilder interp))
  (setv expl (.build-explanation builder ["append" ["/a" "/b"] ["/c" "/d"] "*result"]))
  
  (print "Built explanation for append [/a /b] [/c /d]")
  
  ; Generalize the explanation
  (setv gen-expl (.generalize-explanation builder expl))
  
  (print "Generalized the explanation")
  (print "This generalized explanation can be used to synthesize new programs"))

(defmain [&rest args]
  (basic-example)
  (explanation-example))
#+end_src

* Scheme Usage

Here's how to use the Scheme implementation:

#+begin_src scheme
#!/usr/bin/env guile
!#

(load "../src/scheme/reversible-interpreter.scm")
(load "../src/scheme/ebg.scm")

(define (basic-example)
  (display "Basic Reversible Interpreter Example\n")
  
  ; Define the append program
  (define append-clauses
    '(((append () *l *l) ())
      ((append (*x . *l1) *l2 (*x . *l3)) 
       ((append *l1 *l2 *l3)))))
       
  ; Execute the program
  (define append-queries
    '(((append (/a /b) (/c /d) *ans))))
  
  (define results (prolog append-clauses append-queries))
  
  (display "Execution results:\n")
  (display results)
  (newline)
  
  ; In a real implementation, synthesis would be:
  ; (define synthesized-program (synthesize-program input-examples output-examples))
  (display "\nProgram synthesis is also possible with the reversible interpreter\n"))

(define (explanation-example)
  (display "\nExplanation-Based Learning Example\n")
  
  ; Define the append program
  (define append-clauses
    '(((append () *l *l) ())
      ((append (*x . *l1) *l2 (*x . *l3)) 
       ((append *l1 *l2 *l3)))))
  
  ; Build an explanation
  (define goal '(append (/a /b) (/c /d) (/a /b /c /d)))
  (define expl (build-explanation goal append-clauses))
  
  (display "Built explanation for append [/a /b] [/c /d]\n")
  
  ; Generalize the explanation
  (define gen-expl (generalize-explanation expl))
  
  (display "Generalized the explanation\n")
  (display "This generalized explanation can be used to synthesize new programs\n"))

(basic-example)
(explanation-example)
#+end_src
#+end_src

* Development Plan

Let's outline a development plan for the project:

#+begin_src org :tangle doc/development_plan.org
#+TITLE: Reversible Meta-Synthesis Development Plan
#+AUTHOR: aygp-dr
#+DATE: 2025-03-29

* Phase 1: Core Implementation

** Prolog Implementation
- [X] Implement the reversible interpreter
- [X] Implement explanation-based learning
- [X] Implement composability mechanism
- [X] Implement executable explanations
- [X] Implement basic examples from the paper

** Basic Cross-Platform Support
- [X] Create installation scripts for Linux, FreeBSD, and macOS
- [X] Test basic functionality across platforms

* Phase 2: Alternative Implementations

** Hy Implementation
- [X] Implement the core reversible interpreter
- [X] Implement explanation-based learning
- [ ] Implement full composability mechanism
- [ ] Port all examples from the Prolog implementation

** Scheme Implementation
- [X] Implement the core reversible interpreter
- [X] Implement explanation-based learning
- [ ] Implement full composability mechanism
- [ ] Port all examples from the Prolog implementation

* Phase 3: Testing and Documentation

** Comprehensive Testing
- [X] Create test suite for Prolog implementation
- [X] Create test suite for Hy implementation
- [X] Create test suite for Scheme implementation
- [ ] Create integration tests that work across implementations

** Documentation
- [X] Create basic usage documentation
- [X] Create diagrams illustrating key concepts
- [ ] Create comprehensive API documentation
- [ ] Write tutorials for common use cases

* Phase 4: Extensions and Applications

** Advanced Examples
- [ ] Implement more complex program synthesis examples
- [ ] Implement domain-specific program synthesis
- [ ] Explore applications to machine learning tasks

** Extensions
- [ ] Explore integrating with modern explanation-based learning approaches
- [ ] Implement a web interface for demonstration
- [ ] Support for additional languages
#+end_src

* Contribution Guidelines

Let's add contribution guidelines:

#+begin_src markdown :tangle CONTRIBUTING.md
# Contributing to Reversible Meta-Synthesis

Thank you for considering contributing to this project! This document outlines the process for contributing and guidelines to follow.

## How to Contribute

1. **Fork the repository**
2. **Create a new branch**
   ```bash
   git checkout -b feature/your-feature-name
   ```
3. **Make your changes**
4. **Run tests**
   ```bash
   # For Prolog
   cd tests/prolog
   swipl -s run_tests.pl
   
   # For Hy
   cd tests/hy
   hy run_tests.hy
   
   # For Scheme
   cd tests/scheme
   guile run-tests.scm
   ```
5. **Push your changes to your fork**
6. **Create a pull request**

## Code Style Guidelines

### Prolog
- Use meaningful predicate and variable names
- Document predicates with comments indicating their purpose and arguments
- Follow the style used in the existing codebase

### Hy
- Follow PEP 8 conventions where applicable
- Use Lisp-style naming conventions (kebab-case) for functions and variables
- Document functions with docstrings

### Scheme
- Follow Scheme coding conventions (kebab-case)
- Keep functions small and focused
- Document functions with comments

## Testing

- Add tests for all new functionality
- Ensure all existing tests pass before submitting a pull request
- Include examples demonstrating new functionality

## Documentation

- Update documentation to reflect any changes in API or functionality
- Document any new features or examples
- Ensure diagrams are up-to-date with the current implementation

## Reporting Issues

If you find a bug or have a suggestion for improvement:

1. Check if the issue already exists in the issue tracker
2. If not, create a new issue with:
   - A clear title and description
   - Steps to reproduce the issue
   - Expected behavior
   - Actual behavior
   - Any error messages or logs

## License

By contributing to this project, you agree that your contributions will be licensed under the same license as the project.
#+end_src

* License

Let's add a license file:

#+begin_src markdown :tangle LICENSE
MIT License

Copyright (c) 2025 aygp-dr

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
#+end_src

* Final Tangle Commands

To tangle all files at once, use:

#+begin_src shell :tangle tangle_all.sh
#!/bin/bash
emacs --batch -l org --eval "(org-babel-tangle-file \"SETUP.org\")"
chmod +x install-linux.sh install-freebsd.sh install-macos.sh
chmod +x examples/prolog/*.pl examples/hy/*.hy examples/scheme/*.scm
chmod +x tests/prolog/run_tests.pl tests/hy/run_tests.hy tests/scheme/run-tests.scm
echo "All files tangled successfully!"
#+end_src

* Clojure Implementation

Let's add a Clojure implementation that leverages Clojure's powerful features for working with immutable data structures and its strong Java interoperability.

** Reversible Meta-Interpreter

#+begin_src clojure :tangle src/clojure/reversible_interpreter.clj
(ns reversible-meta-synthesis.reversible-interpreter
  (:require [clojure.string :as str]))

;; Environment management
(defn make-env []
  {})

(defn lookup-var [env var]
  (get env var))

(defn extend-env [env var val]
  (assoc env var val))

;; Predicates for syntax
(defn variable? [x]
  (and (symbol? x) (str/starts-with? (name x) "*")))

(defn constant? [x]
  (and (symbol? x) (str/starts-with? (name x) "/")))

(defn real-value [x]
  (if (constant? x)
    (symbol (subs (name x) 1))
    x))

;; Core reversible interpreter
(defrecord Clause [head body])

(defn match-head
  "Match a pattern (clause head) against a term (goal), returning updated env if successful"
  ([pattern term] (match-head pattern term (make-env)))
  ([pattern term env]
   (cond
     (variable? pattern)
     (let [val (lookup-var env pattern)]
       (if val
         (if (= val term) env nil)
         (extend-env env pattern term)))
     
     (constant? pattern)
     (if (= (real-value pattern) term) env nil)
     
     (and (sequential? pattern) (sequential? term) (= (count pattern) (count term)))
     (reduce 
       (fn [env' [p t]] 
         (if env'
           (match-head p t env')
           (reduced nil)))
       env
       (map vector pattern term))
     
     (= pattern term) env
     
     :else nil)))

(defn find-matching-clauses [clauses goal]
  (filter #(match-head (:head %) (first goal)) clauses))

(defn eval-goal 
  "Evaluate a goal against a set of clauses with the current environment"
  [goal env clauses]
  (let [matching (find-matching-clauses clauses goal)]
    (when-let [clause (first matching)]
      (let [new-env (match-head (:head clause) goal env)]
        (eval-body (:body clause) new-env clauses)))))

(defn eval-body
  "Evaluate the body of a clause with the current environment"
  [body env clauses]
  (if (empty? body)
    env
    (when-let [new-env (eval-goal (first body) env clauses)]
      (eval-body (rest body) new-env clauses))))

;; Main interpreter functions
(defn create-clause 
  "Create a clause from head and body"
  [head body]
  (->Clause head body))

(defn prolog 
  "Main function for the reversible interpreter.
   In execution mode: given clauses and queries, returns results.
   In synthesis mode: given queries and expected results, returns clauses."
  [clauses queries]
  (let [clause-records (map (fn [[h b]] (create-clause h b)) clauses)]
    (map #(eval-goal % (make-env) clause-records) queries)))

;; Synthesis mode
(defn synthesize 
  "Program synthesis from examples"
  [examples]
  (let [query-value-pairs (map (fn [[q v]] [q v]) examples)]
    ;; Implementation would search for clauses that satisfy all examples
    (println "Program synthesis from" (count examples) "examples")))
#+end_src

** Explanation-Based Learning

#+begin_src clojure :tangle src/clojure/ebg.clj
(ns reversible-meta-synthesis.ebg
  (:require [reversible-meta-synthesis.reversible-interpreter :as ri]))

;; Explanation tree structure
(defrecord ExplanationNode [goal children])

(defn make-explanation-node [goal children]
  (->ExplanationNode goal children))

;; Building explanations
(defn build-explanation
  "Build an explanation tree for a given goal using available clauses"
  [goal clauses]
  (let [matching-clauses (ri/find-matching-clauses clauses goal)]
    (if (empty? matching-clauses)
      (make-explanation-node goal [])
      (let [clause (first matching-clauses)
            matched-env (ri/match-head (:head clause) goal)]
        (make-explanation-node 
          goal
          (mapv #(build-explanation % clauses) (:body clause)))))))

;; Generalization
(defn generalize-term
  "Generalize a term by replacing constants with variables"
  [term]
  (cond
    (ri/constant? term) (symbol (str "*GEN" (subs (name term) 1)))
    (symbol? term) term
    (sequential? term) (mapv generalize-term term)
    :else term))

(defn generalize-explanation
  "Generalize an explanation tree"
  [expl-tree]
  (make-explanation-node
    (generalize-term (:goal expl-tree))
    (mapv generalize-explanation (:children expl-tree))))

;; Decomposition based on composability
(defn decompose-explanation
  "Decompose an explanation based on composability values"
  [explanation composability decomp-force]
  (let [goal (:goal explanation)
        comp-value (get composability goal)
        should-decompose (and comp-value (<= comp-value decomp-force))]
    (if should-decompose
      (mapcat #(decompose-explanation % composability decomp-force) 
              (:children explanation))
      [explanation])))

;; Executable explanation
(defn create-executable-explanation
  "Create an executable version of the explanation tree"
  [explanation]
  (let [node-id (gensym "node")]
    {:id node-id
     :goal (:goal explanation)
     :children (mapv create-executable-explanation (:children explanation))}))

(defn apply-explanation
  "Apply an executable explanation to synthesize a program"
  [explanation target-goal]
  (println "Applying explanation to synthesize a program for" target-goal))
#+end_src

** Composability

#+begin_src clojure :tangle src/clojure/composability.clj
(ns reversible-meta-synthesis.composability)

;; Composability values for different predicates
(def composability-values
  {'prolog 1
   'goalseq 3
   'goal 1
   'search 1
   'head 4
   'head1 4
   'bind 4
   'fetch 2
   'fetch1 3})

(defn get-composability
  "Get the composability value for a goal"
  [goal]
  (let [pred (if (sequential? goal) (first goal) goal)]
    (get composability-values pred 0)))

(defn should-decompose?
  "Determine if a goal should be decomposed based on its composability"
  [goal decomp-force]
  (<= (get-composability goal) decomp-force))

;; Analysis functions to automatically determine composability
(defn analyze-dependencies
  "Analyze dependencies between predicates to determine composability"
  [clauses]
  (let [dependencies (atom {})]
    ;; Build dependency graph
    (doseq [[head body] clauses]
      (let [head-pred (if (sequential? head) (first head) head)]
        (doseq [goal body]
          (let [goal-pred (if (sequential? goal) (first goal) goal)]
            (swap! dependencies update head-pred (fnil conj #{}) goal-pred)))))
    
    ;; Calculate composability based on dependency structure
    (let [dep-graph @dependencies
          predicates (keys dep-graph)
          composability (atom {})]
      (doseq [pred predicates]
        (let [depth (atom 0)
              visited (atom #{})]
          ;; Calculate depth of predicate in dependency graph
          (letfn [(calculate-depth [p]
                    (when-not (@visited p)
                      (swap! visited conj p)
                      (if-let [deps (get dep-graph p)]
                        (do
                          (doseq [dep deps]
                            (calculate-depth dep))
                          (swap! depth inc))
                        (swap! depth inc))))]
            (calculate-depth pred)
            (swap! composability assoc pred @depth))))
      
      @composability)))
#+end_src

** Example Programs

#+begin_src clojure :tangle examples/clojure/append_example.clj
(ns reversible-meta-synthesis.examples.append
  (:require [reversible-meta-synthesis.reversible-interpreter :as ri]))

;; Define the append program
(def append-clauses
  [[['append [] '*l '*l] []]
   [['append ['*x '& '*l1] '*l2 ['*x '& '*l3]] 
    [['append '*l1 '*l2 '*l3]]]])

(defn example-execution []
  (println "Running append program in execution mode:")
  
  (let [queries [[['append ['/a '/b] ['/c '/d] '*ans]]]
        results (ri/prolog append-clauses queries)]
    
    (println "Query:" (first queries))
    (println "Result:" (first results))))

(defn example-synthesis []
  (println "\nSynthesizing append program from examples:")
  
  (let [examples [[[['append [] ['/a '/b] ['/a '/b]]] []]
                  [[['append ['/c] ['/d] ['/c '/d]]] []]]
        synthesized (ri/synthesize examples)]
    
    (println "Examples:" examples)
    (println "Synthesized program:" synthesized)))

(defn -main [& args]
  (example-execution)
  (example-synthesis))
#+end_src

#+begin_src clojure :tangle examples/clojure/app3_merge3_example.clj
(ns reversible-meta-synthesis.examples.app3-merge3
  (:require [reversible-meta-synthesis.reversible-interpreter :as ri]
            [reversible-meta-synthesis.ebg :as ebg]
            [reversible-meta-synthesis.composability :as comp]))

;; Define the append and app3 programs
(def append-clauses
  [[['append [] '*x '*x] []]
   [['append ['*x '& '*l1] '*l2 ['*x '& '*l3]] 
    [['append '*l1 '*l2 '*l3]]]])

(def app3-clauses
  [[['app3 '*x '*y '*z '*a] 
    [['append '*x '*y '*aa] ['append '*aa '*z '*a]]]])

;; Define the merge program
(def merge-clauses
  [[['merge [] '*x '*x] []]
   [['merge '*x [] '*x] []]
   [['merge ['*x '& '*l1] ['*y '& '*l2] ['*x '& '*l3]] 
    [['<= '*x '*y] ['merge '*l1 ['*y '& '*l2] '*l3]]]
   [['merge ['*x '& '*l1] ['*y '& '*l2] ['*y '& '*l3]] 
    [[> '*x '*y] ['merge ['*x '& '*l1] '*l2 '*l3]]]])

(defn example-app3 []
  (println "Running app3 program in execution mode:")
  
  (let [all-clauses (concat append-clauses app3-clauses)
        queries [[['app3 ['/a] ['/b] ['/c] '*ans]]]
        results (ri/prolog all-clauses queries)]
    
    (println "Query:" (first queries))
    (println "Result:" (first results))))

(defn synthesize-merge3 []
  (println "\nSynthesizing merge3 program from app3 explanation:")
  
  ;; Build explanation for app3
  (let [all-clauses (concat append-clauses app3-clauses)
        goal ['app3 ['/a] ['/b] ['/c] ['/a '/b '/c]]
        explanation (ebg/build-explanation goal all-clauses)
        
        ;; Generalize and decompose explanation
        gen-expl (ebg/generalize-explanation explanation)
        decomposed (ebg/decompose-explanation 
                     gen-expl 
                     comp/composability-values 
                     1) ;; DECOMP_FORCE = 1 for clause-level
        
        ;; Create executable explanation
        exec-expl (ebg/create-executable-explanation (first decomposed))
        
        ;; Synthesize merge3 using the explanation
        target-goal ['merge3 ['/3] ['/1] ['/2] ['/1 '/2 '/3]]
        synthesized (ebg/apply-explanation exec-expl target-goal)]
    
    (println "Target:" target-goal)
    (println "Synthesized program structure:" 
             '[['merge3 *x *y *z *a] [['merge *x *y *aa] ['merge *aa *z *a]]])))

(defn -main [& args]
  (example-app3)
  (synthesize-merge3))
#+end_src

** Tests

#+begin_src clojure :tangle tests/clojure/reversible_interpreter_test.clj
(ns reversible-meta-synthesis.reversible-interpreter-test
  (:require [clojure.test :refer :all]
            [reversible-meta-synthesis.reversible-interpreter :as ri]))

(deftest variable-predicates-test
  (testing "Testing variable predicates"
    (is (ri/variable? '*x) "Symbol starting with * should be a variable")
    (is (not (ri/variable? 'x)) "Regular symbol should not be a variable")
    (is (ri/constant? '/x) "Symbol starting with / should be a constant")
    (is (not (ri/constant? 'x)) "Regular symbol should not be a constant")))

(deftest match-head-test
  (testing "Match pattern against term"
    (is (ri/match-head '*x 'value) "Variable should match any value")
    (is (= {'*x 'value} (ri/match-head '*x 'value)) "Environment should contain binding")
    (is (ri/match-head '/x 'x) "Constant should match its value")
    (is (not (ri/match-head '/x 'y)) "Constant should not match different value")
    (is (ri/match-head ['*x '*y] ['a 'b]) "List with variables should match")
    (is (not (ri/match-head ['*x '*y] ['a])) "Lists of different lengths should not match")))

(deftest eval-goal-test
  (testing "Evaluating goals against clauses"
    (let [clauses [(ri/create-clause ['append [] '*l '*l] [])
                   (ri/create-clause ['append ['*x '& '*l1] '*l2 ['*x '& '*l3]] 
                                     [['append '*l1 '*l2 '*l3]])]]
      
      (is (ri/eval-goal ['append [] ['/a '/b] '*ans] {} clauses) 
          "Base case should match")
      
      (is (= {'*l ['/a '/b] '*ans ['/a '/b]} 
             (ri/eval-goal ['append [] ['/a '/b] '*ans] {} clauses))
          "Environment should contain correct bindings")
      
      (is (ri/eval-goal ['append ['/a] ['/b] '*ans] {} clauses)
          "Recursive case should match")
      
      (is (= {'*ans ['/a '/b]}
             (ri/eval-goal ['append ['/a] ['/b] '*ans] {} clauses))
          "Recursive evaluation should return correct result"))))

(deftest prolog-test
  (testing "Full prolog execution"
    (let [clauses [[['append [] '*l '*l] []]
                   [['append ['*x '& '*l1] '*l2 ['*x '& '*l3]] 
                    [['append '*l1 '*l2 '*l3]]]]
          
          queries [[['append ['/a '/b] ['/c '/d] '*ans]]]
          expected-results [{'*ans ['/a '/b '/c '/d]}]]
      
      (is (= expected-results (ri/prolog clauses queries))
          "Prolog should return correct results for queries"))))

(defn -main []
  (run-tests 'reversible-meta-synthesis.reversible-interpreter-test))
#+end_src

#+begin_src clojure :tangle tests/clojure/ebg_test.clj
(ns reversible-meta-synthesis.ebg-test
  (:require [clojure.test :refer :all]
            [reversible-meta-synthesis.reversible-interpreter :as ri]
            [reversible-meta-synthesis.ebg :as ebg]))

(deftest explanation-tree-test
  (testing "Building explanation trees"
    (let [clauses [(ri/create-clause ['append [] '*l '*l] [])
                   (ri/create-clause ['append ['*x '& '*l1] '*l2 ['*x '& '*l3]] 
                                     [['append '*l1 '*l2 '*l3]])]
          
          goal ['append ['/a] ['/b] ['/a '/b]]
          explanation (ebg/build-explanation goal clauses)]
      
      (is explanation "Should build an explanation")
      (is (= goal (:goal explanation)) "Root goal should match input goal")
      (is (seq (:children explanation)) "Should have child explanations"))))

(deftest generalization-test
  (testing "Generalizing explanations"
    (let [clauses [(ri/create-clause ['append [] '*l '*l] [])
                   (ri/create-clause ['append ['*x '& '*l1] '*l2 ['*x '& '*l3]] 
                                     [['append '*l1 '*l2 '*l3]])]
          
          goal ['append ['/a] ['/b] ['/a '/b]]
          explanation (ebg/build-explanation goal clauses)
          generalized (ebg/generalize-explanation explanation)]
      
      (is generalized "Should generalize the explanation")
      (is (not= goal (:goal generalized)) "Generalized goal should differ from original")
      (is (= ['append ['*GENa] ['*GENb] ['*GENa '*GENb]] (:goal generalized))
          "Constants should be replaced with variables"))))

(deftest decomposition-test
  (testing "Decomposing explanations"
    (let [clauses [(ri/create-clause ['append [] '*l '*l] [])
                   (ri/create-clause ['append ['*x '& '*l1] '*l2 ['*x '& '*l3]] 
                                     [['append '*l1 '*l2 '*l3]])]
          
          goal ['append ['/a '/b] ['/c '/d] ['/a '/b '/c '/d]]
          explanation (ebg/build-explanation goal clauses)
          generalized (ebg/generalize-explanation explanation)
          decomposed (ebg/decompose-explanation 
                       generalized 
                       {'append 1} 
                       1)]
      
      (is (seq decomposed) "Should produce decomposed explanations")
      (is (<= (count decomposed) (count (tree-seq 
                                          #(seq (:children %)) 
                                          :children 
                                          generalized)))
          "Decomposition should not create more nodes than in the original tree"))))

(defn -main []
  (run-tests 'reversible-meta-synthesis.ebg-test))
#+end_src

** Project Configuration

#+begin_src clojure :tangle deps.edn
{:paths ["src/clojure"]
 :deps {org.clojure/clojure {:mvn/version "1.11.1"}}
 
 :aliases
 {:test {:extra-paths ["test/clojure"]
         :extra-deps {org.clojure/test.check {:mvn/version "1.1.1"}}}
  
  :run {:main-opts ["-m" "reversible-meta-synthesis.core"]}
  
  :examples {:extra-paths ["examples/clojure"]
             :extra-deps {}}
  
  :build {:deps {io.github.clojure/tools.build {:git/tag "v0.8.3" :git/sha "0d20256"}}
          :ns-default build}}}
#+end_src

#+begin_src clojure :tangle build.clj
(ns build
  (:require [clojure.tools.build.api :as b]))

(def lib 'reversible-meta-synthesis)
(def version "0.1.0")
(def class-dir "target/classes")
(def basis (b/create-basis {:project "deps.edn"}))
(def uber-file (format "target/%s-%s-standalone.jar" (name lib) version))

(defn clean [_]
  (b/delete {:path "target"}))

(defn uber [_]
  (clean nil)
  (b/copy-dir {:src-dirs ["src" "resources"]
               :target-dir class-dir})
  (b/compile-clj {:basis basis
                 :src-dirs ["src"]
                 :class-dir class-dir})
  (b/uber {:class-dir class-dir
           :uber-file uber-file
           :basis basis
           :main 'reversible-meta-synthesis.core}))
#+end_src

#+begin_src clojure :tangle src/clojure/core.clj
(ns reversible-meta-synthesis.core
  (:require [reversible-meta-synthesis.reversible-interpreter :as ri]
            [reversible-meta-synthesis.ebg :as ebg]
            [reversible-meta-synthesis.composability :as comp])
  (:gen-class))

(defn -main [& args]
  (println "Reversible Meta-Synthesis")
  (println "=======================")
  (println "An implementation of \"Inductive Program Synthesis by Using")
  (println "a Reversible Meta-Interpreter\" by Numao and Shimura")
  (println)
  
  (println "Available commands:")
  (println "  examples  - Run example programs")
  (println "  tests     - Run tests")
  (println "  synthesis - Run program synthesis demos")
  
  (when (seq args)
    (case (first args)
      "examples" (do
                   (require 'reversible-meta-synthesis.examples.append)
                   (apply (resolve 'reversible-meta-synthesis.examples.append/-main) (rest args)))
      "tests" (do
                (require 'reversible-meta-synthesis.reversible-interpreter-test)
                (apply (resolve 'reversible-meta-synthesis.reversible-interpreter-test/-main) (rest args)))
      "synthesis" (do
                    (require 'reversible-meta-synthesis.examples.app3-merge3)
                    (apply (resolve 'reversible-meta-synthesis.examples.app3-merge3/-main) (rest args)))
      (println "Unknown command:" (first args)))))
#+end_src

* Visual Interface

Let's add a simple web interface for visualizing program synthesis:

#+begin_src org :tangle doc/feature_requests/visual_interface.org
#+TITLE: Feature Request: Visual Interface for Program Synthesis
#+AUTHOR: aygp-dr
#+DATE: 2025-03-29

* Visual Interface for Program Synthesis

** Overview

A web-based visual interface would greatly enhance the usability of the reversible meta-synthesis system. This interface would allow users to:

1. Interactively define examples for program synthesis
2. Visualize explanation trees and their decomposition
3. Step through the synthesis process
4. Compare different synthesis approaches

** Key Features

*** Interactive Example Definition
- Input/output pair creation through a visual interface
- Validation of examples for consistency
- Library of predefined examples

*** Explanation Visualization
- Visual representation of explanation trees
- Interactive decomposition with adjustable DECOMP_FORCE
- Highlighting of key structures in explanations

*** Synthesis Process Visualization
- Step-by-step visualization of the synthesis process
- Highlighting of each rule application
- Comparison of intermediate results

*** Implementation Comparison
- Side-by-side comparison of Prolog, Hy, Scheme, and Clojure implementations
- Performance metrics for different approaches
- Visualization of differences in generated programs

** Technology Stack

The interface could be implemented using:
- Frontend: React with D3.js for visualizations
- Backend: Choose one of:
  - Clojure with Ring/Compojure
  - Python Flask (to interface with the Hy implementation)
  - Node.js with js-interop to the various implementations

** Implementation Plan

1. **Phase 1**: Basic interface for defining examples and running synthesis
2. **Phase 2**: Visualization of explanation trees and decomposition
3. **Phase 3**: Step-by-step synthesis process visualization
4. **Phase 4**: Implementation comparison and performance metrics

** Benefits

This visual interface would make the concepts of explanation-based program synthesis more accessible and provide valuable insights into the synthesis process. It would be particularly useful for educational purposes and for researchers exploring new synthesis techniques.
#+end_src

* Integration with Modern Machine Learning

#+begin_src org :tangle doc/feature_requests/ml_integration.org
#+TITLE: Feature Request: Integration with Modern Machine Learning
#+AUTHOR: aygp-dr
#+DATE: 2025-03-29

* Integration with Modern Machine Learning

** Overview

The original paper on reversible meta-interpreters for program synthesis was published in 1990, predating many modern machine learning techniques. Integrating the core concepts with modern machine learning approaches could significantly enhance the system's capabilities and applicability.

** Key Integration Points

*** Neural-Guided Search
- Use neural networks to guide the search for program structures
- Train models to predict which decomposition level will be most effective
- Leverage large language models (LLMs) for suggesting program structures

*** Learning Composability Values
- Use reinforcement learning to learn optimal composability values
- Dynamically adjust composability based on synthesis success
- Develop meta-learning approaches to transfer knowledge between domains

*** Hybrid Neuro-Symbolic Approaches
- Combine symbolic reasoning of the meta-interpreter with neural networks
- Use neural networks for feature extraction from examples
- Apply symbolic reasoning for generating structured programs

*** Large-Scale Example Learning
- Leverage large codebases as sources of examples
- Use contrastive learning to identify similar programming patterns
- Apply transformer models to learn from diverse programming examples

** Implementation Considerations

*** Neural Network Architecture
- Use graph neural networks (GNNs) for processing explanation trees
- Apply transformers for sequence-to-sequence program translation
- Implement variational autoencoders for learning program representations

*** Training Methodology
- Curriculum learning starting with simple programs
- Active learning to identify informative examples
- Few-shot learning to generalize from limited examples

*** Evaluation Metrics
- Program correctness and efficiency
- Synthesis speed compared to purely symbolic approaches
- Generalization to unseen programming patterns

** Potential Applications

- Automated code generation from natural language specifications
- Program repair and optimization
- Domain-specific language development
- Programming assistants for novice programmers

** Research Directions

- Combining explanation-based learning with neural program synthesis
- Transfer learning between different programming languages and paradigms
- Explainable AI through program synthesis
- Meta-learning for program synthesis strategies
#+end_src

* Docker Integration

#+begin_src dockerfile :tangle Dockerfile
FROM openjdk:17-slim as base

# Install required packages
RUN apt-get update && apt-get install -y \
    swi-prolog \
    python3 \
    python3-pip \
    guile-3.0 \
    leiningen \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install Hy
RUN pip3 install hy==1.0.0

# Set up working directory
WORKDIR /app

# Copy project files
COPY . .

# Set up Clojure environment
RUN cd /app && lein deps

# Define environment variables
ENV PATH="/app/bin:${PATH}"

# Create a script to run tests
RUN echo '#!/bin/bash\n\
echo "Running Prolog tests..."\n\
cd /app/tests/prolog && swipl -s run_tests.pl\n\
\n\
echo "Running Hy tests..."\n\
cd /app/tests/hy && hy run_tests.hy\n\
\n\
echo "Running Scheme tests..."\n\
cd /app/tests/scheme && guile run-tests.scm\n\
\n\
echo "Running Clojure tests..."\n\
cd /app && clojure -M:test\n\
' > /app/bin/run-tests && chmod +x /app/bin/run-tests

# Create a script to run examples
RUN echo '#!/bin/bash\n\
echo "Running Prolog examples..."\n\
cd /app/examples/prolog\n\
for f in *.pl; do swipl -q -l "$f" -t "main"; done\n\
\n\
echo "Running Hy examples..."\n\
cd /app/examples/hy\n\
for f in *.hy; do hy "$f"; done\n\
\n\
echo "Running Scheme examples..."\n\
cd /app/examples/scheme\n\
for f in *.scm; do guile "$f"; done\n\
\n\
echo "Running Clojure examples..."\n\
cd /app && clojure -M:examples\n\
' > /app/bin/run-examples && chmod +x /app/bin/run-examples

# Expose port for potential web interface
EXPOSE 8080

# Default command
CMD ["bash"]
#+end_src

#+begin_src shell :tangle docker-compose.yml
version: '3'

services:
  reversible-meta-synthesis:
    build: .
    volumes:
      - .:/app
    ports:
      - "8080:8080"
    command: bash
#+end_src

* Cross-Language Interoperability

#+begin_src org :tangle doc/feature_requests/interoperability.org
#+TITLE: Feature Request: Cross-Language Interoperability
#+AUTHOR: aygp-dr
#+DATE: 2025-03-29

* Cross-Language Interoperability

** Implementation Details

*** Bridge Modules

For each language, create a bridge module that can serialize and deserialize data structures:

#+begin_src prolog :tangle src/prolog/interop_bridge.pl
:- module(interop_bridge, [
    export_program/2,
    import_program/2,
    export_explanation/2,
    import_explanation/2,
    call_external/4
]).
:- use_module(library(http/json)).
:- use_module(library(http/http_client)).

% Convert a Prolog program (list of clauses) to JSON
export_program(Clauses, JSON) :-
    maplist(clause_to_json, Clauses, ClausesJSON),
    atom_json_term(JSON, ClausesJSON, []).

% Convert JSON back to Prolog clauses
import_program(JSON, Clauses) :-
    atom_json_term(JSON, ClausesJSON, []),
    maplist(json_to_clause, ClausesJSON, Clauses).

% Convert a clause to JSON format
clause_to_json((Head :- Body), json([head=HeadJSON, body=BodyJSON])) :-
    term_to_json(Head, HeadJSON),
    goals_to_json(Body, BodyJSON).

% Convert JSON back to a clause
json_to_clause(json([head=HeadJSON, body=BodyJSON]), (Head :- Body)) :-
    json_to_term(HeadJSON, Head),
    json_to_goals(BodyJSON, Body).

% Convert a term to JSON
term_to_json(Term, json([functor=F, args=ArgsJSON])) :-
    Term =.. [F|Args],
    maplist(arg_to_json, Args, ArgsJSON).

% Convert JSON back to a term
json_to_term(json([functor=F, args=ArgsJSON]), Term) :-
    maplist(json_to_arg, ArgsJSON, Args),
    Term =.. [F|Args].

% Convert list of goals to JSON
goals_to_json([], []).
goals_to_json([G|Gs], [JSON|JSONs]) :-
    term_to_json(G, JSON),
    goals_to_json(Gs, JSONs).

% Convert JSON back to list of goals
json_to_goals([], []).
json_to_goals([JSON|JSONs], [G|Gs]) :-
    json_to_term(JSON, G),
    json_to_goals(JSONs, Gs).

% Handle different argument types
arg_to_json(Arg, json([type=var, name=Name])) :-
    var(Arg), !,
    term_to_atom(Arg, Name).
arg_to_json(Arg, json([type=atom, value=Arg])) :-
    atom(Arg), !.
arg_to_json(Arg, json([type=number, value=Arg])) :-
    number(Arg), !.
arg_to_json(Arg, json([type=compound, value=JSON])) :-
    compound(Arg), !,
    term_to_json(Arg, JSON).
arg_to_json(Arg, json([type=list, value=ListJSON])) :-
    is_list(Arg), !,
    maplist(arg_to_json, Arg, ListJSON).

% Convert JSON back to appropriate argument type
json_to_arg(json([type=var, name=Name]), Arg) :-
    atom_to_term(Name, Arg, _).
json_to_arg(json([type=atom, value=Value]), Value).
json_to_arg(json([type=number, value=Value]), Value).
json_to_arg(json([type=compound, value=JSON]), Arg) :-
    json_to_term(JSON, Arg).
json_to_arg(json([type=list, value=ListJSON]), Arg) :-
    maplist(json_to_arg, ListJSON, Arg).

% Export explanation tree
export_explanation(Explanation, JSON) :-
    explanation_to_json(Explanation, ExplanationJSON),
    atom_json_term(JSON, ExplanationJSON, []).

% Import explanation tree
import_explanation(JSON, Explanation) :-
    atom_json_term(JSON, ExplanationJSON, []),
    json_to_explanation(ExplanationJSON, Explanation).

% Convert explanation tree to JSON
explanation_to_json(expl_node(Goal, Children), json([goal=GoalJSON, children=ChildrenJSON])) :-
    term_to_json(Goal, GoalJSON),
    maplist(explanation_to_json, Children, ChildrenJSON).

% Convert JSON back to explanation tree
json_to_explanation(json([goal=GoalJSON, children=ChildrenJSON]), expl_node(Goal, Children)) :-
    json_to_term(GoalJSON, Goal),
    maplist(json_to_explanation, ChildrenJSON, Children).

% Call external implementation (via HTTP)
call_external(Language, Function, Input, Output) :-
    atom_concat('http://localhost:8080/api/', Language, BaseURL),
    atom_concat(BaseURL, '/', URL1),
    atom_concat(URL1, Function, URL),
    http_post(URL, json(Input), Reply, []),
    atom_json_term(Reply, Output, []).
#+end_src

#+begin_src hy :tangle src/hy/interop_bridge.hy
#!/usr/bin/env hy

(import [json])
(import [requests])
(import [src.hy.reversible_interpreter [Interpreter variable? constant?]])
(import [src.hy.ebg [ExplanationNode]])

(defn export-program [clauses]
  "Convert a program (list of clauses) to JSON"
  (defn clause-to-json [[head body]]
    {"head" (term-to-json head)
     "body" (list (map term-to-json body))})
  
  (list (map clause-to-json clauses)))

(defn import-program [json-data]
  "Convert JSON back to a program"
  (defn json-to-clause [clause-json]
    [(json-to-term (get clause-json "head"))
     (list (map json-to-term (get clause-json "body")))])
  
  (list (map json-to-clause json-data)))

(defn term-to-json [term]
  "Convert a term to JSON"
  (cond
    [(variable? term) {"type" "var" "name" (name term)}]
    [(constant? term) {"type" "const" "name" (name term)}]
    [(symbol? term) {"type" "atom" "value" (name term)}]
    [(string? term) {"type" "string" "value" term}]
    [(number? term) {"type" "number" "value" term}]
    [(list? term) {"type" "list" 
                   "value" (list (map term-to-json term))}]
    [True {"type" "unknown"}]))

(defn json-to-term [json-data]
  "Convert JSON back to a term"
  (let [type (get json-data "type")]
    (cond
      [(= type "var") (symbol (+ "*" (get json-data "name")))]
      [(= type "const") (symbol (+ "/" (get json-data "name")))]
      [(= type "atom") (symbol (get json-data "value"))]
      [(= type "string") (get json-data "value")]
      [(= type "number") (get json-data "value")]
      [(= type "list") (list (map json-to-term (get json-data "value")))]
      [True None])))

(defn export-explanation [explanation]
  "Convert an explanation tree to JSON"
  (defn node-to-json [node]
    {"goal" (term-to-json (. node goal))
     "children" (list (map node-to-json (. node children)))})
  
  (node-to-json explanation))

(defn import-explanation [json-data]
  "Convert JSON back to an explanation tree"
  (defn json-to-node [node-json]
    (ExplanationNode
      (json-to-term (get node-json "goal"))
      (list (map json-to-node (get node-json "children")))))
  
  (json-to-node json-data))

(defn call-external [language function input]
  "Call external implementation via HTTP"
  (let [url (+ f"http://localhost:8080/api/{language}/{function}")
        response (.post requests url :json input)]
    (if (= (. response status_code) 200)
        (.json response)
        None)))
#+end_src

#+begin_src scheme :tangle src/scheme/interop-bridge.scm
#!/usr/bin/env scheme

;; Load required libraries
(use-modules (web client)
             (web response)
             (web uri)
             (json))

;; Convert a program to JSON
(define (export-program clauses)
  (map clause->json clauses))

;; Convert JSON back to a program
(define (import-program json-data)
  (map json->clause json-data))

;; Convert a clause to JSON
(define (clause->json clause)
  (let ((head (car clause))
        (body (cadr clause)))
    `(("head" . ,(term->json head))
      ("body" . ,(map term->json body)))))

;; Convert JSON back to a clause
(define (json->clause json-clause)
  (let ((head (assoc-ref json-clause "head"))
        (body (assoc-ref json-clause "body")))
    (list (json->term head)
          (map json->term body))))

;; Convert a term to JSON
(define (term->json term)
  (cond
    ((variable? term)
     `(("type" . "var")
       ("name" . ,(symbol->string term))))
    ((constant? term)
     `(("type" . "const")
       ("name" . ,(substring (symbol->string term) 1))))
    ((symbol? term)
     `(("type" . "atom")
       ("value" . ,(symbol->string term))))
    ((number? term)
     `(("type" . "number")
       ("value" . ,term)))
    ((string? term)
     `(("type" . "string")
       ("value" . ,term)))
    ((list? term)
     `(("type" . "list")
       ("value" . ,(map term->json term))))
    (else
     `(("type" . "unknown")))))

;; Convert JSON back to a term
(define (json->term json-term)
  (let ((type (assoc-ref json-term "type")))
    (cond
      ((string=? type "var")
       (string->symbol 
         (string-append "*" (assoc-ref json-term "name"))))
      ((string=? type "const")
       (string->symbol 
         (string-append "/" (assoc-ref json-term "name"))))
      ((string=? type "atom")
       (string->symbol (assoc-ref json-term "value")))
      ((string=? type "number")
       (assoc-ref json-term "value"))
      ((string=? type "string")
       (assoc-ref json-term "value"))
      ((string=? type "list")
       (map json->term (assoc-ref json-term "value")))
      (else #f))))

;; Helper predicates
(define (variable? x)
  (and (symbol? x) 
       (let ((s (symbol->string x)))
         (and (> (string-length s) 0)
              (char=? (string-ref s 0) #\*)))))

(define (constant? x)
  (and (symbol? x) 
       (let ((s (symbol->string x)))
         (and (> (string-length s) 0)
              (char=? (string-ref s 0) #\/)))))

;; Convert an explanation tree to JSON
(define (export-explanation expl)
  (explanation->json expl))

;; Convert JSON back to an explanation tree
(define (import-explanation json-data)
  (json->explanation json-data))

;; Convert explanation tree to JSON
(define (explanation->json expl)
  (let ((goal (cadr expl))
        (children (caddr expl)))
    `(("goal" . ,(term->json goal))
      ("children" . ,(map explanation->json children)))))

;; Convert JSON back to explanation tree
(define (json->explanation json-expl)
  (let ((goal (assoc-ref json-expl "goal"))
        (children (assoc-ref json-expl "children")))
    (list 'expl-node 
          (json->term goal)
          (map json->explanation children))))

;; Call external implementation via HTTP
(define (call-external language function input)
  (let* ((uri (build-uri 'http
                         #:host "localhost"
                         #:port 8080
                         #:path (string-append "/api/" 
                                               language "/" 
                                               function)))
         (json-data (scm->json-string input))
         (response (http-post uri #:body json-data)))
    (if (= (response-code response) 200)
        (json-string->scm (response-body-port response))
        #f)))

;; Export functions
(define exports
  (list (cons 'export-program export-program)
        (cons 'import-program import-program)
        (cons 'export-explanation export-explanation)
        (cons 'import-explanation import-explanation)
        (cons 'call-external call-external)))
#+end_src

#+begin_src clojure :tangle src/clojure/interop_bridge.clj
(ns reversible-meta-synthesis.interop-bridge
  (:require [clojure.data.json :as json]
            [clj-http.client :as http]
            [reversible-meta-synthesis.reversible-interpreter :as ri]
            [reversible-meta-synthesis.ebg :as ebg]))

;; Convert a program to JSON
(defn export-program [clauses]
  (mapv clause->json clauses))

;; Convert JSON back to a program
(defn import-program [json-data]
  (mapv json->clause json-data))

;; Convert a clause to JSON
(defn clause->json [[head body]]
  {:head (term->json head)
   :body (mapv term->json body)})

;; Convert JSON back to a clause
(defn json->clause [{:keys [head body]}]
  [(json->term head)
   (mapv json->term body)])

;; Convert a term to JSON
(defn term->json [term]
  (cond
    (ri/variable? term) {:type "var" :name (name term)}
    (ri/constant? term) {:type "const" :name (subs (name term) 1)}
    (symbol? term) {:type "atom" :value (name term)}
    (string? term) {:type "string" :value term}
    (number? term) {:type "number" :value term}
    (sequential? term) {:type "list" :value (mapv term->json term)}
    :else {:type "unknown"}))

;; Convert JSON back to a term
(defn json->term [{:keys [type] :as json-term}]
  (case type
    "var" (symbol (str "*" (:name json-term)))
    "const" (symbol (str "/" (:name json-term)))
    "atom" (symbol (:value json-term))
    "string" (:value json-term)
    "number" (:value json-term)
    "list" (mapv json->term (:value json-term))
    nil))

;; Convert an explanation tree to JSON
(defn export-explanation [explanation]
  (explanation->json explanation))

;; Convert JSON back to an explanation tree
(defn import-explanation [json-data]
  (json->explanation json-data))

;; Convert explanation tree to JSON
(defn explanation->json [{:keys [goal children]}]
  {:goal (term->json goal)
   :children (mapv explanation->json children)})

;; Convert JSON back to explanation tree
(defn json->explanation [{:keys [goal children]}]
  (ebg/->ExplanationNode
    (json->term goal)
    (mapv json->explanation children)))

;; Call external implementation via HTTP
(defn call-external [language function input]
  (let [url (str "http://localhost:8080/api/" language "/" function)
        response (http/post url 
                            {:body (json/write-str input)
                             :content-type :json
                             :accept :json})]
    (when (= (:status response) 200)
      (json/read-str (:body response) :key-fn keyword))))
#+end_src

*** REST API Server

Create a central REST API server that acts as the bridge between implementations:

#+begin_src python :tangle src/api/server.py
#!/usr/bin/env python3

from flask import Flask, request, jsonify
import subprocess
import json
import os
import tempfile

app = Flask(__name__)

# Configuration for each language implementation
IMPLEMENTATIONS = {
    "prolog": {
        "execute": ["swipl", "-q", "-t", "main", "-f"],
        "file_ext": ".pl"
    },
    "hy": {
        "execute": ["hy"],
        "file_ext": ".hy"
    },
    "scheme": {
        "execute": ["guile"],
        "file_ext": ".scm"
    },
    "clojure": {
        "execute": ["clojure", "-m"],
        "file_ext": ".clj"
    }
}

@app.route('/api/<language>/<function>', methods=['POST'])
def call_implementation(language, function):
    """
    Call a function in a specific language implementation
    """
    if language not in IMPLEMENTATIONS:
        return jsonify({"error": f"Unsupported language: {language}"}), 400
    
    # Get implementation config
    impl = IMPLEMENTATIONS[language]
    
    # Create temporary file with input data
    input_data = request.json
    
    with tempfile.NamedTemporaryFile(suffix=impl["file_ext"], mode='w', delete=False) as f:
        # Write appropriate code for each language
        if language == "prolog":
            f.write(f"""
:- consult('../../src/prolog/interop_bridge.pl').
:- consult('../../src/prolog/reversible_interpreter.pl').
:- consult('../../src/prolog/ebg.pl').

main :-
    % Parse input JSON
    atom_json_term('{input_json}', Input, []),
    % Call the requested function
    call_function('{function}', Input, Output),
    % Output the result as JSON
    atom_json_term(JSON, Output, []),
    write(JSON),
    halt.

call_function('execute', Input, Output) :-
    % Extract clauses and queries from input
    member(clauses=Clauses, Input),
    member(queries=Queries, Input),
    % Call the interpreter
    prolog(Clauses, Queries, Output).

call_function('synthesize', Input, Output) :-
    % Extract queries and expected values from input
    member(queries=Queries, Input),
    member(values=Values, Input),
    % Call the interpreter in synthesis mode
    prolog(Output, Queries, Values).

call_function('build_explanation', Input, Output) :-
    % Extract goal and clauses from input
    member(goal=Goal, Input),
    member(clauses=Clauses, Input),
    % Build the explanation
    build_explanation(Goal, Clauses, Output).

:- main.
            """.replace("{function}", function).replace("{input_json}", json.dumps(input_data)))
        
        elif language == "hy":
            f.write(f"""
#!/usr/bin/env hy
(import [json])
(import [src.hy.interop_bridge [import-program export-program]])
(import [src.hy.reversible_interpreter [reverse-interpreter]])
(import [src.hy.ebg [ExplanationBuilder]])

(defn call-function [function input-data]
  (cond
    [(= function "execute")
     (let [clauses (import-program (get input-data "clauses"))
           queries (import-program (get input-data "queries"))]
       (reverse-interpreter clauses queries))]
    
    [(= function "synthesize")
     (let [queries (import-program (get input-data "queries"))
           values (get input-data "values")]
       ; Synthesis implementation here
       {"message" "Program synthesis not fully implemented in Hy"})]
    
    [(= function "build_explanation")
     (let [goal (json-to-term (get input-data "goal"))
           clauses (import-program (get input-data "clauses"))
           interpreter (Interpreter)
           _ (for [[head body] clauses]
               (.add-clause interpreter head body))
           builder (ExplanationBuilder interpreter)
           explanation (.build-explanation builder goal)]
       (export-explanation explanation))]
    
    [True {{"error" (+ "Unknown function: " function)}}]))

(print (json.dumps (call-function "{function}" {input_json})))
            """.replace("{function}", function).replace("{input_json}", json.dumps(input_data)))
        
        elif language == "scheme":
            f.write(f"""
#!/usr/bin/env guile
!#

(use-modules (json))
(load "../../src/scheme/interop-bridge.scm")
(load "../../src/scheme/reversible-interpreter.scm")
(load "../../src/scheme/ebg.scm")

(define (call-function function input-data)
  (cond
    ((string=? function "execute")
     (let ((clauses (import-program (assoc-ref input-data "clauses")))
           (queries (import-program (assoc-ref input-data "queries"))))
       (prolog clauses queries)))
    
    ((string=? function "synthesize")
     (let ((queries (import-program (assoc-ref input-data "queries")))
           (values (assoc-ref input-data "values")))
       ;; Synthesis implementation here
       '(("message" . "Program synthesis not fully implemented in Scheme"))))
    
    ((string=? function "build_explanation")
     (let ((goal (json->term (assoc-ref input-data "goal")))
           (clauses (import-program (assoc-ref input-data "clauses"))))
       (export-explanation (build-explanation goal clauses))))
    
    (else
     `(("error" . ,(string-append "Unknown function: " function))))))

(display (scm->json-string (call-function "{function}" '{input_json})))
            """.replace("{function}", function).replace("{input_json}", json.dumps(input_data)))
        
        elif language == "clojure":
            f.write(f"""
(ns interop-bridge-runner
  (:require [clojure.data.json :as json]
            [reversible-meta-synthesis.interop-bridge :as bridge]
            [reversible-meta-synthesis.reversible-interpreter :as ri]
            [reversible-meta-synthesis.ebg :as ebg]))

(defn call-function [function input-data]
  (case function
    "execute"
    (let [clauses (bridge/import-program (:clauses input-data))
          queries (bridge/import-program (:queries input-data))]
      (ri/prolog clauses queries))
    
    "synthesize"
    (let [queries (bridge/import-program (:queries input-data))
          values (:values input-data)]
      ;; Synthesis implementation here
      {:message "Program synthesis not fully implemented in Clojure"})
    
    "build_explanation"
    (let [goal (bridge/json->term (:goal input-data))
          clauses (bridge/import-program (:clauses input-data))
          explanation (ebg/build-explanation goal clauses)]
      (bridge/export-explanation explanation))
    
    {:error (str "Unknown function: " function)}))

(println (json/write-str (call-function "{function}" {input_json})))
            """.replace("{function}", function).replace("{input_json}", json.dumps(input_data)))
        
        f_path = f.name
    
    try:
        # Execute the script
        result = subprocess.run(
            impl["execute"] + [f_path], 
            capture_output=True,
            text=True
        )
        
        # Process output
        if result.returncode == 0:
            try:
                return jsonify(json.loads(result.stdout))
            except json.JSONDecodeError:
                return jsonify({"error": "Invalid JSON output", "output": result.stdout}), 500
        else:
            return jsonify({"error": result.stderr}), 500
    finally:
        # Clean up temp file
        if os.path.exists(f_path):
            os.unlink(f_path)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080)
#+end_src

*** Client Usage Examples

#+begin_src prolog :tangle examples/interop/prolog_client.pl
:- consult('../../src/prolog/interop_bridge.pl').

% Example of executing a program in another language
example_execute_in_hy :-
    % Define the append program
    Program = [
        ([append, [], *l, *l] :- []),
        ([append, [*x | *l1], *l2, [*x|*l3]] :- [[append, *l1, *l2, *l3]])
    ],
    % Define a query
    Query = [[[append, [/a, /b], [/c, /d], *ans]]],
    
    % Export to JSON
    export_program(Program, ProgramJSON),
    export_program(Query, QueryJSON),
    
    % Construct input for Hy implementation
    Input = [clauses=ProgramJSON, queries=QueryJSON],
    
    % Call Hy implementation
    call_external(hy, execute, Input, Output),
    
    % Display result
    write('Result from Hy implementation: '), write(Output), nl.

% Example of building an explanation in another language
example_explanation_in_clojure :-
    % Define the append program
    Program = [
        ([append, [], *l, *l] :- []),
        ([append, [*x | *l1], *l2, [*x|*l3]] :- [[append, *l1, *l2, *l3]])
    ],
    % Define a goal
    Goal = [append, [/a], [/b], [/a, /b]],
    
    % Export to JSON
    export_program(Program, ProgramJSON),
    term_to_json(Goal, GoalJSON),
    
    % Construct input for Clojure implementation
    Input = [goal=GoalJSON, clauses=ProgramJSON],
    
    % Call Clojure implementation
    call_external(clojure, build_explanation, Input, Output),
    
    % Import the explanation
    import_explanation(Output, Explanation),
    
    % Display result
    write('Explanation built by Clojure: '), nl,
    write_explanation(Explanation, 0).

% Helper to display an explanation tree
write_explanation(expl_node(Goal, Children), Indent) :-
    write_indent(Indent),
    write(Goal), nl,
    NewIndent is Indent + 2,
    maplist(write_explanation_child(NewIndent), Children).

write_explanation_child(Indent, Child) :-
    write_explanation(Child, Indent).

write_indent(0) :- !.
write_indent(N) :-
    write(' '),
    N1 is N - 1,
    write_indent(N1).

% Run examples
main :-
    example_execute_in_hy,
    example_explanation_in_clojure.

:- main.
#+end_src

#+begin_src hy :tangle examples/interop/hy_client.hy
#!/usr/bin/env hy

(import [src.hy.interop_bridge [
    export-program 
    import-program
    export-explanation
    import-explanation
    call-external
]])

(defn example-execute-in-prolog []
  (print "Example: Execute in Prolog")
  
  ;; Define the append program
  (setv program [
    [["append" [] "*l" "*l"] []]
    [["append" ["*x" "*l1"] "*l2" ["*x" "*l3"]] 
     [["append" "*l1" "*l2" "*l3"]]]
  ])
  
  ;; Define a query
  (setv query [[["append" ["/a" "/b"] ["/c" "/d"] "*ans"]]])
  
  ;; Export to JSON
  (setv program-json (export-program program))
  (setv query-json (export-program query))
  
  ;; Construct input for Prolog implementation
  (setv input {
    "clauses" program-json
    "queries" query-json
  })
  
  ;; Call Prolog implementation
  (setv output (call-external "prolog" "execute" input))
  
  ;; Display result
  (print "Result from Prolog implementation:")
  (print output))

(defn example-explanation-in-scheme []
  (print "\nExample: Build explanation in Scheme")
  
  ;; Define the append program
  (setv program [
    [["append" [] "*l" "*l"] []]
    [["append" ["*x" "*l1"] "*l2" ["*x" "*l3"]] 
     [["append" "*l1" "*l2" "*l3"]]]
  ])
  
  ;; Define a goal
  (setv goal ["append" ["/a"] ["/b"] ["/a" "/b"]])
  
  ;; Export to JSON
  (setv program-json (export-program program))
  (setv goal-json (term-to-json goal))
  
  ;; Construct input for Scheme implementation
  (setv input {
    "goal" goal-json
    "clauses" program-json
  })
  
  ;; Call Scheme implementation
  (setv output (call-external "scheme" "build_explanation" input))
  
  ;; Import the explanation
  (setv explanation (import-explanation output))
  
  ;; Display result
  (print "Explanation built by Scheme:")
  (write-explanation explanation 0))

;; Helper function to print an explanation tree
(defn write-explanation [node indent]
  (print (.join "" (list (* indent " "))) (. node goal))
  (for [child (. node children)]
    (write-explanation child (+ indent 2))))

;; Helper function for term_to_json (simplified for example)
(defn term-to-json [term]
  (cond
    [(string? term) {"type" "string" "value" term}]
    [(symbol? term) {"type" "atom" "value" (name term)}]
    [(list? term) {"type" "list" 
                   "value" (list (map term-to-json term))}]
    [True {"type" "unknown"}]))

(defmain [&rest args]
  (example-execute-in-prolog)
  (example-explanation-in-scheme))
#+end_src

#+begin_src clojure :tangle examples/interop/clojure_client.clj
(ns reversible-meta-synthesis.examples.interop-client
  (:require [reversible-meta-synthesis.interop-bridge :as bridge]
            [reversible-meta-synthesis.reversible-interpreter :as ri]
            [reversible-meta-synthesis.ebg :as ebg]
            [clojure.pprint :as pp]))

;; Example of executing a program in Scheme
(defn example-execute-in-scheme []
  (println "Example: Execute in Scheme")
  
  ;; Define the append program
  (let [program [['append [] '*l '*l] []]
                [['append ['*x '& '*l1] '*l2 ['*x '& '*l3]] 
                 [['append '*l1 '*l2 '*l3]]]]
        
        ;; Define a query
        query [[['append ['/a '/b] ['/c '/d] '*ans]]]
        
        ;; Export to JSON
        program-json (bridge/export-program program)
        query-json (bridge/export-program query)
        
        ;; Construct input for Scheme implementation
        input {:clauses program-json
               :queries query-json}
        
        ;; Call Scheme implementation
        output (bridge/call-external "scheme" "execute" input)]
    
    ;; Display result
    (println "Result from Scheme implementation:")
    (pp/pprint output)))

;; Example of building an explanation in Hy
(defn example-explanation-in-hy []
  (println "\nExample: Build explanation in Hy")
  
  ;; Define the append program
  (let [program [['append [] '*l '*l] []]
                [['append ['*x '& '*l1] '*l2 ['*x '& '*l3]] 
                 [['append '*l1 '*l2 '*l3]]]]
        
        ;; Define a goal
        goal ['append ['/a] ['/b] ['/a '/b]]
        
        ;; Export to JSON
        program-json (bridge/export-program program)
        goal-json (bridge/term->json goal)
        
        ;; Construct input for Hy implementation
        input {:goal goal-json
               :clauses program-json}
        
        ;; Call Hy implementation
        output (bridge/call-external "hy" "build_explanation" input)
        
        ;; Import the explanation
        explanation (bridge/import-explanation output)]
    
    ;; Display result
    (println "Explanation built by Hy:")
    (write-explanation explanation 0)))

;; Helper function to print an explanation tree
(defn write-explanation [node indent]
  (println (apply str (repeat indent " ")) (:goal node))
  (doseq [child (:children node)]
    (write-explanation child (+ indent 2))))

(defn -main [& args]
  (example-execute-in-scheme)
  (example-explanation-in-hy))
#+end_src

#+begin_src scheme :tangle examples/interop/scheme_client.scm
#!/usr/bin/env guile
!#

(load "../../src/scheme/interop-bridge.scm")

;; Example of executing a program in Clojure
(define (example-execute-in-clojure)
  (display "Example: Execute in Clojure\n")
  
  ;; Define the append program
  (define program
    '(((append () *l *l) ())
      ((append (*x . *l1) *l2 (*x . *l3)) 
       ((append *l1 *l2 *l3)))))
  
  ;; Define a query
  (define query
    '(((append (/a /b) (/c /d) *ans))))
  
  ;; Export to JSON
  (define program-json (export-program program))
  (define query-json (export-program query))
  
  ;; Construct input for Clojure implementation
  (define input
    `(("clauses" . ,program-json)
      ("queries" . ,query-json)))
  
  ;; Call Clojure implementation
  (define output (call-external "clojure" "execute" input))
  
  ;; Display result
  (display "Result from Clojure implementation:\n")
  (display output)
  (newline))

;; Example of building an explanation in Prolog
(define (example-explanation-in-prolog)
  (display "\nExample: Build explanation in Prolog\n")
  
  ;; Define the append program
  (define program
    '(((append () *l *l) ())
      ((append (*x . *l1) *l2 (*x . *l3)) 
       ((append *l1 *l2 *l3)))))
  
  ;; Define a goal
  (define goal '(append (/a) (/b) (/a /b)))
  
  ;; Export to JSON
  (define program-json (export-program program))
  (define goal-json (term->json goal))
  
  ;; Construct input for Prolog implementation
  (define input
    `(("goal" . ,goal-json)
      ("clauses" . ,program-json)))
  
  ;; Call Prolog implementation
  (define output (call-external "prolog" "build_explanation" input))
  
  ;; Import the explanation
  (define explanation (import-explanation output))
  
  ;; Display result
  (display "Explanation built by Prolog:\n")
  (write-explanation explanation 0))

;; Helper function to print an explanation tree
(define (write-explanation node indent)
  (display (make-string indent #\space))
  (display (cadr node))
  (newline)
  (for-each (lambda (child)
              (write-explanation child (+ indent 2)))
            (caddr node)))

;; Run examples
(example-execute-in-clojure)
(example-explanation-in-prolog)
#+end_src

** Integration Tests

Let's add integration tests for cross-language interoperability:

#+begin_src prolog :tangle tests/interop/prolog_interop_test.pl
:- module(prolog_interop_test, [run_tests/0]).

:- use_module(library(plunit)).
:- consult('../../src/prolog/interop_bridge.pl').

:- begin_tests(interop_bridge).

test(export_import_program) :-
    % Define a program
    Program = [
        ([append, [], *l, *l] :- []),
        ([append, [*x | *l1], *l2, [*x|*l3]] :- [[append, *l1, *l2, *l3]])
    ],
    % Export to JSON
    export_program(Program, JSON),
    % Import back
    import_program(JSON, ImportedProgram),
    % Should be equal
    Program = ImportedProgram.

test(export_import_explanation) :-
    % Define an explanation tree
    Explanation = expl_node(
        [append, [/a], [/b], [/a, /b]],
        [expl_node([append, [], [/b], [/b]], [])]
    ),
    % Export to JSON
    export_explanation(Explanation, JSON),
    % Import back
    import_explanation(JSON, ImportedExplanation),
    % Should be equal
    Explanation = ImportedExplanation.

:- end_tests(interop_bridge).

run_tests :-
    run_tests(interop_bridge).
#+end_src

#+begin_src hy :tangle tests/interop/hy_interop_test.hy
#!/usr/bin/env hy

(import [unittest [TestCase main]])
(import [src.hy.interop_bridge [
    export-program 
    import-program
    export-explanation
    import-explanation
    term-to-json
    json-to-term
]])
(import [src.hy.ebg [ExplanationNode]])

(defclass InteropBridgeTests [TestCase]
  (defn test-export-import-program [self]
    "Test that programs can be exported to JSON and imported back"
    (setv program [
      [["append" [] "*l" "*l"] []]
      [["append" ["*x" "*l1"] "*l2" ["*x" "*l3"]] 
       [["append" "*l1" "*l2" "*l3"]]]
    ])
    
    ; Export to JSON
    (setv json-data (export-program program))
    
    ; Import back
    (setv imported-program (import-program json-data))
    
    ; Should match the original
    (.assertEqual self (str program) (str imported-program)))
  
  (defn test-export-import-explanation [self]
    "Test that explanation trees can be exported to JSON and imported back"
    (setv explanation (ExplanationNode
      ["append" ["/a"] ["/b"] ["/a" "/b"]]
      [(ExplanationNode ["append" [] ["/b"] ["/b"]] [])]))
    
    ; Export to JSON
    (setv json-data (export-explanation explanation))
    
    ; Import back
    (setv imported-explanation (import-explanation json-data))
    
    ; Check structure
    (.assertEqual self 
                  (. explanation goal)
                  (. imported-explanation goal))
    (.assertEqual self 
                  (len (. explanation children))
                  (len (. imported-explanation children)))))

(defmain [&rest args]
  (main))
#+end_src

#+begin_src clojure :tangle tests/interop/clojure_interop_test.clj
(ns reversible-meta-synthesis.interop-bridge-test
  (:require [clojure.test :refer :all]
            [reversible-meta-synthesis.interop-bridge :as bridge]
            [reversible-meta-synthesis.ebg :as ebg]))

(deftest export-import-program-test
  (testing "Export and import of programs via JSON"
    (let [program [['append [] '*l '*l] []]
                  ['append ['*x '& '*l1] '*l2 ['*x '& '*l3]] 
                   [['append '*l1 '*l2 '*l3]]]]
          
          ;; Export to JSON
          json-data (bridge/export-program program)
          
          ;; Import back
          imported-program (bridge/import-program json-data)]
      
      ;; Should match the original
      (is (= program imported-program)))))

(deftest export-import-explanation-test
  (testing "Export and import of explanation trees via JSON"
    (let [explanation (ebg/->ExplanationNode
                        ['append ['/a] ['/b] ['/a '/b]]
                        [(ebg/->ExplanationNode 
                           ['append [] ['/b] ['/b]] 
                           [])])
          
          ;; Export to JSON
          json-data (bridge/export-explanation explanation)
          
          ;; Import back
          imported-explanation (bridge/import-explanation json-data)]
      
      ;; Check structure
      (is (= (:goal explanation) (:goal imported-explanation)))
      (is (= (count (:children explanation)) 
             (count (:children imported-explanation)))))))

(defn -main []
  (run-tests 'reversible-meta-synthesis.interop-bridge-test))
#+end_src

#+begin_src scheme :tangle tests/interop/scheme_interop_test.scm
#!/usr/bin/env guile
!#

(load "../../src/scheme/interop-bridge.scm")

;; Simple test framework
(define (assert-equal expected actual message)
  (if (equal? expected actual)
      (begin
        (display "PASS: ")
        (display message)
        (newline))
      (begin
        (display "FAIL: ")
        (display message)
        (newline)
        (display "  Expected: ")
        (display expected)
        (newline)
        (display "  Actual:   ")
        (display actual)
        (newline))))

;; Test export and import of programs
(define (test-export-import-program)
  (let* ((program
           '(((append () *l *l) ())
             ((append (*x . *l1) *l2 (*x . *l3)) 
              ((append *l1 *l2 *l3)))))
         
         ;; Export to JSON
         (json-data (export-program program))
         
         ;; Import back
         (imported-program (import-program json-data)))
    
    ;; Should match the original
    (assert-equal program imported-program
                  "Programs should match after export/import cycle")))

;; Test export and import of explanation trees
(define (test-export-import-explanation)
  (let* ((explanation
           (list 'expl-node
                 '(append (/a) (/b) (/a /b))
                 (list (list 'expl-node
                             '(append () (/b) (/b))
                             '()))))
         
         ;; Export to JSON
         (json-data (export-explanation explanation))
         
         ;; Import back
         (imported-explanation (import-explanation json-data)))
    
    ;; Should match the original
    (assert-equal explanation imported-explanation
                  "Explanation trees should match after export/import cycle")))

;; Run all tests
(define (run-all-tests)
  (display "\nRunning Interop Bridge Tests:\n")
  (test-export-import-program)
  (test-export-import-explanation)
  (display "\nAll tests completed.\n"))

(run-all-tests)
#+end_src

** Running the Interoperability System

Create a script to start the REST API server and test interoperability:

#+begin_src shell :tangle run_interop.sh
#!/bin/bash
set -e

echo "Starting the interoperability REST API server..."
python3 src/api/server.py &
SERVER_PID=$!

# Wait for server to start
sleep 2

echo "Running Prolog interoperability example..."
cd examples/interop
swipl -q -l prolog_client.pl

echo "Running Hy interoperability example..."
hy hy_client.hy

echo "Running Clojure interoperability example..."
cd ../..
clojure -M -m reversible-meta-synthesis.examples.interop-client

echo "Running Scheme interoperability example..."
cd examples/interop
guile scheme_client.scm

echo "Running interoperability tests..."
cd ../../tests/interop
swipl -q -l prolog_interop_test.pl -t "run_tests, halt"
hy hy_interop_test.hy
cd ../..
clojure -M:test -m reversible-meta-synthesis.interop-bridge-test
cd tests/interop
guile scheme_interop_test.scm

# Clean up
echo "Shutting down the server..."
kill $SERVER_PID
echo "Interoperability examples completed successfully!"
#+end_src

** Benefits of Cross-Language Interoperability

Cross-language interoperability offers several advantages for the Reversible Meta-Synthesis project:

1. **Leveraging Language Strengths**: 
   - Prolog excels at logical reasoning and pattern matching
   - Clojure offers a modern Lisp with excellent concurrency features
   - Hy provides access to Python's ecosystem
   - Scheme offers minimalist functional programming

2. **Comparative Analysis**:
   - Compare synthesis approaches across languages
   - Benchmark performance and expressiveness
   - Identify the best implementation for specific synthesis tasks

3. **Unified API**:
   - Single interface for program synthesis across languages
   - Consistent representation of programs and explanations
   - Reduced duplication of effort

4. **Educational Value**:
   - Demonstrate core concepts in multiple paradigms
   - Compare functional and logic programming approaches
   - Teach the underlying theory through different lenses

5. **Extensibility**:
   - Easily add new language implementations
   - Integrate with existing programming environments
   - Leverage domain-specific languages for specialized synthesis

Future work could include adding additional language implementations (such as Haskell, OCaml, or Rust) and extending the interoperability framework to support distributed program synthesis across a network of specialized language implementations.


* Future Work

This section outlines future directions for extending the Reversible Meta-Synthesis project beyond its initial implementation.

** Extended Synthesis Capabilities

*** Higher-Order Synthesis
The current implementation focuses on synthesizing first-order logic programs. A valuable extension would be supporting higher-order program synthesis, where functions can be passed as arguments and returned as results.

#+begin_src prolog :tangle doc/future_work/higher_order_synthesis.pl
% Example of a higher-order synthesis goal
% Synthesize map function from examples

% Desired behavior
example(map(increment, [1, 2, 3], [2, 3, 4])).
example(map(square, [1, 2, 3], [1, 4, 9])).
example(map(double, [1, 2, 3], [2, 4, 6])).

% Helper functions
increment(X, Y) :- Y is X + 1.
square(X, Y) :- Y is X * X.
double(X, Y) :- Y is X * 2.

% Synthesized map predicate would be:
% map(_, [], []).
% map(F, [H1|T1], [H2|T2]) :- call(F, H1, H2), map(F, T1, T2).
#+end_src

*** Constraint-Based Synthesis
Incorporate constraint-based techniques to guide the synthesis process, allowing specification of desired properties rather than just input-output examples.

#+begin_src clojure :tangle doc/future_work/constraint_synthesis.clj
;; Example of constraint-based synthesis
(defn synthesis-with-constraints [examples constraints]
  (loop [candidate-programs (initial-candidates)
         iteration 0]
    (if (or (empty? candidate-programs) (>= iteration max-iterations))
      nil ;; Failed to find solution
      (let [best-candidate (first candidate-programs)]
        (if (and (satisfies-examples? best-candidate examples)
                 (satisfies-constraints? best-candidate constraints))
          best-candidate
          (recur (next-candidates candidate-programs) (inc iteration)))))))

;; Example constraints
(def example-constraints
  {:complexity {:max-depth 3, :max-predicates 2}
   :efficiency {:max-recursive-calls 1}
   :termination {:must-terminate-for-all-inputs true}})
#+end_src

*** Multi-Language Synthesis
Extend the system to synthesize programs in multiple target languages from the same examples, learning to translate between programming paradigms.

#+begin_src org :tangle doc/future_work/multi_language_synthesis.org
#+TITLE: Multi-Language Synthesis
#+AUTHOR: aygp-dr
#+DATE: 2025-03-29

* Multi-Language Synthesis Framework

** Overview
This framework enables synthesizing programs in multiple target languages from the same set of examples, effectively learning to translate between programming paradigms.

** Architecture
- *Source Examples*: Input-output examples in a language-agnostic format
- *Intermediate Representation*: A universal program representation
- *Language-Specific Code Generators*: Translators from IR to target languages
- *Cross-Language Verification*: Ensuring semantic equivalence

** Implementation Steps
1. Define a universal intermediate representation
2. Create bidirectional mappings between IR and each language
3. Implement verification procedures across languages
4. Develop optimization strategies for each target language

** Example Flow
1. User provides examples of desired behavior
2. System synthesizes program in intermediate representation
3. IR is translated to all supported languages
4. Generated programs are verified for correctness
5. Language-specific optimizations are applied
#+end_src

** Enhanced Learning Mechanisms

*** Incremental Learning
Develop a system that improves its synthesis capabilities over time by learning from successful and failed synthesis attempts.

#+begin_src hy :tangle doc/future_work/incremental_learning.hy
#!/usr/bin/env hy

(defclass SynthesisHistory []
  (defn __init__ [self]
    (setv self.successes [])
    (setv self.failures [])
    (setv self.pattern-database {}))
  
  (defn record-success [self examples program]
    (.append self.successes {"examples" examples "program" program})
    (.update-patterns self examples program))
  
  (defn record-failure [self examples]
    (.append self.failures {"examples" examples "timestamp" (.time time)}))
  
  (defn update-patterns [self examples program]
    "Extract patterns from successful synthesis"
    (for [pattern (.extract-patterns self examples program)]
      (.setdefault self.pattern-database (first pattern) [])
      (.append (get self.pattern-database (first pattern)) (second pattern))))
  
  (defn extract-patterns [self examples program]
    "Extract generalizable patterns from examples and synthesized program"
    ;; Implementation would identify recurring structures
    [])
  
  (defn suggest-template [self new-examples]
    "Suggest program templates based on similar past examples"
    (let [similar-examples (.find-similar-examples self new-examples)]
      (when similar-examples
        (.adapt-program self (get similar-examples "program") new-examples)))))
#+end_src

*** Transfer Learning
Implement techniques for transferring knowledge from one synthesis domain to another, allowing the system to leverage similarities between different programming tasks.

#+begin_src scheme :tangle doc/future_work/transfer_learning.scm
;; Transfer Learning for Program Synthesis

;; Extract features from a synthesis problem
(define (extract-features examples)
  (let ((input-types (map input-type examples))
        (output-types (map output-type examples))
        (relationships (map extract-relationships examples)))
    (make-feature-vector input-types output-types relationships)))

;; Measure similarity between synthesis problems
(define (problem-similarity features1 features2)
  (let ((type-similarity (measure-type-similarity 
                           (feature-vector-input-types features1)
                           (feature-vector-input-types features2)))
        (relationship-similarity (measure-relationship-similarity
                                   (feature-vector-relationships features1)
                                   (feature-vector-relationships features2))))
    (weighted-average type-similarity relationship-similarity)))

;; Adapt a synthesized program from one domain to another
(define (adapt-program program source-features target-features)
  (let ((mapping (create-domain-mapping source-features target-features)))
    (transform-program program mapping)))

;; Example of domain mapping from list processing to tree processing
(define list-to-tree-mapping
  '((car . tree-value)
    (cdr . tree-children)
    (null? . tree-leaf?)
    (cons . make-tree-node)))
#+end_src

*** Meta-Synthesis 
Develop a "meta-synthesis" approach where the system synthesizes synthesis strategies themselves, learning which approach works best for different classes of programs.

#+begin_src prolog :tangle doc/future_work/meta_synthesis.pl
% Meta-Synthesis: Synthesizing Synthesis Strategies

% A strategy is a sequence of synthesis steps
:- dynamic strategy/2.  % strategy(ProblemType, Steps)

% Record a successful synthesis
record_synthesis(ProblemType, Examples, Program, Steps) :-
    assert(synthesis_record(ProblemType, Examples, Program, Steps)).

% Extract synthesis patterns from recorded syntheses
extract_strategy_patterns :-
    findall(ProblemType-Steps, synthesis_record(ProblemType, _, _, Steps), Records),
    group_by_problem_type(Records, GroupedRecords),
    maplist(extract_common_strategy, GroupedRecords, Strategies),
    maplist(assert_strategy, Strategies).

% Apply the best strategy for a given problem
synthesize_with_meta_strategy(ProblemType, Examples, Program) :-
    find_best_strategy(ProblemType, Examples, Strategy),
    apply_strategy(Strategy, Examples, Program).

% Find the best strategy based on problem features
find_best_strategy(ProblemType, Examples, Strategy) :-
    extract_problem_features(Examples, Features),
    (strategy(ProblemType, Strategy) -> 
        true 
    ;   similar_problem_type(Features, SimilarType),
        strategy(SimilarType, Strategy)).

% Apply a synthesis strategy
apply_strategy([], Examples, Program) :-
    % Base case: direct synthesis
    synthesize_directly(Examples, Program).
apply_strategy([decompose_problem | Rest], Examples, Program) :-
    % Decompose problem into subproblems
    decompose_examples(Examples, SubExamples),
    maplist(apply_strategy(Rest), SubExamples, SubPrograms),
    compose_programs(SubPrograms, Program).
apply_strategy([use_explanation_level(Level) | Rest], Examples, Program) :-
    % Set explanation decomposition level
    set_decomp_force(Level),
    apply_strategy(Rest, Examples, Program).
#+end_src

** Applications and Integrations

*** IDE Integration
Develop plugins for popular IDEs (VSCode, Emacs, IntelliJ) that integrate program synthesis capabilities directly into the development workflow.

#+begin_src json :tangle doc/future_work/vscode_extension.json
{
  "name": "reversible-meta-synthesis",
  "displayName": "Reversible Meta-Synthesis",
  "description": "Program synthesis through explanation-based learning",
  "version": "0.1.0",
  "engines": {
    "vscode": "^1.70.0"
  },
  "categories": [
    "Programming Languages",
    "Machine Learning",
    "Other"
  ],
  "activationEvents": [
    "onCommand:reversible-meta-synthesis.synthesizeFromExamples",
    "onCommand:reversible-meta-synthesis.synthesizeFromSelection",
    "onCommand:reversible-meta-synthesis.explainSynthesis"
  ],
  "main": "./extension.js",
  "contributes": {
    "commands": [
      {
        "command": "reversible-meta-synthesis.synthesizeFromExamples",
        "title": "Synthesize Program from Examples"
      },
      {
        "command": "reversible-meta-synthesis.synthesizeFromSelection",
        "title": "Synthesize Program from Selected Examples"
      },
      {
        "command": "reversible-meta-synthesis.explainSynthesis",
        "title": "Explain Synthesis Process"
      }
    ],
    "configuration": {
      "title": "Reversible Meta-Synthesis",
      "properties": {
        "reversible-meta-synthesis.defaultLanguage": {
          "type": "string",
          "default": "prolog",
          "enum": ["prolog", "hy", "scheme", "clojure"],
          "description": "Default language for program synthesis"
        },
        "reversible-meta-synthesis.explanationLevel": {
          "type": "number",
          "default": 1,
          "minimum": 0,
          "maximum": 3,
          "description": "Explanation decomposition level (0-3)"
        }
      }
    }
  }
}
#+end_src

*** Natural Language to Code
Integrate natural language processing to enable program synthesis from natural language descriptions combined with examples.

#+begin_src python :tangle doc/future_work/nl_to_code.py
#!/usr/bin/env python3

import openai
import json
from typing import List, Dict, Any, Tuple

class NLToCodeSynthesizer:
    def __init__(self, api_key: str, synthesizer):
        self.llm_client = openai.Client(api_key=api_key)
        self.synthesizer = synthesizer  # The core synthesizer
    
    def synthesize_from_nl(self, description: str, examples: List[Dict[str, Any]]) -> str:
        """Synthesize code from natural language description and examples"""
        # Step 1: Extract formal specification from natural language
        specification = self._extract_specification(description)
        
        # Step 2: Augment examples based on specification
        augmented_examples = self._augment_examples(specification, examples)
        
        # Step 3: Run the synthesizer with augmented examples
        program = self.synthesizer.synthesize(augmented_examples)
        
        # Step 4: Verify the program meets the natural language requirements
        if not self._verify_program(program, description, examples):
            # Refine the program if verification fails
            program = self._refine_program(program, description, examples)
            
        return program
    
    def _extract_specification(self, description: str) -> Dict[str, Any]:
        """Extract formal specification from natural language description"""
        prompt = f"""
        Extract a formal specification from the following natural language description:
        
        {description}
        
        Return a JSON object containing:
        1. Input type and constraints
        2. Output type and constraints
        3. Core functionality description
        4. Edge cases to handle
        """
        
        response = self.llm_client.chat.completions.create(
            model="gpt-4-turbo",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        
        return json.loads(response.choices[0].message.content)
    
    def _augment_examples(self, spec: Dict[str, Any], examples: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Generate additional examples based on specification and existing examples"""
        # Implementation would generate edge cases, corner cases, etc.
        return examples  # Placeholder
    
    def _verify_program(self, program: str, description: str, examples: List[Dict[str, Any]]) -> bool:
        """Verify the synthesized program meets the natural language requirements"""
        # Implementation would execute program on examples and check correctness
        return True  # Placeholder
    
    def _refine_program(self, program: str, description: str, examples: List[Dict[str, Any]]) -> str:
        """Refine program if it doesn't meet requirements"""
        # Implementation would iteratively improve the program
        return program  # Placeholder
#+end_src

*** Interactive Visualization
Create an interactive visualization system for exploring the synthesis process, helping users understand how programs are derived from examples.

#+begin_src javascript :tangle doc/future_work/synthesis_visualizer.js
// Synthesis Process Visualizer

class SynthesisVisualizer {
  constructor(container) {
    this.container = container;
    this.explanationTree = null;
    this.currentDecompLevel = 0;
    this.synthesizedProgram = null;
    
    // Set up SVG container
    this.svg = d3.select(container)
      .append("svg")
      .attr("width", "100%")
      .attr("height", "600px");
      
    // Set up controls
    this.setupControls();
  }
  
  setupControls() {
    const controls = d3.select(this.container)
      .append("div")
      .attr("class", "controls");
      
    controls.append("button")
      .text("Next Step")
      .on("click", () => this.nextStep());
      
    controls.append("button")
      .text("Previous Step")
      .on("click", () => this.previousStep());
      
    controls.append("input")
      .attr("type", "range")
      .attr("min", 0)
      .attr("max", 3)
      .attr("value", this.currentDecompLevel)
      .on("input", (event) => {
        this.currentDecompLevel = +event.target.value;
        this.updateVisualization();
      });
      
    controls.append("span")
      .text("Decomposition Level: ")
      .append("span")
      .attr("class", "decomp-level")
      .text(this.currentDecompLevel);
  }
  
  loadSynthesisData(explanationTree, program) {
    this.explanationTree = explanationTree;
    this.synthesizedProgram = program;
    this.synthesisSteps = this.extractSynthesisSteps(explanationTree);
    this.currentStep = 0;
    this.updateVisualization();
  }
  
  extractSynthesisSteps(tree) {
    // Convert explanation tree into sequential steps
    // Implementation would extract logical progression
    return [];
  }
  
  nextStep() {
    if (this.currentStep < this.synthesisSteps.length - 1) {
      this.currentStep++;
      this.updateVisualization();
    }
  }
  
  previousStep() {
    if (this.currentStep > 0) {
      this.currentStep--;
      this.updateVisualization();
    }
  }
  
  updateVisualization() {
    // Clear current visualization
    this.svg.selectAll("*").remove();
    
    // Update decomposition level display
    d3.select(this.container)
      .select(".decomp-level")
      .text(this.currentDecompLevel);
    
    // Visualize current state
    const step = this.synthesisSteps[this.currentStep];
    const decomposedTree = this.decomposeTree(
      this.explanationTree, 
      this.currentDecompLevel
    );
    
    this.renderExplanationTree(decomposedTree);
    this.renderProgramState(step);
  }
  
  decomposeTree(tree, level) {
    // Apply decomposition at specified level
    // Implementation would use composability values
    return tree;
  }
  
  renderExplanationTree(tree) {
    // Render tree using D3.js
    // Implementation would create interactive tree visualization
  }
  
  renderProgramState(step) {
    // Render current program state
    // Implementation would show program being constructed
  }
}

// Example usage
document.addEventListener("DOMContentLoaded", () => {
  const visualizer = new SynthesisVisualizer("#synthesis-container");
  
  // Fetch example data
  fetch("/api/synthesis-example")
    .then(response => response.json())
    .then(data => {
      visualizer.loadSynthesisData(data.explanationTree, data.program);
    });
});
#+end_src

** Theoretical Extensions

*** Formal Verification
Integrate formal verification techniques to prove correctness of synthesized programs against their specifications.

#+begin_src clojure :tangle doc/future_work/formal_verification.clj
(ns reversible-meta-synthesis.formal-verification
  (:require [clojure.spec.alpha :as s]
            [reversible-meta-synthesis.reversible-interpreter :as ri]))

;; Define specifications for programs
(defn define-program-spec [program input-spec output-spec]
  (s/fdef program
    :args input-spec
    :ret output-spec
    :fn (fn [{:keys [args ret]}]
          ;; Custom logic to relate inputs to outputs
          true)))

;; Verify a synthesized program against examples
(defn verify-synthesized-program [program examples]
  (let [input-types (derive-input-types examples)
        output-types (derive-output-types examples)
        program-spec (define-program-spec program input-types output-types)]
    
    ;; Check program against spec
    (doseq [[input expected-output] examples]
      (let [actual-output (apply program input)]
        (when (not= actual-output expected-output)
          (throw (ex-info "Program does not match example"
                          {:input input
                           :expected expected-output
                           :actual actual-output})))))
    
    ;; Attempt to prove correctness for all inputs
    (proof-search program-spec)))

;; Search for a proof of correctness
(defn proof-search [spec]
  ;; Implementation would use symbolic execution, model checking,
  ;; or other formal verification techniques
  {:verified true
   :assumptions []
   :proof-steps []})

;; Generate test cases to find counterexamples
(defn generate-test-cases [spec]
  ;; Use property-based testing to generate potential counterexamples
  (let [input-gen (s/gen (:args spec))
        test-cases (take 1000 (s/exercise-fn spec))]
    (filter (fn [[input output]]
              (not (s/valid? (:ret spec) output)))
            test-cases)))
#+end_src

*** Probabilistic Program Synthesis
Extend the framework to incorporate probabilistic reasoning, allowing synthesis of programs that handle uncertainty and probabilistic inference.

#+begin_src prolog :tangle doc/future_work/probabilistic_synthesis.pl
% Probabilistic Program Synthesis

:- use_module(library(clpbn)).

% Define a probabilistic program
prob_program(Name, Args, Body, Prob) :-
    prob_clause(Head, Body, Prob),
    Head =.. [Name|Args].

% Execute a probabilistic program with evidence
prob_execute(Program, Input, Output, Evidence, Probability) :-
    % Set up evidence
    maplist(set_evidence, Evidence),
    
    % Execute program
    Program =.. [Name|Args],
    append(Args, [Output], CallArgs),
    Goal =.. [Name|CallArgs],
    
    % Query probability
    clpbn_probability(Goal, Probability).

% Synthesize a probabilistic program from examples with uncertainties
prob_synthesize(Examples, Program) :-
    % Extract examples with uncertainties
    maplist(extract_probability, Examples, ProbExamples),
    
    % Generate candidate programs
    generate_candidates(Candidates),
    
    % Evaluate candidates
    maplist(evaluate_candidate(ProbExamples), Candidates, Scores),
    
    % Select best candidate
    keysort(Scores, SortedScores),
    member(Score-Program, SortedScores),
    Score > threshold.

% Evaluate a candidate program against examples with uncertainties
evaluate_candidate(Examples, Program, Score-Program) :-
    maplist(prob_example_score(Program), Examples, Scores),
    sum_list(Scores, Score).

% Score how well a program matches a probabilistic example
prob_example_score(Program, example(Input, Output, ExpectedProb), Score) :-
    prob_execute(Program, Input, Output, [], ActualProb),
    Score is 1 - abs(ExpectedProb - ActualProb).
#+end_src

*** Program Repair and Adaptation
Develop techniques for repairing incorrect programs and adapting existing programs to new contexts using the reversible meta-interpreter framework.

#+begin_src scheme :tangle doc/future_work/program_repair.scm
;; Program Repair and Adaptation

;; Identify where a program fails
(define (localize-failure program examples)
  (let ((failing-examples (filter (lambda (example)
                                   (not (matches-example? program example)))
                                  examples)))
    (map (lambda (example)
           (find-failure-point program example))
         failing-examples)))

;; Find the specific point of failure in execution
(define (find-failure-point program example)
  (let ((trace (trace-execution program (car example))))
    (find-first-divergence trace (cadr example))))

;; Repair a program by modifying the failing components
(define (repair-program program failure-points examples)
  (let ((modified-programs (generate-repair-candidates program failure-points)))
    (find-first (lambda (candidate)
                  (all-examples-pass? candidate examples))
                modified-programs)))

;; Adapt a program to a new domain
(define (adapt-program program source-domain target-domain)
  (let ((domain-mapping (infer-domain-mapping source-domain target-domain))
        (structure-preserving-transformations (infer-transformations program)))
    (apply-transformations program domain-mapping structure-preserving-transformations)))

;; Generate repair candidates
(define (generate-repair-candidates program failure-points)
  (let ((repair-templates (get-repair-templates)))
    (apply-templates program failure-points repair-templates)))

;; Apply repair templates to a program
(define (apply-templates program failure-points templates)
  (flatten
    (map (lambda (template)
           (map (lambda (point)
                  (apply-template program point template))
                failure-points))
         templates)))

;; Common repair templates
(define repair-templates
  '((replace-constant . (lambda (expr const) (replace-in-expr expr const)))
    (add-condition . (lambda (expr cond) (add-guard expr cond)))
    (change-recursive-case . (lambda (expr new-case) (replace-recursion expr new-case)))
    (generalize-pattern . (lambda (expr pattern) (generalize expr pattern)))))
#+end_src

** Scaling and Performance

*** Parallel Synthesis
Implement parallel synthesis algorithms to handle more complex programs and larger example sets.

#+begin_src python :tangle doc/future_work/parallel_synthesis.py
#!/usr/bin/env python3

import multiprocessing as mp
import itertools
from typing import List, Dict, Any, Callable, Tuple

class ParallelSynthesizer:
    def __init__(self, num_processes: int = None):
        self.num_processes = num_processes or mp.cpu_count()
        
    def synthesize(self, examples: List[Dict[str, Any]]) -> str:
        """Synthesize program in parallel using multiple strategies"""
        with mp.Pool(self.num_processes) as pool:
            # Generate search space divisions
            search_divisions = self._partition_search_space(examples)
            
            # Launch parallel workers
            results = pool.map(self._worker_synthesize, search_divisions)
            
            # Collect and evaluate results
            valid_programs = [prog for prog, score in results if prog]
            
            if not valid_programs:
                return None
                
            # Return the best program
            return self._select_best_program(valid_programs, examples)
    
    def _partition_search_space(self, examples: List[Dict[str, Any]]) -> List[Tuple[Dict, Dict]]:
        """Partition the synthesis search space for parallel exploration"""
        # Strategy 1: Different program templates
        templates = self._generate_program_templates(examples)
        
        # Strategy 2: Different decomposition levels
        decomp_levels = list(range(4))  # 0-3
        
        # Strategy 3: Different subsets of examples
        example_subsets = self._generate_example_subsets(examples)
        
        # Create combinations of strategies
        strategy_space = list(itertools.product(
            templates, 
            decomp_levels, 
            example_subsets
        ))
        
        # Balance workload across processes
        batch_size = max(1, len(strategy_space) // self.num_processes)
        partitions = [
            strategy_space[i:i+batch_size] 
            for i in range(0, len(strategy_space), batch_size)
        ]
        
        return [(examples, {"strategies": partition}) for partition in partitions]
    
    def _worker_synthesize(self, args: Tuple[List[Dict[str, Any]], Dict]) -> Tuple[str, float]:
        """Worker process for synthesis"""
        examples, config = args
        strategies = config["strategies"]
        
        best_program = None
        best_score = float('-inf')
        
        for template, decomp_level, example_subset in strategies:
            # Try synthesis with this strategy
            program = self._synthesize_with_strategy(
                examples, 
                template, 
                decomp_level, 
                example_subset
            )
            
            if program:
                score = self._evaluate_program(program, examples)
                if score > best_score:
                    best_program = program
                    best_score = score
        
        return best_program, best_score
    
    def _synthesize_with_strategy(
        self, 
        examples: List[Dict[str, Any]], 
        template: Dict, 
        decomp_level: int, 
        example_subset: List[int]
    ) -> str:
        """Synthesize using a specific strategy"""
        # Implementation would use the core synthesis algorithm
        # with the specified template, decomposition level, and examples
        return "synthesized_program"  # Placeholder
    
    def _generate_program_templates(self, examples: List[Dict[str, Any]]) -> List[Dict]:
        """Generate candidate program templates based on examples"""
        # Implementation would analyze examples to derive templates
        return [{"template": "recursive"}, {"template": "iterative"}]
    
    def _generate_example_subsets(self, examples: List[Dict[str, Any]]) -> List[List[int]]:
        """Generate subsets of examples for parallel exploration"""
        n = len(examples)
        if n <= 5:
            return [list(range(n))]  # Use all examples if few
        
        # Generate different subsets
        all_indices = list(range(n))
        return [
            all_indices,  # All examples
            all_indices[:n//2],  # First half
            all_indices[n//2:],  # Second half
            all_indices[::2],  # Every other example
            [0, n-1] + all_indices[n//3:2*n//3]  # First, last, and middle third
        ]
    
    def _evaluate_program(self, program: str, examples: List[Dict[str, Any]]) -> float:
        """Evaluate a synthesized program"""
        # Implementation would execute program on examples
        # and calculate a score based on correctness, efficiency, etc.
        return 0.0  # Placeholder
    
    def _select_best_program(self, programs: List[str], examples: List[Dict[str, Any]]) -> str:
        """Select the best program from candidates"""
        scores = [(prog, self._evaluate_program(prog, examples)) for prog in programs]
        return max(scores, key=lambda x: x[1])[0]
#+end_src

*** Cloud-Based Synthesis
Develop a cloud-based service for program synthesis that can handle complex synthesis tasks and provide an API for integration with other tools.

#+begin_src yaml :tangle doc/future_work/cloud_synthesis_architecture.yaml
# Cloud-Based Synthesis Service Architecture

Service:
  name: ReversibleMetaSynthesis
  version: 1.0.0
  description: Cloud-based program synthesis service

Components:
  - name: API Gateway
    description: Entry point for all synthesis requests
    technologies:
      - AWS API Gateway
      - Kong API Gateway
    endpoints:
      - path: /api/v1/synthesize
        method: POST
        description: Submit synthesis job
      - path: /api/v1/status/{jobId}
        method: GET
        description: Check job status
      - path: /api/v1/results/{jobId}
        method: GET
        description: Get synthesis results

  - name: Synthesis Orchestrator
    description: Manages synthesis jobs and workflow
    technologies:
      - AWS Step Functions
      - Temporal
    states:
      - name: ValidateInput
        description: Validate synthesis request
      - name: DetermineStrategy
        description: Select optimal synthesis strategy
      - name: AllocateResources
        description: Provision compute resources
      - name: ExecuteSynthesis
        description: Run the synthesis algorithm
      - name: OptimizeResults
        description: Optimize synthesized program
      - name: ReturnResults
        description: Return results to client

  - name: Synthesis Worker Pool
    description: Pool of workers that perform synthesis
    technologies:
      - AWS Lambda
      - AWS Batch
      - Kubernetes
    configurations:
      - name: StandardWorker
        cpu: 2
        memory: 4GB
        timeout: 5m
      - name: HighMemoryWorker
        cpu: 4
        memory: 16GB
        timeout: 15m
      - name: GPUWorker
        cpu: 8
        memory: 32GB
        gpu: 1
        timeout: 30m

  - name: Knowledge Base
    description: Store of synthesis knowledge and examples
    technologies:
      - MongoDB
      - Neo4j
    collections:
      - name: SynthesisExamples
        description: Repository of example programs
      - name: ExplanationTemplates
        description: Templates for explanation-based synthesis
      - name: UserPrograms
        description: User-submitted programs and examples
      - name: SynthesisStrategies
        description: Learned synthesis strategies

  - name: Observability Stack
    description: Monitoring and logging for synthesis jobs
    technologies:
      - Prometheus
      - Grafana
      - ELK Stack
    metrics:
      - name: SynthesisLatency
        description: Time to complete synthesis
      - name: SynthesisSuccess
        description: Success rate of synthesis attempts
      - name: ResourceUtilization
        description: CPU/Memory/GPU utilization during synthesis
      - name: ProgramComplexity
        description: Complexity metrics of synthesized programs

  - name: Multi-Language Support
    description: Support for multiple programming languages
    technologies:
      - Docker
      - Language Servers
    languages:
      - name: Prolog
        image: reversible-meta-synthesis/prolog:latest
      - name: Hy
        image: reversible-meta-synthesis/hy:latest
      - name: Scheme
        image: reversible-meta-synthesis/scheme:latest
      - name: Clojure
        image: reversible-meta-synthesis/clojure:latest
      - name: Python
        image: reversible-meta-synthesis/python:latest
      - name: Haskell
        image: reversible-meta-synthesis/haskell:latest

API:
  openapi: 3.0.0
  info:
    title: Reversible Meta-Synthesis API
    version: 1.0.0
    description: API for cloud-based program synthesis
  paths:
    /api/v1/synthesize:
      post:
        summary: Submit a program synthesis job
        requestBody:
          content:
            application/json:
              schema:
                type: object
                properties:
                  examples:
                    type: array
                    description: Input-output examples
                  language:
                    type: string
                    description: Target programming language
                  constraints:
                    type: object
                    description: Additional constraints on synthesis
                  explanationLevel:
                    type: integer
                    description: Explanation decomposition level (0-3)
        responses:
          '202':
            description: Synthesis job submitted successfully
            content:
              application/json:
                schema:
                  type: object
                  properties:
                    jobId:
                      type: string
                      description: Unique identifier for the synthesis job
                    estimatedCompletionTime:
                      type: string
                      format: date-time
                      description: Estimated completion time

    /api/v1/status/{jobId}:
      get:
        summary: Get status of a synthesis job
        parameters:
          - name: jobId
            in: path
            required: true
            schema:
              type: string
        responses:
          '200':
            description: Job status retrieved successfully
            content:
              application/json:
                schema:
                  type: object
                  properties:
                    status:
                      type: string
                      enum: [queued, running, completed, failed]
                    progress:
                      type: number
                      description: Progress percentage (0-100)
                    statusMessage:
                      type: string
                      description: Human-readable status message

    /api/v1/results/{jobId}:
      get:
        summary: Get results of a completed synthesis job
        parameters:
          - name: jobId
            in: path
            required: true
            schema:
              type: string
        responses:
          '200':
            description: Results retrieved successfully
            content:
              application/json:
                schema:
                  type: object
                  properties:
                    program:
                      type: string
                      description: Synthesized program
                    explanationTree:
                      type: object
                      description: Explanation tree for the synthesis
                    metrics:
                      type: object
                      description: Performance metrics for the synthesis

Deployment:
  environments:
    - name: Development
      region: us-west-2
      scaling:
        min: 1
        max: 5
    - name: Production
      region: multi-region
      scaling:
        min: 5
        max: 50
  cicd:
    repository: github.com/aygp-dr/reversible-meta-synthesis
    pipeline:
      - name: Build
        steps:
          - name: UnitTests
            description: Run unit tests
          - name: IntegrationTests
            description: Run integration tests
          - name: BuildDockerImages
            description: Build language-specific Docker images
      - name: Deploy
        steps:
          - name: DeployInfrastructure
            description: Deploy cloud infrastructure
          - name: DeployServices
            description: Deploy microservices
          - name: RunSmokeTests
            description: Run smoke tests
#+end_src

#+begin_src typescript :tangle doc/future_work/cloud_synthesis_client.ts
// Client SDK for the Cloud-Based Synthesis Service

export interface SynthesisExample {
  input: any;
  output: any;
}

export interface SynthesisConstraints {
  maxDepth?: number;
  maxPredicates?: number;
  timeLimit?: number;
  memoryLimit?: number;
}

export interface SynthesisRequest {
  examples: SynthesisExample[];
  language: 'prolog' | 'hy' | 'scheme' | 'clojure' | 'python' | 'haskell';
  constraints?: SynthesisConstraints;
  explanationLevel?: number;
}

export interface SynthesisJob {
  jobId: string;
  estimatedCompletionTime: Date;
}

export interface JobStatus {
  status: 'queued' | 'running' | 'completed' | 'failed';
  progress: number;
  statusMessage: string;
}

export interface SynthesisResult {
  program: string;
  explanationTree: any;
  metrics: {
    synthesisTime: number;
    programComplexity: number;
    resourceUsage: {
      cpu: number;
      memory: number;
      gpu?: number;
    };
  };
}

export class SynthesisClient {
  private apiUrl: string;
  private apiKey: string;

  constructor(apiUrl: string, apiKey: string) {
    this.apiUrl = apiUrl;
    this.apiKey = apiKey;
  }

  /**
   * Submit a synthesis job
   */
  async submitSynthesisJob(request: SynthesisRequest): Promise<SynthesisJob> {
    const response = await fetch(`${this.apiUrl}/api/v1/synthesize`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.apiKey}`
      },
      body: JSON.stringify(request)
    });

    if (!response.ok) {
      throw new Error(`Synthesis job submission failed: ${response.statusText}`);
    }

    return await response.json();
  }

  /**
   * Get the status of a synthesis job
   */
  async getJobStatus(jobId: string): Promise<JobStatus> {
    const response = await fetch(`${this.apiUrl}/api/v1/status/${jobId}`, {
      headers: {
        'Authorization': `Bearer ${this.apiKey}`
      }
    });

    if (!response.ok) {
      throw new Error(`Failed to get job status: ${response.statusText}`);
    }

    return await response.json();
  }

  /**
   * Get the results of a completed synthesis job
   */
  async getJobResults(jobId: string): Promise<SynthesisResult> {
    const response = await fetch(`${this.apiUrl}/api/v1/results/${jobId}`, {
      headers: {
        'Authorization': `Bearer ${this.apiKey}`
      }
    });

    if (!response.ok) {
      throw new Error(`Failed to get job results: ${response.statusText}`);
    }

    return await response.json();
  }

  /**
   * Wait for a synthesis job to complete
   */
  async waitForCompletion(
    jobId: string,
    options: {
      pollingInterval?: number;
      timeout?: number;
      onProgress?: (status: JobStatus) => void;
    } = {}
  ): Promise<SynthesisResult> {
    const {
      pollingInterval = 1000,
      timeout = 300000, // 5 minutes
      onProgress
    } = options;

    const startTime = Date.now();
    
    while (Date.now() - startTime < timeout) {
      const status = await this.getJobStatus(jobId);
      
      if (onProgress) {
        onProgress(status);
      }

      if (status.status === 'completed') {
        return await this.getJobResults(jobId);
      }

      if (status.status === 'failed') {
        throw new Error(`Synthesis job failed: ${status.statusMessage}`);
      }

      await new Promise(resolve => setTimeout(resolve, pollingInterval));
    }

    throw new Error('Synthesis job timed out');
  }

  /**
   * Convenience method to submit a job and wait for completion
   */
  async synthesize(
    request: SynthesisRequest,
    options?: {
      pollingInterval?: number;
      timeout?: number;
      onProgress?: (status: JobStatus) => void;
    }
  ): Promise<SynthesisResult> {
    const job = await this.submitSynthesisJob(request);
    return await this.waitForCompletion(job.jobId, options);
  }
}

// Example usage
async function exampleUsage() {
  const client = new SynthesisClient(
    'https://api.reversible-meta-synthesis.com',
    'your-api-key'
  );

  // Define synthesis request
  const request: SynthesisRequest = {
    examples: [
      { input: [1, 2, 3], output: [3, 2, 1] },
      { input: ['a', 'b', 'c'], output: ['c', 'b', 'a'] }
    ],
    language: 'clojure',
    explanationLevel: 1
  };

  try {
    // Option 1: Submit and poll manually
    const job = await client.submitSynthesisJob(request);
    console.log(`Job submitted with ID: ${job.jobId}`);
    console.log(`Estimated completion time: ${job.estimatedCompletionTime}`);

    // Poll for status
    let status = await client.getJobStatus(job.jobId);
    while (status.status !== 'completed' && status.status !== 'failed') {
      console.log(`Job status: ${status.status}, progress: ${status.progress}%`);
      await new Promise(resolve => setTimeout(resolve, 1000));
      status = await client.getJobStatus(job.jobId);
    }

    if (status.status === 'completed') {
      const result = await client.getJobResults(job.jobId);
      console.log('Synthesized program:');
      console.log(result.program);
    } else {
      console.error(`Job failed: ${status.statusMessage}`);
    }

    // Option 2: Use convenience method
    const result = await client.synthesize(request, {
      onProgress: (status) => {
        console.log(`Job status: ${status.status}, progress: ${status.progress}%`);
      }
    });

    console.log('Synthesized program:');
    console.log(result.program);
    console.log('Synthesis time:', result.metrics.synthesisTime, 'ms');
  } catch (error) {
    console.error('Synthesis failed:', error);
  }
}
#+end_src

#+begin_src python :tangle doc/future_work/cloud_synthesis_worker.py
#!/usr/bin/env python3

import os
import sys
import json
import time
import logging
import subprocess
import tempfile
from typing import Dict, List, Any, Tuple, Optional

import boto3
import docker
from prometheus_client import Counter, Gauge, Histogram, start_http_server

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# Metrics
SYNTHESIS_ATTEMPTS = Counter('synthesis_attempts_total', 'Number of synthesis attempts', ['language', 'explanationLevel'])
SYNTHESIS_SUCCESS = Counter('synthesis_success_total', 'Number of successful syntheses', ['language', 'explanationLevel'])
SYNTHESIS_DURATION = Histogram('synthesis_duration_seconds', 'Duration of synthesis', ['language', 'explanationLevel'])
WORKER_MEMORY_USAGE = Gauge('worker_memory_usage_bytes', 'Memory usage of worker')
WORKER_CPU_USAGE = Gauge('worker_cpu_usage_percent', 'CPU usage of worker')

class SynthesisWorker:
    def __init__(self, config_file: str = "worker_config.json"):
        """Initialize the synthesis worker"""
        # Load configuration
        with open(config_file, 'r') as f:
            self.config = json.load(f)
        
        # Set up SQS client for job queue
        self.sqs = boto3.client('sqs')
        self.queue_url = self.config['queue_url']
        
        # Set up S3 client for storing results
        self.s3 = boto3.client('s3')
        self.results_bucket = self.config['results_bucket']
        
        # Set up Docker client for language environments
        self.docker_client = docker.from_env()
        
        # Start metrics server
        start_http_server(self.config.get('metrics_port', 8000))
        
        logger.info(f"Worker initialized with queue {self.queue_url}")
    
    def start(self):
        """Start the worker process"""
        logger.info("Starting synthesis worker")
        
        while True:
            try:
                # Poll for job
                job = self._get_next_job()
                
                if job:
                    # Process job
                    logger.info(f"Processing job {job['jobId']}")
                    self._process_job(job)
                else:
                    # No job available, wait before polling again
                    time.sleep(self.config.get('polling_interval', 5))
            
            except Exception as e:
                logger.exception(f"Error processing job: {e}")
                time.sleep(10)  # Wait longer after error
    
    def _get_next_job(self) -> Optional[Dict[str, Any]]:
        """Get the next job from the queue"""
        response = self.sqs.receive_message(
            QueueUrl=self.queue_url,
            AttributeNames=['All'],
            MaxNumberOfMessages=1,
            WaitTimeSeconds=20
        )
        
        if 'Messages' in response:
            message = response['Messages'][0]
            receipt_handle = message['ReceiptHandle']
            
            try:
                job = json.loads(message['Body'])
                
                # Update job status to running
                self._update_job_status(job['jobId'], 'running', 0, 'Starting synthesis')
                
                # Delete message from queue
                self.sqs.delete_message(
                    QueueUrl=self.queue_url,
                    ReceiptHandle=receipt_handle
                )
                
                return job
            
            except json.JSONDecodeError:
                logger.error(f"Invalid job format: {message['Body']}")
                
                # Delete invalid message
                self.sqs.delete_message(
                    QueueUrl=self.queue_url,
                    ReceiptHandle=receipt_handle
                )
                
                return None
        
        return None
    
    def _process_job(self, job: Dict[str, Any]):
        """Process a synthesis job"""
        job_id = job['jobId']
        language = job['language']
        examples = job['examples']
        explanation_level = job.get('explanationLevel', 1)
        constraints = job.get('constraints', {})
        
        # Track metrics
        SYNTHESIS_ATTEMPTS.labels(language=language, explanationLevel=explanation_level).inc()
        
        try:
            with SYNTHESIS_DURATION.labels(language=language, explanationLevel=explanation_level).time():
                # Update status
                self._update_job_status(job_id, 'running', 10, 'Preparing synthesis environment')
                
                # Prepare input data
                input_data = self._prepare_input_data(job)
                
                # Update status
                self._update_job_status(job_id, 'running', 20, 'Starting synthesis process')
                
                # Run synthesis in appropriate container
                result = self._run_synthesis(language, input_data, explanation_level, constraints)
                
                # Update status
                self._update_job_status(job_id, 'running', 90, 'Finalizing results')
                
                # Store results
                self._store_results(job_id, result)
                
                # Update job status to completed
                self._update_job_status(job_id, 'completed', 100, 'Synthesis completed successfully')
                
                # Track success
                SYNTHESIS_SUCCESS.labels(language=language, explanationLevel=explanation_level).inc()
        
        except Exception as e:
            logger.exception(f"Error processing job {job_id}: {e}")
            
            # Update job status to failed
            self._update_job_status(job_id, 'failed', 0, f"Synthesis failed: {str(e)}")
    
    def _prepare_input_data(self, job: Dict[str, Any]) -> Dict[str, Any]:
        """Prepare input data for synthesis"""
        # Copy relevant job properties
        input_data = {
            'examples': job['examples'],
            'explanationLevel': job.get('explanationLevel', 1),
            'constraints': job.get('constraints', {})
        }
        
        return input_data
    
    def _run_synthesis(
        self, 
        language: str, 
        input_data: Dict[str, Any],
        explanation_level: int,
        constraints: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Run synthesis in the appropriate language container"""
        # Get container image for language
        image_name = self._get_language_image(language)
        
        # Create temporary directory for input/output
        with tempfile.TemporaryDirectory() as temp_dir:
            # Write input data to file
            input_file = os.path.join(temp_dir, 'input.json')
            with open(input_file, 'w') as f:
                json.dump(input_data, f)
            
            # Run container
            logger.info(f"Running synthesis in {image_name} container")
            start_time = time.time()
            
            try:
                container = self.docker_client.containers.run(
                    image_name,
                    command=f"synthesize --input /data/input.json --output /data/output.json --explanation-level {explanation_level}",
                    volumes={temp_dir: {'bind': '/data', 'mode': 'rw'}},
                    environment={
                        'MAX_MEMORY': str(constraints.get('memoryLimit', '4g')),
                        'MAX_CPU': str(constraints.get('cpuLimit', '2')),
                        'TIME_LIMIT': str(constraints.get('timeLimit', '300'))
                    },
                    mem_limit=constraints.get('memoryLimit', '4g'),
                    cpu_quota=int(float(constraints.get('cpuLimit', 2)) * 100000),
                    detach=True
                )
                
                # Monitor container
                self._monitor_container(container)
                
                # Wait for container to finish
                exit_code = container.wait()['StatusCode']
                
                if exit_code != 0:
                    logs = container.logs().decode('utf-8')
                    raise Exception(f"Synthesis failed with exit code {exit_code}: {logs}")
                
                # Read output data
                output_file = os.path.join(temp_dir, 'output.json')
                if os.path.exists(output_file):
                    with open(output_file, 'r') as f:
                        result = json.load(f)
                else:
                    raise Exception("Synthesis completed but no output file was produced")
                
                # Add metrics
                result['metrics'] = {
                    'synthesisTime': time.time() - start_time,
                    'programComplexity': self._calculate_complexity(result['program']),
                    'resourceUsage': self._get_resource_usage(container)
                }
                
                return result
                
            finally:
                # Clean up container
                try:
                    container.remove(force=True)
                except:
                    pass
    
    def _monitor_container(self, container):
        """Monitor container resources and update status"""
        job_id = container.labels.get('job_id')
        
        # Update status every 5 seconds
        progress = 20
        while container.status == 'running':
            try:
                # Get container stats
                stats = container.stats(stream=False)
                
                # Calculate CPU and memory usage
                cpu_usage = self._calculate_cpu_usage(stats)
                memory_usage = self._calculate_memory_usage(stats)
                
                # Update metrics
                WORKER_CPU_USAGE.set(cpu_usage)
                WORKER_MEMORY_USAGE.set(memory_usage)
                
                # Increment progress (simple approach, could be more sophisticated)
                progress = min(progress + 2, 89)  # Max 89% until completion
                
                # Update job status
                if job_id:
                    self._update_job_status(
                        job_id, 
                        'running', 
                        progress, 
                        f"Synthesis in progress - CPU: {cpu_usage:.1f}%, Memory: {memory_usage / 1024 / 1024:.1f}MB"
                    )
                
                time.sleep(5)
                container.reload()  # Refresh container status
                
            except Exception as e:
                logger.warning(f"Error monitoring container: {e}")
                break
    
    def _calculate_cpu_usage(self, stats: Dict[str, Any]) -> float:
        """Calculate CPU usage percentage from container stats"""
        # Implementation depends on Docker stats format
        # This is a simplified version
        try:
            cpu_delta = stats['cpu_stats']['cpu_usage']['total_usage'] - stats['precpu_stats']['cpu_usage']['total_usage']
            system_delta = stats['cpu_stats']['system_cpu_usage'] - stats['precpu_stats']['system_cpu_usage']
            num_cpus = len(stats['cpu_stats']['cpu_usage']['percpu_usage'])
            
            if system_delta > 0:
                return (cpu_delta / system_delta) * num_cpus * 100.0
            return 0.0
        except:
            return 0.0
    
    def _calculate_memory_usage(self, stats: Dict[str, Any]) -> float:
        """Calculate memory usage in bytes from container stats"""
        try:
            return stats['memory_stats']['usage']
        except:
            return 0.0
    
    def _get_resource_usage(self, container) -> Dict[str, float]:
        """Get resource usage statistics from container"""
        try:
            stats = container.stats(stream=False)
            return {
                'cpu': self._calculate_cpu_usage(stats),
                'memory': self._calculate_memory_usage(stats)
            }
        except:
            return {'cpu': 0.0, 'memory': 0.0}
    
    def _calculate_complexity(self, program: str) -> float:
        """Calculate complexity metrics for the synthesized program"""
        # Simple implementation: length + structural complexity estimate
        try:
            lines = program.count('\n') + 1
            tokens = len(program.split())
            nested_depth = program.count('(') - program.count(')') if '(' in program else 0
            
            # Simple weighted formula
            complexity = (0.1 * lines) + (0.05 * tokens) + (0.2 * nested_depth)
            return max(1.0, complexity)
        except:
            return 1.0
    
    def _get_language_image(self, language: str) -> str:
        """Get Docker image name for the specified language"""
        image_map = {
            'prolog': 'reversible-meta-synthesis/prolog:latest',
            'hy': 'reversible-meta-synthesis/hy:latest',
            'scheme': 'reversible-meta-synthesis/scheme:latest',
            'clojure': 'reversible-meta-synthesis/clojure:latest',
            'python': 'reversible-meta-synthesis/python:latest',
            'haskell': 'reversible-meta-synthesis/haskell:latest'
        }
        
        if language not in image_map:
            raise ValueError(f"Unsupported language: {language}")
        
        return image_map[language]
    
    def _store_results(self, job_id: str, result: Dict[str, Any]):
        """Store synthesis results in S3"""
        result_json = json.dumps(result)
        
        self.s3.put_object(
            Bucket=self.results_bucket,
            Key=f"results/{job_id}.json",
            Body=result_json,
            ContentType='application/json'
        )
        
        logger.info(f"Stored results for job {job_id} in S3")
    
    def _update_job_status(self, job_id: str, status: str, progress: int, message: str):
        """Update job status in DynamoDB"""
        # Use boto3 to update DynamoDB
        dynamodb = boto3.resource('dynamodb')
        table = dynamodb.Table(self.config['status_table'])
        
        table.update_item(
            Key={'jobId': job_id},
            UpdateExpression="set #status = :s, progress = :p, statusMessage = :m, updatedAt = :t",
            ExpressionAttributeNames={
                '#status': 'status'
            },
            ExpressionAttributeValues={
                ':s': status,
                ':p': progress,
                ':m': message,
                ':t': int(time.time())
            }
        )
        
        logger.info(f"Updated status for job {job_id}: {status} - {progress}% - {message}")

if __name__ == "__main__":
    # Parse command line arguments
    import argparse
    parser = argparse.ArgumentParser(description='Synthesis Worker')
    parser.add_argument('--config', type=str, default='worker_config.json', help='Path to config file')
    args = parser.parse_args()
    
    # Start worker
    worker = SynthesisWorker(args.config)
    worker.start()
#+end_src

*** Database Design and Optimization

Design specialized database schemas and indexes to optimize storage and retrieval of programs, examples, and explanations.

#+begin_src sql :tangle doc/future_work/synthesis_database.sql
-- Database schema for the Reversible Meta-Synthesis service

-- Users and Authentication
CREATE TABLE users (
    user_id UUID PRIMARY KEY,
    username VARCHAR(50) NOT NULL UNIQUE,
    email VARCHAR(255) NOT NULL UNIQUE,
    password_hash VARCHAR(255) NOT NULL,
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    last_login TIMESTAMP,
    role VARCHAR(20) NOT NULL DEFAULT 'user'
);

CREATE TABLE api_keys (
    key_id UUID PRIMARY KEY,
    user_id UUID NOT NULL REFERENCES users(user_id) ON DELETE CASCADE,
    api_key VARCHAR(64) NOT NULL UNIQUE,
    description VARCHAR(255),
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    expires_at TIMESTAMP,
    last_used TIMESTAMP,
    is_active BOOLEAN NOT NULL DEFAULT TRUE
);

-- Synthesis Jobs
CREATE TABLE synthesis_jobs (
    job_id UUID PRIMARY KEY,
    user_id UUID NOT NULL REFERENCES users(user_id),
    language VARCHAR(20) NOT NULL,
    explanation_level INTEGER NOT NULL DEFAULT 1,
    status VARCHAR(20) NOT NULL,
    progress INTEGER NOT NULL DEFAULT 0,
    status_message TEXT,
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    completed_at TIMESTAMP,
    worker_id VARCHAR(50)
);

-- Job specifications are stored in a separate table due to their size
CREATE TABLE job_specifications (
    job_id UUID PRIMARY KEY REFERENCES synthesis_jobs(job_id) ON DELETE CASCADE,
    examples JSONB NOT NULL,
    constraints JSONB
);

-- Synthesis Results
CREATE TABLE synthesis_results (
    result_id UUID PRIMARY KEY,
    job_id UUID NOT NULL UNIQUE REFERENCES synthesis_jobs(job_id) ON DELETE CASCADE,
    program TEXT NOT NULL,
    explanation_tree JSONB NOT NULL,
    metrics JSONB NOT NULL,
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
);

-- Example Programs & Patterns
CREATE TABLE example_programs (
    program_id UUID PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    description TEXT,
    language VARCHAR(20) NOT NULL,
    program_code TEXT NOT NULL,
    is_public BOOLEAN NOT NULL DEFAULT FALSE,
    user_id UUID REFERENCES users(user_id) ON DELETE SET NULL,
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    tags VARCHAR(50)[] NOT NULL DEFAULT '{}'
);

-- Input-Output Examples
CREATE TABLE io_examples (
    example_id UUID PRIMARY KEY,
    program_id UUID REFERENCES example_programs(program_id) ON DELETE CASCADE,
    input JSONB NOT NULL,
    output JSONB NOT NULL,
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
);

-- Explanation Templates (stored in a specialized structure for quick retrieval)
CREATE TABLE explanation_templates (
    template_id UUID PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    description TEXT,
    language VARCHAR(20) NOT NULL,
    decomposition_level INTEGER NOT NULL,
    pattern JSONB NOT NULL,
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    usage_count INTEGER NOT NULL DEFAULT 0
);

-- Synthesis Strategies (learned from past syntheses)
CREATE TABLE synthesis_strategies (
    strategy_id UUID PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    description TEXT,
    problem_features JSONB NOT NULL, -- Features that trigger this strategy
    steps JSONB NOT NULL, -- Sequence of synthesis steps
    success_rate REAL NOT NULL DEFAULT 0,
    average_synthesis_time INTEGER, -- in milliseconds
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    usage_count INTEGER NOT NULL DEFAULT 0
);

-- Synthesis Usage Analytics
CREATE TABLE synthesis_analytics (
    analytics_id UUID PRIMARY KEY,
    job_id UUID REFERENCES synthesis_jobs(job_id) ON DELETE SET NULL,
    user_id UUID REFERENCES users(user_id) ON DELETE SET NULL,
    language VARCHAR(20) NOT NULL,
    explanation_level INTEGER NOT NULL,
    synthesis_time INTEGER NOT NULL, -- in milliseconds
    cpu_usage REAL NOT NULL, -- percentage
    memory_usage BIGINT NOT NULL, -- in bytes
    program_complexity REAL NOT NULL,
    strategy_id UUID REFERENCES synthesis_strategies(strategy_id) ON DELETE SET NULL,
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
);

-- Knowledge Base for Program Patterns
CREATE TABLE program_patterns (
    pattern_id UUID PRIMARY KEY,
    language VARCHAR(20) NOT NULL,
    pattern_type VARCHAR(50) NOT NULL, -- e.g., recursive, iterative, higher-order
    pattern_structure JSONB NOT NULL, -- structure of the pattern
    description TEXT,
    usage_count INTEGER NOT NULL DEFAULT 0,
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
);

-- Program Pattern Examples
CREATE TABLE pattern_examples (
    example_id UUID PRIMARY KEY,
    pattern_id UUID REFERENCES program_patterns(pattern_id) ON DELETE CASCADE,
    program_code TEXT NOT NULL,
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
);

-- Indexes for performance optimization

-- For job lookup and filtering
CREATE INDEX idx_synthesis_jobs_user_id ON synthesis_jobs(user_id);
CREATE INDEX idx_synthesis_jobs_status ON synthesis_jobs(status);
CREATE INDEX idx_synthesis_jobs_language ON synthesis_jobs(language);
CREATE INDEX idx_synthesis_jobs_created_at ON synthesis_jobs(created_at);

-- For example programs
CREATE INDEX idx_example_programs_language ON example_programs(language);
CREATE INDEX idx_example_programs_is_public ON example_programs(is_public);
CREATE INDEX idx_example_programs_user_id ON example_programs(user_id);
CREATE INDEX idx_example_programs_tags ON example_programs USING GIN(tags);

-- For explanation templates
CREATE INDEX idx_explanation_templates_language ON explanation_templates(language);
CREATE INDEX idx_explanation_templates_decomposition_level ON explanation_templates(decomposition_level);

-- For synthesis strategies
CREATE INDEX idx_synthesis_strategies_problem_features ON synthesis_strategies USING GIN(problem_features jsonb_path_ops);

-- For analytics
CREATE INDEX idx_synthesis_analytics_user_id ON synthesis_analytics(user_id);
CREATE INDEX idx_synthesis_analytics_language ON synthesis_analytics(language);
CREATE INDEX idx_synthesis_analytics_created_at ON synthesis_analytics(created_at);

-- Full-text search for program code and descriptions
CREATE EXTENSION IF NOT EXISTS pg_trgm;
CREATE INDEX idx_example_programs_code_search ON example_programs USING GIN(program_code gin_trgm_ops);
CREATE INDEX idx_example_programs_description_search ON example_programs USING GIN(description gin_trgm_ops);

-- Specialized Views

-- View for active synthesis jobs
CREATE VIEW active_synthesis_jobs AS
SELECT sj.job_id, sj.user_id, u.username, sj.language, sj.explanation_level, 
       sj.status, sj.progress, sj.status_message, sj.created_at, sj.worker_id
FROM synthesis_jobs sj
JOIN users u ON sj.user_id = u.user_id
WHERE sj.status IN ('queued', 'running')
ORDER BY sj.created_at;

-- View for synthesis performance analytics
CREATE VIEW synthesis_performance AS
SELECT 
    language,
    explanation_level,
    COUNT(*) as job_count,
    AVG(synthesis_time) as avg_synthesis_time,
    MIN(synthesis_time) as min_synthesis_time,
    MAX(synthesis_time) as max_synthesis_time,
    AVG(cpu_usage) as avg_cpu_usage,
    AVG(memory_usage) as avg_memory_usage,
    AVG(program_complexity) as avg_program_complexity,
    COUNT(CASE WHEN sa.strategy_id IS NOT NULL THEN 1 END) as strategy_usage_count
FROM 
    synthesis_analytics sa
GROUP BY 
    language, explanation_level;

-- View for user synthesis history
CREATE VIEW user_synthesis_history AS
SELECT 
    u.user_id,
    u.username,
    COUNT(sj.job_id) as total_jobs,
    COUNT(CASE WHEN sj.status = 'completed' THEN 1 END) as completed_jobs,
    COUNT(CASE WHEN sj.status = 'failed' THEN 1 END) as failed_jobs,
    AVG(sa.synthesis_time) as avg_synthesis_time,
    MAX(sj.created_at) as last_synthesis
FROM 
    users u
LEFT JOIN 
    synthesis_jobs sj ON u.user_id = sj.user_id
LEFT JOIN 
    synthesis_analytics sa ON sj.job_id = sa.job_id
GROUP BY 
    u.user_id, u.username;

-- Functions and Procedures

-- Function to calculate program complexity
CREATE OR REPLACE FUNCTION calculate_program_complexity(program_code TEXT)
RETURNS REAL AS $$
DECLARE
    line_count INTEGER;
    token_count INTEGER;
    nested_depth INTEGER;
    complexity REAL;
BEGIN
    -- Count lines
    line_count := array_length(string_to_array(program_code, E'\n'), 1);
    
    -- Count tokens (simplistic approach)
    token_count := array_length(regexp_split_to_array(program_code, E'\\s+'), 1);
    
    -- Calculate nesting depth (simplistic approach for lisp-like languages)
    nested_depth := length(program_code) - length(replace(program_code, '(', ''));
    
    -- Calculate complexity with weights
    complexity := (0.1 * line_count) + (0.05 * token_count) + (0.2 * nested_depth);
    
    RETURN GREATEST(1.0, complexity);
END;
$$ LANGUAGE plpgsql;

-- Procedure to clean up old jobs
CREATE OR REPLACE PROCEDURE cleanup_old_jobs(retention_days INTEGER)
LANGUAGE plpgsql AS $$
BEGIN
    -- Archive completed jobs
    INSERT INTO archived_synthesis_jobs
    SELECT * FROM synthesis_jobs
    WHERE status IN ('completed', 'failed')
    AND created_at < NOW() - (retention_days || ' days')::INTERVAL;
    
    -- Delete from main table
    DELETE FROM synthesis_jobs
    WHERE status IN ('completed', 'failed')
    AND created_at < NOW() - (retention_days || ' days')::INTERVAL;
    
    COMMIT;
END;
$$;

-- Trigger to update metrics when a synthesis job completes
CREATE OR REPLACE FUNCTION update_strategy_metrics()
RETURNS TRIGGER AS $$
BEGIN
    IF NEW.strategy_id IS NOT NULL THEN
        -- Update strategy success rate and performance metrics
        UPDATE synthesis_strategies
        SET 
            usage_count = usage_count + 1,
            success_rate = (success_rate * usage_count + CASE WHEN NEW.job_id IN (SELECT job_id FROM synthesis_jobs WHERE status = 'completed') THEN 1.0 ELSE 0.0 END) / (usage_count + 1),
            average_synthesis_time = (average_synthesis_time * usage_count + NEW.synthesis_time) / (usage_count + 1),
            updated_at = NOW()
        WHERE 
            strategy_id = NEW.strategy_id;
    END IF;
    
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER trigger_update_strategy_metrics
AFTER INSERT ON synthesis_analytics
FOR EACH ROW
EXECUTE FUNCTION update_strategy_metrics();

-- Function to find similar synthesis problems
CREATE OR REPLACE FUNCTION find_similar_problems(
    p_input_features JSONB,
    p_language VARCHAR,
    p_limit INTEGER DEFAULT 5
)
RETURNS TABLE (
    job_id UUID,
    similarity REAL,
    examples JSONB,
    program TEXT
) AS $$
BEGIN
    RETURN QUERY
    SELECT 
        sj.job_id,
        -- Calculate similarity between problem features
        -- This is a simplified approach; in reality would use more sophisticated similarity metrics
        (jsonb_array_length(p_input_features) - jsonb_array_length(p_input_features - js.examples)) / 
        jsonb_array_length(p_input_features)::REAL AS similarity,
        js.examples,
        sr.program
    FROM 
        synthesis_jobs sj
    JOIN 
        job_specifications js ON sj.job_id = js.job_id
    JOIN 
        synthesis_results sr ON sj.job_id = sr.job_id
    WHERE 
        sj.language = p_language
        AND sj.status = 'completed'
    ORDER BY 
        similarity DESC
    LIMIT p_limit;
END;
$$ LANGUAGE plpgsql;
#+end_src

#+begin_src python :tangle doc/future_work/database_optimizations.py
#!/usr/bin/env python3
"""
Database optimization techniques for the Reversible Meta-Synthesis service.
This module provides specialized functions for working with synthesis data
efficiently in the database.
"""

import json
import uuid
import psycopg2
from psycopg2.extras import Json, DictCursor
from typing import Dict, List, Any, Tuple, Optional
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

class SynthesisDatabase:
    def __init__(self, connection_string: str):
        """Initialize the database connection"""
        self.conn_string = connection_string
        
    def connect(self):
        """Create a database connection"""
        return psycopg2.connect(
            self.conn_string,
            cursor_factory=DictCursor
        )
    
    def store_program_with_examples(self, 
                                   program_code: str, 
                                   language: str,
                                   examples: List[Dict[str, Any]],
                                   name: str,
                                   description: str = None,
                                   user_id: uuid.UUID = None,
                                   is_public: bool = False,
                                   tags: List[str] = None) -> uuid.UUID:
        """
        Store a program with its examples in the database
        
        Args:
            program_code: The source code of the program
            language: Programming language (prolog, hy, scheme, clojure, etc.)
            examples: List of input-output examples
            name: Name of the program
            description: Description of the program
            user_id: ID of the user who created the program
            is_public: Whether the program is publicly accessible
            tags: List of tags to categorize the program
            
        Returns:
            UUID of the created program
        """
        program_id = uuid.uuid4()
        
        with self.connect() as conn:
            with conn.cursor() as cur:
                # Insert the program
                cur.execute(
                    """
                    INSERT INTO example_programs (
                        program_id, name, description, language, program_code, 
                        is_public, user_id, tags
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                    RETURNING program_id
                    """,
                    (
                        program_id, name, description, language, program_code,
                        is_public, user_id, tags or []
                    )
                )
                
                # Insert examples
                for example in examples:
                    cur.execute(
                        """
                        INSERT INTO io_examples (
                            example_id, program_id, input, output
                        ) VALUES (%s, %s, %s, %s)
                        """,
                        (
                            uuid.uuid4(), program_id, 
                            Json(example['input']), Json(example['output'])
                        )
                    )
                
                # Extract program patterns
                self._extract_and_store_patterns(cur, program_code, language, program_id)
                
                conn.commit()
                
        return program_id
    
    def _extract_and_store_patterns(self, 
                                   cursor, 
                                   program_code: str, 
                                   language: str,
                                   program_id: uuid.UUID) -> None:
        """
        Extract patterns from a program and store them in the database
        
        Args:
            cursor: Database cursor
            program_code: The source code of the program
            language: Programming language
            program_id: ID of the program
        """
        # This is a simplified implementation
        # In practice, would use language-specific parsers and pattern recognition
        
        # Determine pattern type based on simple heuristics
        pattern_type = self._determine_pattern_type(program_code, language)
        
        # Extract pattern structure
        pattern_structure = self._extract_pattern_structure(program_code, language)
        
        # Check if similar pattern exists
        cursor.execute(
            """
            SELECT pattern_id, usage_count 
            FROM program_patterns
            WHERE language = %s AND pattern_type = %s
            """,
            (language, pattern_type)
        )
        
        existing_pattern = cursor.fetchone()
        
        if existing_pattern:
            # Update existing pattern
            pattern_id = existing_pattern['pattern_id']
            cursor.execute(
                """
                UPDATE program_patterns
                SET usage_count = usage_count + 1,
                    updated_at = NOW()
                WHERE pattern_id = %s
                """,
                (pattern_id,)
            )
        else:
            # Create new pattern
            pattern_id = uuid.uuid4()
            cursor.execute(
                """
                INSERT INTO program_patterns (
                    pattern_id, language, pattern_type, 
                    pattern_structure, usage_count
                ) VALUES (%s, %s, %s, %s, 1)
                """,
                (
                    pattern_id, language, pattern_type,
                    Json(pattern_structure)
                )
            )
        
        # Store pattern example
        cursor.execute(
            """
            INSERT INTO pattern_examples (
                example_id, pattern_id, program_code
            ) VALUES (%s, %s, %s)
            """,
            (uuid.uuid4(), pattern_id, program_code)
        )
    
    def _determine_pattern_type(self, program_code: str, language: str) -> str:
        """
        Determine the pattern type of a program
        
        Args:
            program_code: The source code of the program
            language: Programming language
            
        Returns:
            Pattern type (e.g., recursive, iterative, higher-order)
        """
        # Simplified implementation based on simple heuristics
        # Would use language-specific parsers in practice
        
        if language in ('prolog', 'hy', 'scheme', 'clojure'):
            # Check for recursion patterns in lisp-like languages
            if program_code.count('(defun ') > 0 or program_code.count('(defn ') > 0:
                if program_code.count(' :- ') > 0 and program_code.count(program_code.split(' :- ')[0].strip()) > 1:
                    return 'recursive'
            
            # Check for higher-order functions
            higher_order_indicators = ['map', 'filter', 'reduce', 'apply', 'foldl', 'foldr']
            if any(indicator in program_code for indicator in higher_order_indicators):
                return 'higher-order'
            
            return 'simple'
        else:
            # Default for unknown languages
            return 'unknown'
    
    def _extract_pattern_structure(self, program_code: str, language: str) -> Dict[str, Any]:
        """
        Extract the structure of a program pattern
        
        Args:
            program_code: The source code of the program
            language: Programming language
            
        Returns:
            Structure of the pattern as a JSON-serializable dictionary
        """
        # Simplified implementation
        # Would use language-specific parsers in practice
        
        # Count occurrences of key syntax elements
        structure = {
            "length": len(program_code),
            "lines": program_code.count('\n') + 1,
            "functions": program_code.count('defun') + program_code.count('defn'),
            "recursion": program_code.count('self') + program_code.count(' :- '),
            "conditionals": program_code.count('if') + program_code.count('cond'),
            "loops": program_code.count('for') + program_code.count('while'),
            "higher_order": program_code.count('map') + program_code.count('filter') + program_code.count('reduce')
        }
        
        return structure
    
    def find_similar_programs(self, 
                             query_code: str, 
                             language: str,
                             limit: int = 5) -> List[Dict[str, Any]]:
        """
        Find programs similar to the given code using TF-IDF and cosine similarity
        
        Args:
            query_code: Program code to find similar programs for
            language: Programming language
            limit: Maximum number of results to return
            
        Returns:
            List of similar programs with similarity scores
        """
        with self.connect() as conn:
            with conn.cursor() as cur:
                # Get all programs in the same language
                cur.execute(
                    """
                    SELECT program_id, name, program_code
                    FROM example_programs
                    WHERE language = %s
                    """,
                    (language,)
                )
                
                programs = cur.fetchall()
                
                if not programs:
                    return []
                
                # Create corpus for TF-IDF
                corpus = [program['program_code'] for program in programs]
                corpus.append(query_code)  # Add query code to the end
                
                # Create TF-IDF vectorizer
                vectorizer = TfidfVectorizer(
                    analyzer='word',
                    token_pattern=r'[^\s]+',  # Treat program code tokens appropriately
                    min_df=1
                )
                
                # Calculate TF-IDF matrix
                tfidf_matrix = vectorizer.fit_transform(corpus)
                
                # Calculate cosine similarity between query and all programs
                query_index = len(corpus) - 1  # Query is the last item
                cosine_similarities = cosine_similarity(
                    tfidf_matrix[query_index:query_index+1], 
                    tfidf_matrix[:query_index]
                ).flatten()
                
                # Create result list with program IDs and similarity scores
                similar_programs = []
                for idx, similarity in enumerate(cosine_similarities):
                    if similarity > 0.1:  # Threshold to filter irrelevant results
                        program = programs[idx]
                        similar_programs.append({
                            "program_id": program['program_id'],
                            "name": program['name'],
                            "similarity": float(similarity),
                            "program_code": program['program_code']
                        })
                
                # Sort by similarity (descending) and limit results
                similar_programs.sort(key=lambda x: x['similarity'], reverse=True)
                return similar_programs[:limit]
    
    def store_explanation_template(self, 
                                  name: str,
                                  description: str,
                                  language: str,
                                  decomposition_level: int,
                                  pattern: Dict[str, Any]) -> uuid.UUID:
        """
        Store an explanation template in the database
        
        Args:
            name: Name of the template
            description: Description of the template
            language: Programming language
            decomposition_level: Explanation decomposition level (0-3)
            pattern: Pattern structure as a JSON-serializable dictionary
            
        Returns:
            UUID of the created template
        """
        template_id = uuid.uuid4()
        
        with self.connect() as conn:
            with conn.cursor() as cur:
                cur.execute(
                    """
                    INSERT INTO explanation_templates (
                        template_id, name, description, language,
                        decomposition_level, pattern
                    ) VALUES (%s, %s, %s, %s, %s, %s)
                    RETURNING template_id
                    """,
                    (
                        template_id, name, description, language,
                        decomposition_level, Json(pattern)
                    )
                )
                
                conn.commit()
                
        return template_id
    
    def find_matching_explanation_template(self,
                                         language: str,
                                         decomposition_level: int,
                                         goal_pattern: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Find a matching explanation template based on the goal pattern
        
        Args:
            language: Programming language
            decomposition_level: Explanation decomposition level (0-3)
            goal_pattern: Pattern to match
            
        Returns:
            Matching template or None if no match found
        """
        with self.connect() as conn:
            with conn.cursor() as cur:
                # Get templates matching language and decomposition level
                cur.execute(
                    """
                    SELECT template_id, name, description, pattern
                    FROM explanation_templates
                    WHERE language = %s AND decomposition_level = %s
                    ORDER BY usage_count DESC
                    """,
                    (language, decomposition_level)
                )
                
                templates = cur.fetchall()
                
                # Find best matching template using pattern similarity
                best_match = None
                best_similarity = 0.0
                
                for template in templates:
                    similarity = self._calculate_pattern_similarity(
                        goal_pattern, 
                        template['pattern']
                    )
                    
                    if similarity > best_similarity and similarity > 0.7:  # Threshold
                        best_similarity = similarity
                        best_match = {
                            "template_id": template['template_id'],
                            "name": template['name'],
                            "description": template['description'],
                            "pattern": template['pattern'],
                            "similarity": similarity
                        }
                
                # Update usage count for matching template
                if best_match:
                    cur.execute(
                        """
                        UPDATE explanation_templates
                        SET usage_count = usage_count + 1
                        WHERE template_id = %s
                        """,
                        (best_match['template_id'],)
                    )
                    
                    conn.commit()
                
                return best_match
    
    def _calculate_pattern_similarity(self, 
                                     pattern1: Dict[str, Any], 
                                     pattern2: Dict[str, Any]) -> float:
        """
        Calculate similarity between two patterns
        
        Args:
            pattern1: First pattern
            pattern2: Second pattern
            
        Returns:
            Similarity score between 0.0 and 1.0
        """
        # Simplified implementation
        # Would use more sophisticated similarity metrics in practice
        
        # Find common keys
        common_keys = set(pattern1.keys()) & set(pattern2.keys())
        
        if not common_keys:
            return 0.0
        
        # Calculate similarity for each common key
        similarities = []
        for key in common_keys:
            if isinstance(pattern1[key], (int, float)) and isinstance(pattern2[key], (int, float)):
                # Numerical features - calculate relative difference
                max_val = max(abs(pattern1[key]), abs(pattern2[key]))
                if max_val > 0:
                    diff = abs(pattern1[key] - pattern2[key]) / max_val
                    similarities.append(1.0 - min(diff, 1.0))
                else:
                    similarities.append(1.0)  # Both zero
            elif isinstance(pattern1[key], str) and isinstance(pattern2[key], str):
                # String features - calculate string similarity
                if pattern1[key] == pattern2[key]:
                    similarities.append(1.0)
                else:
                    similarities.append(0.0)
            elif isinstance(pattern1[key], (list, tuple)) and isinstance(pattern2[key], (list, tuple)):
                # List features - calculate overlap
                set1 = set(pattern1[key])
                set2 = set(pattern2[key])
                if set1 or set2:
                    overlap = len(set1 & set2) / len(set1 | set2)
                    similarities.append(overlap)
                else:
                    similarities.append(1.0)  # Both empty
            elif isinstance(pattern1[key], dict) and isinstance(pattern2[key], dict):
                # Nested dictionaries - recursive similarity
                similarities.append(self._calculate_pattern_similarity(pattern1[key], pattern2[key]))
            else:
                # Different types
                similarities.append(0.0)
        
        # Return average similarity
        return sum(similarities) / len(similarities) if similarities else 0.0
    
    def get_synthesis_statistics(self) -> Dict[str, Any]:
        """
        Get overall synthesis statistics
        
        Returns:
            Dictionary with synthesis statistics
        """
        with self.connect() as conn:
            with conn.cursor() as cur:
                # Get overall counts
                cur.execute(
                    """
                    SELECT
                        (SELECT COUNT(*) FROM synthesis_jobs) AS total_jobs,
                        (SELECT COUNT(*) FROM synthesis_jobs WHERE status = 'completed') AS completed_jobs,
                        (SELECT COUNT(*) FROM synthesis_jobs WHERE status = 'failed') AS failed_jobs,
                        (SELECT COUNT(*) FROM users) AS total_users,
                        (SELECT COUNT(*) FROM example_programs) AS total_programs,
                        (SELECT COUNT(*) FROM explanation_templates) AS total_templates,
                        (SELECT COUNT(*) FROM program_patterns) AS total_patterns
                    """
                )
                
                stats = cur.fetchone()
                
                # Get synthesis performance by language
                cur.execute(
                    """
                    SELECT 
                        language,
                        COUNT(*) as job_count,
                        AVG(synthesis_time) as avg_synthesis_time,
                        MIN(synthesis_time) as min_synthesis_time,
                        MAX(synthesis_time) as max_synthesis_time,
                        AVG(program_complexity) as avg_program_complexity
                    FROM 
                        synthesis_analytics
                    GROUP BY 
                        language
                    """
                )
                
                language_stats = cur.fetchall()
                
                # Get synthesis performance by explanation level
                cur.execute(
                    """
                    SELECT 
                        explanation_level,
                        COUNT(*) as job_count,
                        AVG(synthesis_time) as avg_synthesis_time,
                        AVG(program_complexity) as avg_program_complexity
                    FROM 
                        synthesis_analytics
                    GROUP BY 
                        explanation_level
                    ORDER BY
                        explanation_level
                    """
                )
                
                level_stats = cur.fetchall()
                
                # Return combined statistics
                return {
                    "overall": dict(stats),
                    "by_language": [dict(ls) for ls in language_stats],
                    "by_explanation_level": [dict(ls) for ls in level_stats]
                }
    
    def optimize_database(self) -> None:
        """
        Perform database optimization operations
        """
        with self.connect() as conn:
            with conn.cursor() as cur:
                # Clean up old jobs
                cur.execute("CALL cleanup_old_jobs(30)")
                
                # Analyze tables for query optimization
                cur.execute("ANALYZE")
                
                # Vacuum to reclaim space
                conn.autocommit = True
                cur.execute("VACUUM ANALYZE")
                
                # Update database statistics
                cur.execute("VACUUM FULL")
    
    def create_database_partitions(self) -> None:
        """
        Create partitions for large tables to improve performance
        """
        with self.connect() as conn:
            with conn.cursor() as cur:
                # Create partitioned table for synthesis analytics by date
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS synthesis_analytics_partitioned (
                        analytics_id UUID NOT NULL,
                        job_id UUID,
                        user_id UUID,
                        language VARCHAR(20) NOT NULL,
                        explanation_level INTEGER NOT NULL,
                        synthesis_time INTEGER NOT NULL,
                        cpu_usage REAL NOT NULL,
                        memory_usage BIGINT NOT NULL,
                        program_complexity REAL NOT NULL,
                        strategy_id UUID,
                        created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
                    ) PARTITION BY RANGE (created_at);
                """)
                
                # Create monthly partitions for the past year and future year
                # In practice, would dynamically create these based on actual data
                for i in range(-12, 13):  # -12 months to +12 months
                    month_start = f"DATE_TRUNC('month', CURRENT_DATE + INTERVAL '{i} month')"
                    month_end = f"DATE_TRUNC('month', CURRENT_DATE + INTERVAL '{i+1} month')"
                    
                    partition_name = f"synthesis_analytics_y{datetime.now().year}_m{(datetime.now().month + i) % 12 + 1}"
                    
                    # Create partition
                    cur.execute(f"""
                        CREATE TABLE IF NOT EXISTS {partition_name} PARTITION OF synthesis_analytics_partitioned
                        FOR VALUES FROM ({month_start}) TO ({month_end});
                    """)
                    
                    # Create indexes on the partition
                    cur.execute(f"""
                        CREATE INDEX IF NOT EXISTS idx_{partition_name}_job_id ON {partition_name}(job_id);
                        CREATE INDEX IF NOT EXISTS idx_{partition_name}_user_id ON {partition_name}(user_id);
                        CREATE INDEX IF NOT EXISTS idx_{partition_name}_language ON {partition_name}(language);
                    """)
                
                conn.commit()
    
    def implement_database_caching(self) -> None:
        """
        Implement database caching for frequently accessed data
        """
        with self.connect() as conn:
            with conn.cursor() as cur:
                # Create materialized view for frequently accessed synthesis statistics
                cur.execute("""
                    CREATE MATERIALIZED VIEW IF NOT EXISTS cached_synthesis_stats AS
                    SELECT 
                        language,
                        explanation_level,
                        COUNT(*) as job_count,
                        AVG(synthesis_time) as avg_synthesis_time,
                        MIN(synthesis_time) as min_synthesis_time,
                        MAX(synthesis_time) as max_synthesis_time,
                        AVG(cpu_usage) as avg_cpu_usage,
                        AVG(memory_usage) as avg_memory_usage,
                        AVG(program_complexity) as avg_program_complexity,
                        COUNT(CASE WHEN sa.strategy_id IS NOT NULL THEN 1 END) as strategy_usage_count
                    FROM 
                        synthesis_analytics sa
                    GROUP BY 
                        language, explanation_level;
                """)
                
                # Create index on the materialized view
                cur.execute("""
                    CREATE INDEX IF NOT EXISTS idx_cached_synthesis_stats_language
                    ON cached_synthesis_stats(language);
                    
                    CREATE INDEX IF NOT EXISTS idx_cached_synthesis_stats_explanation_level
                    ON cached_synthesis_stats(explanation_level);
                """)
                
                # Create function to refresh the materialized view
                cur.execute("""
                    CREATE OR REPLACE FUNCTION refresh_cached_synthesis_stats()
                    RETURNS TRIGGER AS $$
                    BEGIN
                        REFRESH MATERIALIZED VIEW CONCURRENTLY cached_synthesis_stats;
                        RETURN NULL;
                    END;
                    $$ LANGUAGE plpgsql;
                """)
                
                # Create trigger to refresh the materialized view
                cur.execute("""
                    DROP TRIGGER IF EXISTS trigger_refresh_cached_synthesis_stats ON synthesis_analytics;
                    
                    CREATE TRIGGER trigger_refresh_cached_synthesis_stats
                    AFTER INSERT OR UPDATE OR DELETE ON synthesis_analytics
                    FOR EACH STATEMENT
                    EXECUTE FUNCTION refresh_cached_synthesis_stats();
                """)
                
                conn.commit()
    
    def optimize_query_performance(self) -> None:
        """
        Optimize query performance for common operations
        """
        with self.connect() as conn:
            with conn.cursor() as cur:
                # Create specialized indexes for common queries
                
                # Index for finding programs by example inputs/outputs
                cur.execute("""
                    CREATE INDEX IF NOT EXISTS idx_io_examples_input_jsonb
                    ON io_examples USING GIN(input jsonb_path_ops);
                    
                    CREATE INDEX IF NOT EXISTS idx_io_examples_output_jsonb
                    ON io_examples USING GIN(output jsonb_path_ops);
                """)
                
                # Hash index for exact matches on constant values
                cur.execute("""
                    CREATE INDEX IF NOT EXISTS idx_example_programs_language_hash
                    ON example_programs USING HASH(language);
                """)
                
                # Create function index for program complexity
                cur.execute("""
                    CREATE INDEX IF NOT EXISTS idx_example_programs_complexity
                    ON example_programs(calculate_program_complexity(program_code));
                """)
                
                # Create index for jsonb containment operations
                cur.execute("""
                    CREATE INDEX IF NOT EXISTS idx_synthesis_strategies_features_containment
                    ON synthesis_strategies USING GIN(problem_features jsonb_path_ops);
                """)
                
                conn.commit()
    
    def implement_data_retention_policies(self) -> None:
        """
        Implement data retention policies to manage database size
        """
        with self.connect() as conn:
            with conn.cursor() as cur:
                # Create table for archived jobs
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS archived_synthesis_jobs (
                        job_id UUID PRIMARY KEY,
                        user_id UUID NOT NULL,
                        language VARCHAR(20) NOT NULL,
                        explanation_level INTEGER NOT NULL DEFAULT 1,
                        status VARCHAR(20) NOT NULL,
                        progress INTEGER NOT NULL DEFAULT 0,
                        status_message TEXT,
                        created_at TIMESTAMP NOT NULL,
                        updated_at TIMESTAMP NOT NULL,
                        completed_at TIMESTAMP,
                        worker_id VARCHAR(50),
                        archived_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
                    );
                """)
                
                # Create table for archived results (compressed)
                cur.execute("""
                    CREATE TABLE IF NOT EXISTS archived_synthesis_results (
                        result_id UUID PRIMARY KEY,
                        job_id UUID NOT NULL,
                        program TEXT NOT NULL,
                        explanation_tree JSONB NOT NULL,
                        metrics JSONB NOT NULL,
                        created_at TIMESTAMP NOT NULL,
                        archived_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                        compressed_data BYTEA
                    );
                """)
                
                # Create functions to compress and decompress data
                cur.execute("""
                    CREATE OR REPLACE FUNCTION compress_synthesis_result(
                        p_result_id UUID,
                        p_job_id UUID,
                        p_program TEXT,
                        p_explanation_tree JSONB,
                        p_metrics JSONB,
                        p_created_at TIMESTAMP
                    ) RETURNS BYTEA AS $$
                    DECLARE
                        v_data TEXT;
                        v_compressed BYTEA;
                    BEGIN
                        -- Serialize data as JSON
                        v_data := json_build_object(
                            'result_id', p_result_id,
                            'job_id', p_job_id,
                            'program', p_program,
                            'explanation_tree', p_explanation_tree,
                            'metrics', p_metrics,
                            'created_at', p_created_at
                        )::text;
                        
                        -- Compress using GZIP
                        v_compressed := COMPRESS(v_data::bytea, 'gzip');
                        
                        RETURN v_compressed;
                    END;
                    $$ LANGUAGE plpgsql;
                """)
                
                cur.execute("""
                    CREATE OR REPLACE FUNCTION decompress_synthesis_result(
                        p_compressed_data BYTEA
                    ) RETURNS TABLE (
                        result_id UUID,
                        job_id UUID,
                        program TEXT,
                        explanation_tree JSONB,
                        metrics JSONB,
                        created_at TIMESTAMP
                    ) AS $$
                    DECLARE
                        v_decompressed TEXT;
                        v_json JSONB;
                    BEGIN
                        -- Decompress data
                        v_decompressed := CONVERT_FROM(DECOMPRESS(p_compressed_data, 'gzip'), 'UTF8');
                        
                        -- Parse JSON
                        v_json := v_decompressed::jsonb;
                        
                        -- Return data
                        RETURN QUERY SELECT
                            (v_json->>'result_id')::UUID,
                            (v_json->>'job_id')::UUID,
                            v_json->>'program',
                            v_json->'explanation_tree',
                            v_json->'metrics',
                            (v_json->>'created_at')::TIMESTAMP;
                    END;
                    $$ LANGUAGE plpgsql;
                """)
                
                # Create stored procedure for archiving old data
                cur.execute("""
                    CREATE OR REPLACE PROCEDURE archive_old_synthesis_data(
                        p_retention_days INTEGER
                    ) AS $$
                    DECLARE
                        v_cutoff_date TIMESTAMP;
                        v_job_rec RECORD;
                        v_compressed_data BYTEA;
                    BEGIN
                        -- Calculate cutoff date
                        v_cutoff_date := CURRENT_TIMESTAMP - (p_retention_days || ' days')::INTERVAL;
                        
                        -- Find completed jobs older than cutoff date
                        FOR v_job_rec IN (
                            SELECT j.job_id, j.user_id, j.language, j.explanation_level,
                                   j.status, j.progress, j.status_message, j.created_at,
                                   j.updated_at, j.completed_at, j.worker_id,
                                   r.result_id, r.program, r.explanation_tree, r.metrics, r.created_at as result_created_at
                            FROM synthesis_jobs j
                            JOIN synthesis_results r ON j.job_id = r.job_id
                            WHERE j.status IN ('completed', 'failed')
                            AND j.created_at < v_cutoff_date
                        ) LOOP
                            -- Compress result data
                            v_compressed_data := compress_synthesis_result(
                                v_job_rec.result_id,
                                v_job_rec.job_id,
                                v_job_rec.program,
                                v_job_rec.explanation_tree,
                                v_job_rec.metrics,
                                v_job_rec.result_created_at
                            );
                            
                            -- Insert into archived tables
                            INSERT INTO archived_synthesis_jobs (
                                job_id, user_id, language, explanation_level, status,
                                progress, status_message, created_at, updated_at,
                                completed_at, worker_id
                            ) VALUES (
                                v_job_rec.job_id, v_job_rec.user_id, v_job_rec.language,
                                v_job_rec.explanation_level, v_job_rec.status, v_job_rec.progress,
                                v_job_rec.status_message, v_job_rec.created_at,
                                v_job_rec.updated_at, v_job_rec.completed_at, v_job_rec.worker_id
                            );
                            
                            INSERT INTO archived_synthesis_results (
                                result_id, job_id, program, explanation_tree, metrics,
                                created_at, compressed_data
                            ) VALUES (
                                v_job_rec.result_id, v_job_rec.job_id, v_job_rec.program,
                                v_job_rec.explanation_tree, v_job_rec.metrics,
                                v_job_rec.result_created_at, v_compressed_data
                            );
                            
                            -- Delete from original tables
                            DELETE FROM synthesis_results WHERE result_id = v_job_rec.result_id;
                            DELETE FROM synthesis_jobs WHERE job_id = v_job_rec.job_id;
                        END LOOP;
                        
                        COMMIT;
                    END;
                    $$ LANGUAGE plpgsql;
                """)
                
                conn.commit()
    
    def create_distributed_indices(self) -> None:
        """
        Create distributed indices for large-scale deployments
        """
        # This would typically be implemented using PostgreSQL extensions
        # such as Citus or using a distributed database system
        
        with self.connect() as conn:
            with conn.cursor() as cur:
                # Check if Citus extension is available
                cur.execute("SELECT EXISTS(SELECT 1 FROM pg_available_extensions WHERE name = 'citus')")
                has_citus = cur.fetchone()[0]
                
                if has_citus:
                    # Create distributed tables
                    cur.execute("CREATE EXTENSION IF NOT EXISTS citus")
                    
                    # Distribute the synthesis_jobs table
                    cur.execute("""
                        SELECT create_distributed_table(
                            'synthesis_jobs', 'user_id'
                        );
                    """)
                    
                    # Distribute the synthesis_results table
                    cur.execute("""
                        SELECT create_distributed_table(
                            'synthesis_results', 'job_id'
                        );
                    """)
                    
                    # Distribute the example_programs table
                    cur.execute("""
                        SELECT create_distributed_table(
                            'example_programs', 'user_id'
                        );
                    """)
                    
                    conn.commit()
                else:
                    print("Citus extension not available for distributed indices")
    
    def create_search_indices(self) -> None:
        """
        Create specialized search indices for program code and descriptions
        """
        with self.connect() as conn:
            with conn.cursor() as cur:
                # Check if PostgreSQL full-text search extensions are available
                cur.execute("SELECT EXISTS(SELECT 1 FROM pg_available_extensions WHERE name = 'pg_trgm')")
                has_trgm = cur.fetchone()[0]
                
                if has_trgm:
                    # Create trigram indices for fuzzy text search
                    cur.execute("CREATE EXTENSION IF NOT EXISTS pg_trgm")
                    
                    # Create GIN index for program code search
                    cur.execute("""
                        CREATE INDEX IF NOT EXISTS idx_example_programs_code_search
                        ON example_programs USING GIN(program_code gin_trgm_ops);
                    """)
                    
                    # Create GIN index for description search
                    cur.execute("""
                        CREATE INDEX IF NOT EXISTS idx_example_programs_description_search
                        ON example_programs USING GIN(description gin_trgm_ops);
                    """)
                    
                    conn.commit()
                
                # Create full-text search vectors
                cur.execute("""
                    ALTER TABLE example_programs
                    ADD COLUMN IF NOT EXISTS search_vector tsvector
                    GENERATED ALWAYS AS (
                        setweight(to_tsvector('english', coalesce(name, '')), 'A') ||
                        setweight(to_tsvector('english', coalesce(description, '')), 'B') ||
                        setweight(to_tsvector('english', coalesce(program_code, '')), 'C')
                    ) STORED;
                """)
                
                # Create GIN index for full-text search
                cur.execute("""
                    CREATE INDEX IF NOT EXISTS idx_example_programs_search_vector
                    ON example_programs USING GIN(search_vector);
                """)
                
                conn.commit()
    
    def search_programs(self, 
                      query: str, 
                      language: str = None,
                      tags: List[str] = None,
                      limit: int = 10) -> List[Dict[str, Any]]:
        """
        Search for programs using full-text search
        
        Args:
            query: Search query
            language: Filter by programming language
            tags: Filter by tags
            limit: Maximum number of results
            
        Returns:
            List of matching programs
        """
        with self.connect() as conn:
            with conn.cursor() as cur:
                # Build the SQL query
                sql = """
                    SELECT 
                        p.program_id, 
                        p.name, 
                        p.description, 
                        p.language, 
                        p.tags,
                        p.is_public,
                        ts_rank(p.search_vector, to_tsquery('english', %s)) AS rank
                    FROM 
                        example_programs p
                    WHERE 
                        p.search_vector @@ to_tsquery('english', %s)
                """
                
                params = []
                
                # Preprocess the query for full-text search
                processed_query = ' & '.join(query.split())
                params.extend([processed_query, processed_query])
                
                # Add language filter if specified
                if language:
                    sql += " AND p.language = %s"
                    params.append(language)
                
                # Add tags filter if specified
                if tags and len(tags) > 0:
                    placeholders = ', '.join(['%s' for _ in tags])
                    sql += f" AND p.tags && ARRAY[{placeholders}]"
                    params.extend(tags)
                
                # Add public filter (always include public programs)
                sql += " AND p.is_public = TRUE"
                
                # Order by rank and limit results
                sql += " ORDER BY rank DESC LIMIT %s"
                params.append(limit)
                
                # Execute the query
                cur.execute(sql, params)
                
                # Return results
                results = []
                for row in cur.fetchall():
                    results.append({
                        "program_id": row['program_id'],
                        "name": row['name'],
                        "description": row['description'],
                        "language": row['language'],
                        "tags": row['tags'],
                        "rank": float(row['rank'])
                    })
                
                return results
#+end_src


#+begin_src python :tangle doc/future_work/database_metrics.py
#!/usr/bin/env python3
"""
Database metrics collection and monitoring for the Reversible Meta-Synthesis service.
"""

import time
import logging
import psycopg2
from psycopg2.extras import DictCursor
import prometheus_client as prometheus
from prometheus_client import Gauge, Counter, Histogram, Summary
from typing import Dict, List, Optional
import threading

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s'
)
logger = logging.getLogger("db-metrics")

class DatabaseMetricsCollector:
    """
    Collects and exposes metrics about the synthesis database performance.
    """
    
    def __init__(self, conn_string: str, poll_interval: int = 60):
        """
        Initialize the metrics collector
        
        Args:
            conn_string: PostgreSQL connection string
            poll_interval: Interval in seconds to poll for metrics
        """
        self.conn_string = conn_string
        self.poll_interval = poll_interval
        self.running = False
        self.poll_thread = None
        
        # Initialize Prometheus metrics
        
        # Database performance metrics
        self.db_size = Gauge(
            'synthesis_db_size_bytes',
            'Total size of the synthesis database in bytes',
            ['database']
        )
        
        self.table_sizes = Gauge(
            'synthesis_table_size_bytes',
            'Size of each table in the synthesis database',
            ['table', 'database']
        )
        
        self.index_sizes = Gauge(
            'synthesis_index_size_bytes',
            'Size of each index in the synthesis database',
            ['index', 'table', 'database']
        )
        
        self.connection_count = Gauge(
            'synthesis_db_connections',
            'Number of active database connections',
            ['database', 'state']
        )
        
        self.transaction_count = Counter(
            'synthesis_db_transactions_total',
            'Total number of database transactions',
            ['database', 'type']
        )
        
        self.query_duration = Histogram(
            'synthesis_db_query_duration_seconds',
            'Duration of database queries',
            ['database', 'table', 'operation'],
            buckets=(0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0)
        )
        
        self.cache_hit_ratio = Gauge(
            'synthesis_db_cache_hit_ratio',
            'Database cache hit ratio',
            ['database', 'type']
        )
        
        # Synthesis-specific metrics
        self.synthesis_job_count = Gauge(
            'synthesis_jobs_count',
            'Number of synthesis jobs',
            ['status']
        )
        
        self.synthesis_job_duration = Histogram(
            'synthesis_job_duration_seconds',
            'Duration of synthesis jobs',
            ['language', 'explanation_level'],
            buckets=(1.0, 5.0, 10.0, 30.0, 60.0, 120.0, 300.0, 600.0, 1800.0, 3600.0)
        )
        
        self.program_complexity = Gauge(
            'synthesis_program_complexity',
            'Complexity of synthesized programs',
            ['language', 'explanation_level']
        )
        
        self.explanation_tree_size = Gauge(
            'synthesis_explanation_tree_size',
            'Size of explanation trees in nodes',
            ['language', 'explanation_level']
        )
        
        # Replication metrics
        self.replication_lag = Gauge(
            'synthesis_db_replication_lag_seconds',
            'Replication lag in seconds',
            ['database', 'replica']
        )
        
        # Start the metrics server
        prometheus.start_http_server(8000)
    
    def start(self):
        """Start collecting metrics"""
        if self.running:
            logger.warning("Metrics collector is already running")
            return
        
        self.running = True
        self.poll_thread = threading.Thread(target=self._metrics_loop)
        self.poll_thread.daemon = True
        self.poll_thread.start()
        logger.info("Database metrics collector started")
    
    def stop(self):
        """Stop collecting metrics"""
        self.running = False
        if self.poll_thread:
            self.poll_thread.join(timeout=10)
            logger.info("Database metrics collector stopped")
    
    def _metrics_loop(self):
        """Main metrics collection loop"""
        while self.running:
            try:
                # Collect all metrics
                self._collect_database_size_metrics()
                self._collect_connection_metrics()
                self._collect_performance_metrics()
                self._collect_synthesis_metrics()
                self._collect_replication_metrics()
                
                # Log successful collection
                logger.debug("Collected database metrics successfully")
            
            except Exception as e:
                logger.error(f"Error collecting database metrics: {e}", exc_info=True)
            
            # Wait for next collection interval
            time.sleep(self.poll_interval)
    
    def connect(self):
        """Create a database connection"""
        return psycopg2.connect(
            self.conn_string,
            cursor_factory=DictCursor
        )
    
    def _collect_database_size_metrics(self):
        """Collect metrics about database size"""
        with self.connect() as conn:
            with conn.cursor() as cur:
                # Get database name
                cur.execute("SELECT current_database()")
                db_name = cur.fetchone()[0]
                
                # Get total database size
                cur.execute("""
                    SELECT pg_database_size(current_database())
                """)
                db_size = cur.fetchone()[0]
                self.db_size.labels(database=db_name).set(db_size)
                
                # Get table sizes
                cur.execute("""
                    SELECT 
                        relname as table_name,
                        pg_total_relation_size(relid) as total_size
                    FROM 
                        pg_catalog.pg_statio_user_tables
                    ORDER BY 
                        total_size DESC
                """)
                
                for row in cur.fetchall():
                    self.table_sizes.labels(
                        table=row['table_name'],
                        database=db_name
                    ).set(row['total_size'])
                
                # Get index sizes
                cur.execute("""
                    SELECT 
                        indexrelname as index_name,
                        relname as table_name,
                        pg_relation_size(indexrelid) as index_size
                    FROM 
                        pg_catalog.pg_statio_user_indexes
                    ORDER BY 
                        index_size DESC
                """)
                
                for row in cur.fetchall():
                    self.index_sizes.labels(
                        index=row['index_name'],
                        table=row['table_name'],
                        database=db_name
                    ).set(row['index_size'])
    
    def _collect_connection_metrics(self):
        """Collect metrics about database connections"""
        with self.connect() as conn:
            with conn.cursor() as cur:
                # Get database name
                cur.execute("SELECT current_database()")
                db_name = cur.fetchone()[0]
                
                # Get connection count by state
                cur.execute("""
                    SELECT 
                        state,
                        COUNT(*) as count
                    FROM 
                        pg_stat_activity
                    WHERE 
                        datname = current_database()
                    GROUP BY 
                        state
                """)
                
                for row in cur.fetchall():
                    state = row['state'] or 'unknown'
                    self.connection_count.labels(
                        database=db_name,
                        state=state
                    ).set(row['count'])
                
                # Get transaction counts
                cur.execute("""
                    SELECT 
                        'commit' as type,
                        SUM(xact_commit) as count
                    FROM 
                        pg_stat_database
                    WHERE 
                        datname = current_database()
                    UNION ALL
                    SELECT 
                        'rollback' as type,
                        SUM(xact_rollback) as count
                    FROM 
                        pg_stat_database
                    WHERE 
                        datname = current_database()
                """)
                
                for row in cur.fetchall():
                    # Save the current value
                    current_count = self.transaction_count.labels(
                        database=db_name,
                        type=row['type']
                    )._value.get()
                    
                    # Increment by the difference
                    if current_count is not None:
                        increment = max(0, row['count'] - current_count)
                        if increment > 0:
                            self.transaction_count.labels(
                                database=db_name,
                                type=row['type']
                            ).inc(increment)
                    else:
                        # First measurement, just set the counter
                        self.transaction_count.labels(
                            database=db_name,
                            type=row['type']
                        )._value.set(row['count'])
    
    def _collect_performance_metrics(self):
        """Collect metrics about database performance"""
        with self.connect() as conn:
            with conn.cursor() as cur:
                # Get database name
                cur.execute("SELECT current_database()")
                db_name = cur.fetchone()[0]
                
                # Get cache hit ratios
                cur.execute("""
                    SELECT 
                        'buffer' as type,
                        CASE 
                            WHEN blks_hit + blks_read > 0 
                            THEN blks_hit::float / (blks_hit + blks_read) 
                            ELSE 0 
                        END as hit_ratio
                    FROM 
                        pg_stat_database
                    WHERE 
                        datname = current_database()
                    UNION ALL
                    SELECT 
                        'index' as type,
                        CASE 
                            WHEN idx_blks_hit + idx_blks_read > 0 
                            THEN idx_blks_hit::float / (idx_blks_hit + idx_blks_read) 
                            ELSE 0 
                        END as hit_ratio
                    FROM 
                        pg_statio_user_tables
                    ORDER BY 
                        type
                """)
                
                for row in cur.fetchall():
                    self.cache_hit_ratio.labels(
                        database=db_name,
                        type=row['type']
                    ).set(row['hit_ratio'])
                
                # Get query durations from pg_stat_statements (if extension is enabled)
                try:
                    cur.execute("""
                        SELECT EXISTS (
                            SELECT 1 FROM pg_extension WHERE extname = 'pg_stat_statements'
                        )
                    """)
                    
                    has_pg_stat_statements = cur.fetchone()[0]
                    
                    if has_pg_stat_statements:
                        cur.execute("""
                            SELECT 
                                substring(query from 1 for 50) as query_sample,
                                calls,
                                total_time / calls as avg_time,
                                (regexp_matches(query, '^\\s*(\\w+)'))[1] as operation,
                                (regexp_matches(query, '\\s+FROM\\s+([\\w\\.]+)'))[1] as table_name
                            FROM 
                                pg_stat_statements
                            WHERE 
                                dbid = (SELECT oid FROM pg_database WHERE datname = current_database())
                                AND total_time > 0
                            ORDER BY 
                                total_time DESC
                            LIMIT 20
                        """)
                        
                        for row in cur.fetchall():
                            if row['operation'] and row['table_name']:
                                operation = row['operation'].lower()
                                table = row['table_name'].lower()
                                
                                # Record in histogram
                                self.query_duration.labels(
                                    database=db_name,
                                    table=table,
                                    operation=operation
                                ).observe(row['avg_time'] / 1000.0)  # Convert to seconds
                except Exception as e:
                    logger.warning(f"Could not collect query duration metrics: {e}")
    
    def _collect_synthesis_metrics(self):
        """Collect synthesis-specific metrics"""
        with self.connect() as conn:
            with conn.cursor() as cur:
                # Get job counts by status
                cur.execute("""
                    SELECT 
                        status,
                        COUNT(*) as count
                    FROM 
                        synthesis_jobs
                    GROUP BY 
                        status
                """)
                
                for row in cur.fetchall():
                    self.synthesis_job_count.labels(
                        status=row['status']
                    ).set(row['count'])
                
                # Get job durations
                cur.execute("""
                    SELECT 
                        language,
                        explanation_level,
                        AVG(EXTRACT(EPOCH FROM (completed_at - created_at))) as avg_duration,
                        COUNT(*) as count
                    FROM 
                        synthesis_jobs
                    WHERE 
                        status = 'completed'
                        AND completed_at IS NOT NULL
                    GROUP BY 
                        language, explanation_level
                """)
                
                for row in cur.fetchall():
                    # Record average in histogram by simulating observations
                    # This is an approximation since we don't have the raw data
                    if row['count'] > 0:
                        self.synthesis_job_duration.labels(
                            language=row['language'],
                            explanation_level=str(row['explanation_level'])
                        ).observe(row['avg_duration'])
                
                # Get program complexity
                cur.execute("""
                    SELECT 
                        j.language,
                        j.explanation_level,
                        AVG(calculate_program_complexity(r.program)) as avg_complexity
                    FROM 
                        synthesis_jobs j
                    JOIN 
                        synthesis_results r ON j.job_id = r.job_id
                    WHERE 
                        j.status = 'completed'
                    GROUP BY 
                        j.language, j.explanation_level
                """)
                
                for row in cur.fetchall():
                    self.program_complexity.labels(
                        language=row['language'],
                        explanation_level=str(row['explanation_level'])
                    ).set(row['avg_complexity'])
                
                # Get explanation tree sizes
                cur.execute("""
                    SELECT 
                        j.language,
                        j.explanation_level,
                        AVG(jsonb_array_length(r.explanation_tree->'children')) as avg_tree_size
                    FROM 
                        synthesis_jobs j
                    JOIN 
                        synthesis_results r ON j.job_id = r.job_id
                    WHERE 
                        j.status = 'completed'
                    GROUP BY 
                        j.language, j.explanation_level
                """)
                
                for row in cur.fetchall():
                    self.explanation_tree_size.labels(
                        language=row['language'],
                        explanation_level=str(row['explanation_level'])
                    ).set(row['avg_tree_size'])
    
    def _collect_replication_metrics(self):
        """Collect metrics about database replication"""
        with self.connect() as conn:
            with conn.cursor() as cur:
                # Get database name
                cur.execute("SELECT current_database()")
                db_name = cur.fetchone()[0]
                
                # Check if database is configured for replication
                cur.execute("""
                    SELECT 
                        count(*) > 0 as has_replication
                    FROM 
                        pg_catalog.pg_stat_replication
                """)
                
                has_replication = cur.fetchone()['has_replication']
                
                if has_replication:
                    # Get replication lag for each replica
                    cur.execute("""
                        SELECT 
                            application_name as replica_name,
                            EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) as lag_seconds
                        FROM 
                            pg_catalog.pg_stat_replication
                    """)
                    
                    for row in cur.fetchall():
                        self.replication_lag.labels(
                            database=db_name,
                            replica=row['replica_name']
                        ).set(row['lag_seconds'])

class QueryPerformanceMonitor:
    """
    Monitors and logs slow queries in the synthesis database.
    """
    
    def __init__(self, conn_string: str, slow_query_threshold_ms: int = 500):
        """
        Initialize the query performance monitor
        
        Args:
            conn_string: PostgreSQL connection string
            slow_query_threshold_ms: Threshold in milliseconds to consider a query slow
        """
        self.conn_string = conn_string
        self.slow_query_threshold_ms = slow_query_threshold_ms
        self.running = False
        self.monitor_thread = None
        
        # Configure logging
        self.logger = logging.getLogger("slow-query-monitor")
        
        # Metrics for slow queries
        self.slow_query_count = Counter(
            'synthesis_db_slow_queries_total',
            'Total number of slow queries',
            ['database', 'query_type']
        )
    
    def start(self):
        """Start monitoring slow queries"""
        if self.running:
            self.logger.warning("Query performance monitor is already running")
            return
        
        self.running = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        self.logger.info(f"Query performance monitor started (threshold: {self.slow_query_threshold_ms}ms)")
    
    def stop(self):
        """Stop monitoring slow queries"""
        self.running = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=10)
            self.logger.info("Query performance monitor stopped")
    
    def connect(self):
        """Create a database connection"""
        return psycopg2.connect(
            self.conn_string,
            cursor_factory=DictCursor
        )
    
    def _monitor_loop(self):
        """Main monitoring loop"""
        # Check if pg_stat_statements extension is available
        have_pg_stat_statements = False
        
        with self.connect() as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    SELECT EXISTS (
                        SELECT 1 FROM pg_extension WHERE extname = 'pg_stat_statements'
                    )
                """)
                have_pg_stat_statements = cur.fetchone()[0]
        
        if not have_pg_stat_statements:
            self.logger.warning("pg_stat_statements extension not available. Slow query monitoring disabled.")
            return
        
        # Initialize last query IDs dictionary
        last_query_ids = {}
        
        # Main monitoring loop
        while self.running:
            try:
                with self.connect() as conn:
                    with conn.cursor() as cur:
                        # Get database name
                        cur.execute("SELECT current_database()")
                        db_name = cur.fetchone()[0]
                        
                        # Reset pg_stat_statements if this is the first run
                        if not last_query_ids:
                            try:
                                cur.execute("SELECT pg_stat_statements_reset()")
                                conn.commit()
                            except Exception as e:
                                self.logger.warning(f"Could not reset pg_stat_statements: {e}")
                        
                        # Get slow queries
                        cur.execute("""
                            SELECT 
                                queryid,
                                query,
                                calls,
                                total_time / calls as avg_time_ms,
                                max_time as max_time_ms,
                                mean_time as mean_time_ms,
                                (regexp_matches(query, '^\\s*(\\w+)'))[1] as query_type
                            FROM 
                                pg_stat_statements
                            WHERE 
                                dbid = (SELECT oid FROM pg_database WHERE datname = current_database())
                                AND (total_time / calls) > %s
                                OR max_time > %s
                            ORDER BY 
                                avg_time_ms DESC
                        """, (self.slow_query_threshold_ms, self.slow_query_threshold_ms))
                        
                        for row in cur.fetchall():
                            # Skip if we've already seen this query
                            if row['queryid'] in last_query_ids:
                                continue
                            
                            # Log slow query
                            query_type = row['query_type'].lower() if row['query_type'] else 'unknown'
                            self.logger.warning(
                                f"Slow query detected ({row['avg_time_ms']:.2f}ms avg, {row['max_time_ms']:.2f}ms max): "
                                f"{query_type.upper()} {row['query'].strip()[:100]}..."
                            )
                            
                            # Update metrics
                            self.slow_query_count.labels(
                                database=db_name,
                                query_type=query_type
                            ).inc()
                            
                            # Remember this query ID
                            last_query_ids[row['queryid']] = True
                
                # Sleep before next check
                time.sleep(60)
            
            except Exception as e:
                self.logger.error(f"Error monitoring query performance: {e}", exc_info=True)
                time.sleep(60)  # Sleep before retry

# Example usage
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Database metrics collection for Reversible Meta-Synthesis')
    parser.add_argument('--conn-string', type=str, required=True, help='PostgreSQL connection string')
    parser.add_argument('--poll-interval', type=int, default=60, help='Metrics polling interval in seconds')
    parser.add_argument('--slow-query-threshold', type=int, default=500, help='Slow query threshold in milliseconds')
    args = parser.parse_args()
    
    try:
        # Start metrics collector
        metrics_collector = DatabaseMetricsCollector(
            args.conn_string,
            poll_interval=args.poll_interval
        )
        metrics_collector.start()
        
        # Start query performance monitor
        query_monitor = QueryPerformanceMonitor(
            args.conn_string,
            slow_query_threshold_ms=args.slow_query_threshold
        )
        query_monitor.start()
        
        # Keep main thread running
        print(f"Database metrics collection started (poll interval: {args.poll_interval}s)")
        print(f"Slow query monitoring started (threshold: {args.slow_query_threshold}ms)")
        print("Press Ctrl+C to stop...")
        
        while True:
            time.sleep(1)
    
    except KeyboardInterrupt:
        print("Stopping...")
        metrics_collector.stop()
        query_monitor.stop()
        print("Stopped")
#+end_src

*** Automated Backups and Disaster Recovery

Design an automated backup system and disaster recovery procedures for synthesis data.

#+begin_src python :tangle doc/future_work/database_backup.py
#!/usr/bin/env python3
"""
Automated backup and disaster recovery for the Reversible Meta-Synthesis database.
"""

import os
import sys
import time
import logging
import subprocess
import datetime
import boto3
import argparse
import json
import yaml
from typing import Dict, List, Optional

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('/var/log/synthesis-db-backup.log')
    ]
)
logger = logging.getLogger("synthesis-db-backup")

class DatabaseBackupManager:
    """
    Manages automated backups and disaster recovery for the synthesis database.
    """
    
    def __init__(self, config_file: str):
        """
        Initialize the backup manager
        
        Args:
            config_file: Path to the configuration file
        """
        # Load configuration
        with open(config_file, 'r') as f:
            self.config = yaml.safe_load(f)
        
        # Create S3 client for backup storage
        self.s3 = boto3.client(
            's3',
            aws_access_key_id=self.config['s3']['access_key'],
            aws_secret_access_key=self.config['s3']['secret_key'],
            region_name=self.config['s3']['region']
        )
        
        # Create SNS client for notifications
        self.sns = boto3.client(
            'sns',
            aws_access_key_id=self.config['s3']['access_key'],
            aws_secret_access_key=self.config['s3']['secret_key'],
            region_name=self.config['s3']['region']
        )
        
        # Validate configuration
        self._validate_config()
        
        # Create backup directory if it doesn't exist
        os.makedirs(self.config['local']['backup_dir'], exist_ok=True)
    
    def _validate_config(self):
        """Validate the configuration"""
        required_keys = [
            's3.bucket',
            's3.prefix',
            's3.region',
            'database.host',
            'database.port',
            'database.name',
            'database.user',
            'database.password',
            'local.backup_dir',
            'retention.daily',
            'retention.weekly',
            'retention.monthly'
        ]
        
        for key in required_keys:
            parts = key.split('.')
            config = self.config
            for part in parts:
                if part not in config:
                    raise ValueError(f"Missing required configuration key: {key}")
                config = config[part]
    
    def perform_backup(self, backup_type: str = 'daily'):
        """
        Perform a database backup
        
        Args:
            backup_type: Type of backup (daily, weekly, monthly)
        """
        logger.info(f"Starting {backup_type} backup of {self.config['database']['name']}")
        
        try:
            # Create backup filename
            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_file = os.path.join(
                self.config['local']['backup_dir'], 
                f"{self.config['database']['name']}_{backup_type}_{timestamp}.sql.gz"
            )
            
            # Create connection string for pg_dump
            conn_string = f"postgresql://{self.config['database']['user']}:{self.config['database']['password']}@{self.config['database']['host']}:{self.config['database']['port']}/{self.config['database']['name']}"
            
            # Run pg_dump to create backup
            logger.info(f"Creating backup file: {backup_file}")
            result = subprocess.run(
                [
                    'pg_dump',
                    '--format=custom',
                    '--compress=9',
                    '--file', backup_file,
                    conn_string
                ],
                capture_output=True,
                text=True
            )
            
            if result.returncode != 0:
                raise Exception(f"pg_dump failed: {result.stderr}")
            
            # Upload to S3
            s3_key = f"{self.config['s3']['prefix']}/{backup_type}/{os.path.basename(backup_file)}"
            logger.info(f"Uploading backup to S3: s3://{self.config['s3']['bucket']}/{s3_key}")
            
            self.s3.upload_file(
                backup_file,
                self.config['s3']['bucket'],
                s3_key,
                ExtraArgs={
                    'ServerSideEncryption': 'AES256',
                    'Metadata': {
                        'backup-type': backup_type,
                        'database': self.config['database']['name'],
                        'timestamp': timestamp
                    }
                }
            )
            
            # Remove local backup file if configured
            if not self.config.get('local', {}).get('keep_local', False):
                logger.info(f"Removing local backup file: {backup_file}")
                os.remove(backup_file)
            
            # Apply retention policy
            self._apply_retention_policy(backup_type)
            
            # Send success notification
            self._send_notification(
                f"Database backup successful: {self.config['database']['name']}",
                f"Successfully completed {backup_type} backup of {self.config['database']['name']} database.\n"
                f"Backup file: s3://{self.config['s3']['bucket']}/{s3_key}\n"
                f"Timestamp: {timestamp}"
            )
            
            logger.info(f"Backup completed successfully: {s3_key}")
            return True
        
        except Exception as e:
            logger.error(f"Backup failed: {e}", exc_info=True)
            
            # Send failure notification
            self._send_notification(
                f"Database backup failed: {self.config['database']['name']}",
                f"Failed to complete {backup_type} backup of {self.config['database']['name']} database.\n"
                f"Error: {str(e)}"
            )
            
            return False
    
    def _apply_retention_policy(self, backup_type: str):
        """
        Apply retention policy to backups
        
        Args:
            backup_type: Type of backup (daily, weekly, monthly)
        """
        logger.info(f"Applying retention policy for {backup_type} backups")
        
        # Get retention period for this backup type
        retention_days = self.config['retention'].get(backup_type, 7)
        
        # List backups
        prefix = f"{self.config['s3']['prefix']}/{backup_type}/"
        response = self.s3.list_objects_v2(
            Bucket=self.config['s3']['bucket'],
            Prefix=prefix
        )
        
        if 'Contents' not in response:
            logger.info(f"No backups found with prefix: {prefix}")
            return
        
        # Sort backups by date (oldest first)
        backups = sorted(
            response['Contents'],
            key=lambda x: x['LastModified']
        )
        
        # Keep only the latest N backups based on retention policy
        backups_to_keep = min(retention_days, len(backups))
        backups_to_delete = backups[:-backups_to_keep] if backups_to_keep > 0 else backups
        
        # Delete old backups
        for backup in backups_to_delete:
            logger.info(f"Deleting old backup: {backup['Key']}")
            self.s3.delete_object(
                Bucket=self.config['s3']['bucket'],
                Key=backup['Key']
            )
    
    def _send_notification(self, subject: str, message: str):
        """
        Send a notification
        
        Args:
            subject: Notification subject
message: Notification message
        """
        if 'notifications' in self.config and 'sns_topic_arn' in self.config['notifications']:
            try:
                self.sns.publish(
                    TopicArn=self.config['notifications']['sns_topic_arn'],
                    Subject=subject,
                    Message=message
                )
                logger.info(f"Sent notification: {subject}")
            except Exception as e:
                logger.error(f"Failed to send notification: {e}")
    
    def restore_backup(self, backup_key: str, target_db: Optional[str] = None):
        """
        Restore a database from backup
        
        Args:
            backup_key: S3 key of the backup to restore
            target_db: Name of the target database (defaults to original DB name)
        """
        logger.info(f"Starting restore from backup: {backup_key}")
        
        try:
            # Create temporary directory for restore
            with tempfile.TemporaryDirectory() as temp_dir:
                # Download backup file
                backup_file = os.path.join(temp_dir, os.path.basename(backup_key))
                logger.info(f"Downloading backup from S3: s3://{self.config['s3']['bucket']}/{backup_key}")
                
                self.s3.download_file(
                    self.config['s3']['bucket'],
                    backup_key,
                    backup_file
                )
                
                # Determine target database
                target_database = target_db or self.config['database']['name']
                
                # Create connection string for pg_restore
                conn_string = f"postgresql://{self.config['database']['user']}:{self.config['database']['password']}@{self.config['database']['host']}:{self.config['database']['port']}/{target_database}"
                
                # Run pg_restore to restore backup
                logger.info(f"Restoring backup to database: {target_database}")
                result = subprocess.run(
                    [
                        'pg_restore',
                        '--clean',
                        '--if-exists',
                        '--exit-on-error',
                        '--dbname', conn_string,
                        backup_file
                    ],
                    capture_output=True,
                    text=True
                )
                
                if result.returncode != 0:
                    raise Exception(f"pg_restore failed: {result.stderr}")
                
                # Send success notification
                self._send_notification(
                    f"Database restore successful: {target_database}",
                    f"Successfully restored {target_database} database from backup.\n"
                    f"Backup file: s3://{self.config['s3']['bucket']}/{backup_key}"
                )
                
                logger.info(f"Restore completed successfully: {target_database}")
                return True
        
        except Exception as e:
            logger.error(f"Restore failed: {e}", exc_info=True)
            
            # Send failure notification
            self._send_notification(
                f"Database restore failed: {target_db or self.config['database']['name']}",
                f"Failed to restore database from backup.\n"
                f"Backup file: s3://{self.config['s3']['bucket']}/{backup_key}\n"
                f"Error: {str(e)}"
            )
            
            return False
    
    def list_available_backups(self) -> List[Dict[str, Any]]:
        """
        List available backups
        
        Returns:
            List of available backups with metadata
        """
        logger.info(f"Listing available backups")
        
        # List backups
        prefix = f"{self.config['s3']['prefix']}/"
        paginator = self.s3.get_paginator('list_objects_v2')
        
        backups = []
        for page in paginator.paginate(Bucket=self.config['s3']['bucket'], Prefix=prefix):
            if 'Contents' in page:
                for obj in page['Contents']:
                    # Get backup metadata
                    try:
                        response = self.s3.head_object(
                            Bucket=self.config['s3']['bucket'],
                            Key=obj['Key']
                        )
                        
                        # Extract backup type from key
                        backup_parts = obj['Key'].split('/')
                        backup_type = backup_parts[-2] if len(backup_parts) >= 2 else 'unknown'
                        
                        backups.append({
                            'key': obj['Key'],
                            'size': obj['Size'],
                            'last_modified': obj['LastModified'].isoformat(),
                            'type': response.get('Metadata', {}).get('backup-type', backup_type),
                            'database': response.get('Metadata', {}).get('database', self.config['database']['name']),
                            'timestamp': response.get('Metadata', {}).get('timestamp', '')
                        })
                    
                    except Exception as e:
                        logger.warning(f"Failed to get metadata for backup {obj['Key']}: {e}")
        
        # Sort backups by date (newest first)
        backups.sort(key=lambda x: x['last_modified'], reverse=True)
        
        return backups
    
    def verify_backup(self, backup_key: str) -> bool:
        """
        Verify a backup file
        
        Args:
            backup_key: S3 key of the backup to verify
            
        Returns:
            True if backup is valid, False otherwise
        """
        logger.info(f"Verifying backup: {backup_key}")
        
        try:
            # Create temporary directory for verification
            with tempfile.TemporaryDirectory() as temp_dir:
                # Download backup file
                backup_file = os.path.join(temp_dir, os.path.basename(backup_key))
                logger.info(f"Downloading backup from S3: s3://{self.config['s3']['bucket']}/{backup_key}")
                
                self.s3.download_file(
                    self.config['s3']['bucket'],
                    backup_key,
                    backup_file
                )
                
                # Verify backup file
                logger.info(f"Verifying backup file: {backup_file}")
                result = subprocess.run(
                    [
                        'pg_restore',
                        '--list',
                        backup_file
                    ],
                    capture_output=True,
                    text=True
                )
                
                if result.returncode != 0:
                    raise Exception(f"Backup verification failed: {result.stderr}")
                
                logger.info(f"Backup verified successfully: {backup_key}")
                return True
        
        except Exception as e:
            logger.error(f"Backup verification failed: {e}", exc_info=True)
            return False
    
    def setup_continuous_archiving(self):
        """
        Set up continuous archiving using PostgreSQL's WAL (Write-Ahead Log)
        """
        logger.info(f"Setting up continuous archiving")
        
        try:
            # Create directory for WAL archives
            wal_dir = os.path.join(self.config['local']['backup_dir'], 'wal_archives')
            os.makedirs(wal_dir, exist_ok=True)
            
            # Create SQL script to configure continuous archiving
            sql_script = f"""
            ALTER SYSTEM SET wal_level = 'replica';
            ALTER SYSTEM SET archive_mode = 'on';
            ALTER SYSTEM SET archive_command = 'test ! -f {wal_dir}/%f && cp %p {wal_dir}/%f';
            ALTER SYSTEM SET archive_timeout = '1h';
            """
            
            # Write SQL script to temporary file
            with tempfile.NamedTemporaryFile(suffix='.sql', mode='w', delete=False) as f:
                f.write(sql_script)
                sql_file = f.name
            
            try:
                # Run SQL script
                conn_string = f"postgresql://{self.config['database']['user']}:{self.config['database']['password']}@{self.config['database']['host']}:{self.config['database']['port']}/{self.config['database']['name']}"
                
                result = subprocess.run(
                    [
                        'psql',
                        '-f', sql_file,
                        conn_string
                    ],
                    capture_output=True,
                    text=True
                )
                
                if result.returncode != 0:
                    raise Exception(f"Failed to configure WAL archiving: {result.stderr}")
                
                # Reload PostgreSQL configuration
                result = subprocess.run(
                    [
                        'psql',
                        '-c', 'SELECT pg_reload_conf();',
                        conn_string
                    ],
                    capture_output=True,
                    text=True
                )
                
                if result.returncode != 0:
                    raise Exception(f"Failed to reload PostgreSQL configuration: {result.stderr}")
                
                logger.info(f"Continuous archiving configured successfully")
                return True
            
            finally:
                # Remove temporary SQL file
                os.unlink(sql_file)
        
        except Exception as e:
            logger.error(f"Failed to set up continuous archiving: {e}", exc_info=True)
            return False
    
    def setup_streaming_replication(self, replica_host: str, replica_port: int = 5432):
        """
        Set up streaming replication to a replica server
        
        Args:
            replica_host: Hostname of the replica server
            replica_port: Port of the replica server
        """
        logger.info(f"Setting up streaming replication to {replica_host}:{replica_port}")
        
        try:
            # Create replication user if not exists
            replication_user = self.config.get('replication', {}).get('user', 'replication_user')
            replication_password = self.config.get('replication', {}).get('password', 'replication_password')
            
            conn_string = f"postgresql://{self.config['database']['user']}:{self.config['database']['password']}@{self.config['database']['host']}:{self.config['database']['port']}/{self.config['database']['name']}"
            
            # Create SQL script for primary server
            primary_script = f"""
            -- Create replication user
            CREATE USER {replication_user} WITH REPLICATION ENCRYPTED PASSWORD '{replication_password}';
            
            -- Configure pg_hba.conf for replication
            ALTER SYSTEM SET listen_addresses = '*';
            """
            
            # Write SQL script to temporary file
            with tempfile.NamedTemporaryFile(suffix='.sql', mode='w', delete=False) as f:
                f.write(primary_script)
                primary_sql_file = f.name
            
            try:
                # Run SQL script on primary
                result = subprocess.run(
                    [
                        'psql',
                        '-f', primary_sql_file,
                        conn_string
                    ],
                    capture_output=True,
                    text=True
                )
                
                if result.returncode != 0:
                    raise Exception(f"Failed to configure primary server: {result.stderr}")
                
                # Reload PostgreSQL configuration on primary
                result = subprocess.run(
                    [
                        'psql',
                        '-c', 'SELECT pg_reload_conf();',
                        conn_string
                    ],
                    capture_output=True,
                    text=True
                )
                
                if result.returncode != 0:
                    raise Exception(f"Failed to reload PostgreSQL configuration: {result.stderr}")
                
                # Create recovery.conf for replica
                recovery_conf = f"""
                # recovery.conf for PostgreSQL replica
                standby_mode = 'on'
                primary_conninfo = 'host={self.config['database']['host']} port={self.config['database']['port']} user={replication_user} password={replication_password}'
                trigger_file = '/tmp/pg_failover_trigger'
                """
                
                logger.info(f"Replica configuration (recovery.conf):\n{recovery_conf}")
                
                logger.info(f"Streaming replication configured successfully")
                logger.info(f"To complete setup, create a base backup and configure the replica server with recovery.conf")
                
                return True
            
            finally:
                # Remove temporary SQL file
                os.unlink(primary_sql_file)
        
        except Exception as e:
            logger.error(f"Failed to set up streaming replication: {e}", exc_info=True)
            return False
    
    def setup_automatic_backups(self):
        """
        Set up automatic backup schedule using cron
        """
        logger.info(f"Setting up automatic backup schedule")
        
        try:
            # Get backup intervals from config
            daily_time = self.config.get('schedule', {}).get('daily', '01:00')
            weekly_day = self.config.get('schedule', {}).get('weekly_day', '0')  # Sunday
            weekly_time = self.config.get('schedule', {}).get('weekly', '02:00')
            monthly_day = self.config.get('schedule', {}).get('monthly_day', '1')  # 1st of month
            monthly_time = self.config.get('schedule', {}).get('monthly', '03:00')
            
            # Parse times
            daily_hour, daily_minute = daily_time.split(':')
            weekly_hour, weekly_minute = weekly_time.split(':')
            monthly_hour, monthly_minute = monthly_time.split(':')
            
            # Create cron entries
            script_path = os.path.abspath(sys.argv[0])
            config_path = os.path.abspath(self.config.get('config_file', 'config.yaml'))
            
            daily_cron = f"{daily_minute} {daily_hour} * * * {script_path} --config {config_path} --type daily\n"
            weekly_cron = f"{weekly_minute} {weekly_hour} * * {weekly_day} {script_path} --config {config_path} --type weekly\n"
            monthly_cron = f"{monthly_minute} {monthly_hour} {monthly_day} * * {script_path} --config {config_path} --type monthly\n"
            
            # Write cron file
            cron_file = '/etc/cron.d/synthesis-db-backup'
            with open(cron_file, 'w') as f:
                f.write(f"# Automated backups for Reversible Meta-Synthesis database\n")
                f.write(f"SHELL=/bin/bash\n")
                f.write(f"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n\n")
                f.write(daily_cron)
                f.write(weekly_cron)
                f.write(monthly_cron)
            
            # Set proper permissions
            os.chmod(cron_file, 0o644)
            
            logger.info(f"Automatic backup schedule configured successfully")
            logger.info(f"Daily backups: {daily_time}")
            logger.info(f"Weekly backups: Day {weekly_day} at {weekly_time}")
            logger.info(f"Monthly backups: Day {monthly_day} at {monthly_time}")
            
            return True
        
        except Exception as e:
            logger.error(f"Failed to set up automatic backup schedule: {e}", exc_info=True)
            return False

# Example usage
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Database backup and restore for Reversible Meta-Synthesis')
    parser.add_argument('--config', type=str, default='backup_config.yaml', help='Path to configuration file')
    parser.add_argument('--action', type=str, choices=['backup', 'restore', 'list', 'verify', 'setup-archiving', 'setup-replication', 'setup-schedule'], help='Action to perform')
    parser.add_argument('--type', type=str, choices=['daily', 'weekly', 'monthly'], default='daily', help='Type of backup')
    parser.add_argument('--backup-key', type=str, help='S3 key of backup (for restore/verify)')
    parser.add_argument('--target-db', type=str, help='Target database for restore')
    parser.add_argument('--replica-host', type=str, help='Hostname of replica server')
    parser.add_argument('--replica-port', type=int, default=5432, help='Port of replica server')
    args = parser.parse_args()
    
    # Create backup manager
    manager = DatabaseBackupManager(args.config)
    
    # Perform requested action
    if args.action == 'backup':
        success = manager.perform_backup(args.type)
        sys.exit(0 if success else 1)
    
    elif args.action == 'restore':
        if not args.backup_key:
            print("Error: --backup-key is required for restore action")
            sys.exit(1)
        
        success = manager.restore_backup(args.backup_key, args.target_db)
        sys.exit(0 if success else 1)
    
    elif args.action == 'list':
        backups = manager.list_available_backups()
        print(f"Available backups:")
        for backup in backups:
            print(f"- {backup['key']}")
            print(f"  Type: {backup['type']}")
            print(f"  Database: {backup['database']}")
            print(f"  Modified: {backup['last_modified']}")
            print(f"  Size: {backup['size'] / (1024*1024):.2f} MB")
            print()
    
    elif args.action == 'verify':
        if not args.backup_key:
            print("Error: --backup-key is required for verify action")
            sys.exit(1)
        
        success = manager.verify_backup(args.backup_key)
        sys.exit(0 if success else 1)
    
    elif args.action == 'setup-archiving':
        success = manager.setup_continuous_archiving()
        sys.exit(0 if success else 1)
    
    elif args.action == 'setup-replication':
        if not args.replica_host:
            print("Error: --replica-host is required for setup-replication action")
            sys.exit(1)
        
        success = manager.setup_streaming_replication(args.replica_host, args.replica_port)
        sys.exit(0 if success else 1)
    
    elif args.action == 'setup-schedule':
        success = manager.setup_automatic_backups()
        sys.exit(0 if success else 1)
    
    else:
        parser.print_help()
        sys.exit(1)
#+end_src

#+begin_src yaml :tangle doc/future_work/backup_config.yaml
# Configuration for database backup and restore

# S3 Configuration
s3:
  bucket: reversible-meta-synthesis-backups
  prefix: db-backups
  region: us-west-2
  access_key: "YOUR_ACCESS_KEY"
  secret_key: "YOUR_SECRET_KEY"

# Database Configuration
database:
  host: db.reversible-meta-synthesis.com
  port: 5432
  name: synthesis
  user: backup_user
  password: "YOUR_PASSWORD"

# Local Configuration
local:
  backup_dir: /var/backups/synthesis
  keep_local: false

# Retention Policy (in days)
retention:
  daily: 7
  weekly: 30
  monthly: 365

# Backup Schedule
schedule:
  daily: "01:00"     # Daily at 1 AM
  weekly_day: "0"    # Sunday
  weekly: "02:00"    # Weekly at 2 AM
  monthly_day: "1"   # 1st of month
  monthly: "03:00"   # Monthly at 3 AM

# Replication Configuration
replication:
  user: replication_user
  password: "YOUR_REPLICATION_PASSWORD"

# Notification Configuration
notifications:
  sns_topic_arn: "arn:aws:sns:us-west-2:123456789012:synthesis-db-alerts"
#+end_src

*** Data Warehousing and Analytics

Design a data warehousing solution for long-term storage and analysis of synthesis data.

#+begin_src sql :tangle doc/future_work/data_warehouse_schema.sql
-- Data Warehouse Schema for Reversible Meta-Synthesis Analytics

-- Time dimension
CREATE TABLE dim_time (
    time_id SERIAL PRIMARY KEY,
    date_actual DATE NOT NULL,
    day_of_week INTEGER NOT NULL,
    day_name VARCHAR(10) NOT NULL,
    day_of_month INTEGER NOT NULL,
    day_of_year INTEGER NOT NULL,
    week_of_year INTEGER NOT NULL,
    month_actual INTEGER NOT NULL,
    month_name VARCHAR(10) NOT NULL,
    quarter_actual INTEGER NOT NULL,
    year_actual INTEGER NOT NULL,
    is_weekend BOOLEAN NOT NULL,
    UNIQUE(date_actual)
);

-- Language dimension
CREATE TABLE dim_language (
    language_id SERIAL PRIMARY KEY,
    language_name VARCHAR(50) NOT NULL,
    language_version VARCHAR(20),
    language_family VARCHAR(50),
    paradigm VARCHAR(50),
    UNIQUE(language_name, language_version)
);

-- Explanation level dimension
CREATE TABLE dim_explanation_level (
    level_id SERIAL PRIMARY KEY,
    level_value INTEGER NOT NULL,
    level_name VARCHAR(50) NOT NULL,
    level_description TEXT,
    UNIQUE(level_value)
);

-- User dimension
CREATE TABLE dim_user (
    user_id UUID PRIMARY KEY,
    username VARCHAR(50) NOT NULL,
    user_type VARCHAR(20) NOT NULL,
    organization VARCHAR(100),
    country VARCHAR(50),
    created_at TIMESTAMP NOT NULL,
    is_active BOOLEAN NOT NULL,
    UNIQUE(username)
);

-- Program pattern dimension
CREATE TABLE dim_program_pattern (
    pattern_id SERIAL PRIMARY KEY,
    pattern_type VARCHAR(50) NOT NULL,
    pattern_name VARCHAR(100) NOT NULL,
    pattern_structure JSONB,
    complexity_level INTEGER NOT NULL,
    description TEXT
);

-- Synthesis job fact table
CREATE TABLE fact_synthesis_job (
    job_id UUID PRIMARY KEY,
    user_id UUID NOT NULL REFERENCES dim_user(user_id),
    language_id INTEGER NOT NULL REFERENCES dim_language(language_id),
    explanation_level_id INTEGER NOT NULL REFERENCES dim_explanation_level(level_id),
    time_id INTEGER NOT NULL REFERENCES dim_time(time_id),
    pattern_id INTEGER REFERENCES dim_program_pattern(pattern_id),
    
    -- Facts
    synthesis_time_seconds INTEGER NOT NULL,
    cpu_usage_percent REAL NOT NULL,
    memory_usage_mb REAL NOT NULL,
    explanation_tree_nodes INTEGER NOT NULL,
    program_complexity REAL NOT NULL,
    program_size_bytes INTEGER NOT NULL,
    status VARCHAR(20) NOT NULL,
    success BOOLEAN NOT NULL,
    
    -- Additional attributes
    query_count INTEGER NOT NULL,
    example_count INTEGER NOT NULL,
    decomposition_usage JSONB,
    strategy_name VARCHAR(100),
    
    UNIQUE(job_id)
);

-- Synthesis performance over time
CREATE TABLE fact_synthesis_performance (
    time_id INTEGER NOT NULL REFERENCES dim_time(time_id),
    language_id INTEGER NOT NULL REFERENCES dim_language(language_id),
    explanation_level_id INTEGER NOT NULL REFERENCES dim_explanation_level(level_id),
    
    -- Facts
    job_count INTEGER NOT NULL,
    success_rate REAL NOT NULL,
    avg_synthesis_time REAL NOT NULL,
    min_synthesis_time REAL NOT NULL,
    max_synthesis_time REAL NOT NULL,
    avg_cpu_usage REAL NOT NULL,
    avg_memory_usage REAL NOT NULL,
    avg_program_complexity REAL NOT NULL,
    
    PRIMARY KEY (time_id, language_id, explanation_level_id)
);

-- Example usage patterns
CREATE TABLE fact_example_usage (
    time_id INTEGER NOT NULL REFERENCES dim_time(time_id),
    language_id INTEGER NOT NULL REFERENCES dim_language(language_id),
    pattern_id INTEGER NOT NULL REFERENCES dim_program_pattern(pattern_id),
    
    -- Facts
    usage_count INTEGER NOT NULL,
    success_rate REAL NOT NULL,
    avg_synthesis_time REAL NOT NULL,
    
    PRIMARY KEY (time_id, language_id, pattern_id)
);

-- User activity
CREATE TABLE fact_user_activity (
    time_id INTEGER NOT NULL REFERENCES dim_time(time_id),
    user_id UUID NOT NULL REFERENCES dim_user(user_id),
    
    -- Facts
    job_count INTEGER NOT NULL,
    success_rate REAL NOT NULL,
    avg_synthesis_time REAL NOT NULL,
    distinct_languages INTEGER NOT NULL,
    
    PRIMARY KEY (time_id, user_id)
);

-- ETL Procedures

-- Populate Time Dimension
CREATE OR REPLACE PROCEDURE populate_dim_time(start_date DATE, end_date DATE)
LANGUAGE plpgsql AS $$
DECLARE
    curr_date DATE := start_date;
BEGIN
    WHILE curr_date <= end_date LOOP
        INSERT INTO dim_time (
            date_actual,
            day_of_week,
            day_name,
            day_of_month,
            day_of_year,
            week_of_year,
            month_actual,
            month_name,
            quarter_actual,
            year_actual,
            is_weekend
        )
        VALUES (
            curr_date,
            EXTRACT(DOW FROM curr_date),
            TO_CHAR(curr_date, 'Day'),
            EXTRACT(DAY FROM curr_date),
            EXTRACT(DOY FROM curr_date),
            EXTRACT(WEEK FROM curr_date),
            EXTRACT(MONTH FROM curr_date),
            TO_CHAR(curr_date, 'Month'),
            EXTRACT(QUARTER FROM curr_date),
            EXTRACT(YEAR FROM curr_date),
            CASE WHEN EXTRACT(DOW FROM curr_date) IN (0, 6) THEN TRUE ELSE FALSE END
        )
        ON CONFLICT (date_actual) DO NOTHING;
        
        curr_date := curr_date + INTERVAL '1 day';
    END LOOP;
END;
$$;

-- ETL for Synthesis Jobs
CREATE OR REPLACE PROCEDURE etl_synthesis_jobs()
LANGUAGE plpgsql AS $$
BEGIN
    -- Ensure time dimension has entries for all job dates
    INSERT INTO dim_time (
        date_actual,
        day_of_week,
        day_name,
        day_of_month,
        day_of_year,
        week_of_year,
        month_actual,
        month_name,
        quarter_actual,
        year_actual,
        is_weekend
    )
    SELECT DISTINCT
        DATE(created_at),
        EXTRACT(DOW FROM created_at),
        TO_CHAR(created_at, 'Day'),
        EXTRACT(DAY FROM created_at),
        EXTRACT(DOY FROM created_at),
        EXTRACT(WEEK FROM created_at),
        EXTRACT(MONTH FROM created_at),
        TO_CHAR(created_at, 'Month'),
        EXTRACT(QUARTER FROM created_at),
        EXTRACT(YEAR FROM created_at),
        CASE WHEN EXTRACT(DOW FROM created_at) IN (0, 6) THEN TRUE ELSE FALSE END
    FROM
        synthesis_jobs
    WHERE
        DATE(created_at) NOT IN (SELECT date_actual FROM dim_time)
    ORDER BY
        1;
    
    -- Ensure language dimension has all languages
    INSERT INTO dim_language (language_name, language_version, language_family, paradigm)
    SELECT DISTINCT
        language,
        NULL,
        CASE
            WHEN language IN ('prolog') THEN 'Logic Programming'
            WHEN language IN ('clojure', 'scheme', 'hy') THEN 'Lisp Family'
            WHEN language IN ('python', 'haskell') THEN language
            ELSE 'Other'
        END,
        CASE
            WHEN language IN ('prolog') THEN 'Logic'
            WHEN language IN ('clojure', 'scheme', 'hy', 'haskell') THEN 'Functional'
            WHEN language IN ('python') THEN 'Multi-paradigm'
            ELSE 'Other'
        END
    FROM
        synthesis_jobs
    WHERE
        language NOT IN (SELECT language_name FROM dim_language)
    ORDER BY
        1;
    
    -- Ensure explanation level dimension has all levels
    INSERT INTO dim_explanation_level (level_value, level_name, level_description)
    SELECT DISTINCT
        explanation_level,
        CASE
            WHEN explanation_level = 0 THEN 'Program-Level'
            WHEN explanation_level = 1 THEN 'Clause-Level'
            WHEN explanation_level = 2 THEN 'Clause-Structure-Level'
            WHEN explanation_level = 3 THEN 'Term-Level'
            ELSE 'Unknown'
        END,
        CASE
            WHEN explanation_level = 0 THEN 'Whole program explanation'
            WHEN explanation_level = 1 THEN 'Decomposed at clause level'
            WHEN explanation_level = 2 THEN 'Decomposed at clause structure level'
            WHEN explanation_level = 3 THEN 'Decomposed at term level'
            ELSE 'Unknown level'
        END
    FROM
        synthesis_jobs
    WHERE
        explanation_level NOT IN (SELECT level_value FROM dim_explanation_level)
    ORDER BY
        1;
    
    -- Ensure user dimension has all users
    INSERT INTO dim_user (user_id, username, user_type, organization, country, created_at, is_active)
    SELECT DISTINCT
        u.user_id,
        u.username,
        u.role,
        NULL,
        NULL,
        u.created_at,
        TRUE
    FROM
        users u
    JOIN
        synthesis_jobs sj ON u.user_id = sj.user_id
    WHERE
        u.user_id NOT IN (SELECT user_id FROM dim_user)
    ORDER BY
        1;
    
    -- Insert synthesis job facts
    INSERT INTO fact_synthesis_job (
        job_id,
        user_id,
        language_id,
        explanation_level_id,
        time_id,
        pattern_id,
        synthesis_time_seconds,
        cpu_usage_percent,
        memory_usage_mb,
        explanation_tree_nodes,
        program_complexity,
        program_size_bytes,
        status,
        success,
        query_count,
        example_count,
        decomposition_usage,
        strategy_name
    )
    SELECT
        sj.job_id,
        sj.user_id,
        dl.language_id,
        del.level_id,
        dt.time_id,
        NULL, -- pattern_id (to be updated later)
        sa.synthesis_time / 1000, -- convert ms to seconds
        sa.cpu_usage,
        sa.memory_usage / (1024*1024), -- convert bytes to MB
        COALESCE(
            (SELECT COUNT(*) FROM JSONB_ARRAY_ELEMENTS(sr.explanation_tree->'children')),
            0
        ),
        sa.program_complexity,
        LENGTH(sr.program),
        sj.status,
        CASE WHEN sj.status = 'completed' THEN TRUE ELSE FALSE END,
        1, -- query_count (placeholder)
        COALESCE(
(SELECT JSONB_ARRAY_LENGTH(js.examples) FROM job_specifications js WHERE js.job_id = sj.job_id),
            0
        ),
        NULL, -- decomposition_usage (to be updated later)
        NULL  -- strategy_name (to be updated later)
    FROM
        synthesis_jobs sj
    JOIN
        synthesis_analytics sa ON sj.job_id = sa.job_id
    LEFT JOIN
        synthesis_results sr ON sj.job_id = sr.job_id
    JOIN
        dim_language dl ON sj.language = dl.language_name
    JOIN
        dim_explanation_level del ON sj.explanation_level = del.level_value
    JOIN
        dim_time dt ON DATE(sj.created_at) = dt.date_actual
    LEFT JOIN
        dim_user du ON sj.user_id = du.user_id
    WHERE
        sj.job_id NOT IN (SELECT job_id FROM fact_synthesis_job)
    ORDER BY
        sj.created_at;
    
    -- Update pattern_id for synthesis jobs where applicable
    UPDATE fact_synthesis_job fsj
    SET pattern_id = dpp.pattern_id
    FROM
        synthesis_results sr
    JOIN
        program_patterns pp ON sr.pattern_id = pp.pattern_id
    JOIN
        dim_program_pattern dpp ON pp.pattern_type = dpp.pattern_type
    WHERE
        fsj.job_id = sr.job_id
        AND fsj.pattern_id IS NULL;
    
    -- Populate decomposition_usage
    UPDATE fact_synthesis_job fsj
    SET decomposition_usage = sr.metrics->'decomposition_usage'
    FROM
        synthesis_results sr
    WHERE
        fsj.job_id = sr.job_id
        AND fsj.decomposition_usage IS NULL
        AND sr.metrics ? 'decomposition_usage';
    
    -- Populate strategy_name
    UPDATE fact_synthesis_job fsj
    SET strategy_name = ss.name
    FROM
        synthesis_analytics sa
    JOIN
        synthesis_strategies ss ON sa.strategy_id = ss.strategy_id
    WHERE
        fsj.job_id = sa.job_id
        AND fsj.strategy_name IS NULL;
END;
$$;

-- ETL for Synthesis Performance
CREATE OR REPLACE PROCEDURE etl_synthesis_performance()
LANGUAGE plpgsql AS $$
BEGIN
    -- Insert/update synthesis performance facts
    INSERT INTO fact_synthesis_performance (
        time_id,
        language_id,
        explanation_level_id,
        job_count,
        success_rate,
        avg_synthesis_time,
        min_synthesis_time,
        max_synthesis_time,
        avg_cpu_usage,
        avg_memory_usage,
        avg_program_complexity
    )
    SELECT
        dt.time_id,
        dl.language_id,
        del.level_id,
        COUNT(fsj.job_id),
        AVG(CASE WHEN fsj.success THEN 1.0 ELSE 0.0 END),
        AVG(fsj.synthesis_time_seconds),
        MIN(fsj.synthesis_time_seconds),
        MAX(fsj.synthesis_time_seconds),
        AVG(fsj.cpu_usage_percent),
        AVG(fsj.memory_usage_mb),
        AVG(fsj.program_complexity)
    FROM
        fact_synthesis_job fsj
    JOIN
        dim_time dt ON fsj.time_id = dt.time_id
    JOIN
        dim_language dl ON fsj.language_id = dl.language_id
    JOIN
        dim_explanation_level del ON fsj.explanation_level_id = del.level_id
    GROUP BY
        dt.time_id,
        dl.language_id,
        del.level_id
    ON CONFLICT (time_id, language_id, explanation_level_id)
    DO UPDATE SET
        job_count = EXCLUDED.job_count,
        success_rate = EXCLUDED.success_rate,
        avg_synthesis_time = EXCLUDED.avg_synthesis_time,
        min_synthesis_time = EXCLUDED.min_synthesis_time,
        max_synthesis_time = EXCLUDED.max_synthesis_time,
        avg_cpu_usage = EXCLUDED.avg_cpu_usage,
        avg_memory_usage = EXCLUDED.avg_memory_usage,
        avg_program_complexity = EXCLUDED.avg_program_complexity;
END;
$$;

-- ETL for Example Usage
CREATE OR REPLACE PROCEDURE etl_example_usage()
LANGUAGE plpgsql AS $$
BEGIN
    -- Insert/update example usage facts
    INSERT INTO fact_example_usage (
        time_id,
        language_id,
        pattern_id,
        usage_count,
        success_rate,
        avg_synthesis_time
    )
    SELECT
        dt.time_id,
        dl.language_id,
        dpp.pattern_id,
        COUNT(fsj.job_id),
        AVG(CASE WHEN fsj.success THEN 1.0 ELSE 0.0 END),
        AVG(fsj.synthesis_time_seconds)
    FROM
        fact_synthesis_job fsj
    JOIN
        dim_time dt ON fsj.time_id = dt.time_id
    JOIN
        dim_language dl ON fsj.language_id = dl.language_id
    JOIN
        dim_program_pattern dpp ON fsj.pattern_id = dpp.pattern_id
    WHERE
        fsj.pattern_id IS NOT NULL
    GROUP BY
        dt.time_id,
        dl.language_id,
        dpp.pattern_id
    ON CONFLICT (time_id, language_id, pattern_id)
    DO UPDATE SET
        usage_count = EXCLUDED.usage_count,
        success_rate = EXCLUDED.success_rate,
        avg_synthesis_time = EXCLUDED.avg_synthesis_time;
END;
$$;

-- ETL for User Activity
CREATE OR REPLACE PROCEDURE etl_user_activity()
LANGUAGE plpgsql AS $$
BEGIN
    -- Insert/update user activity facts
    INSERT INTO fact_user_activity (
        time_id,
        user_id,
        job_count,
        success_rate,
        avg_synthesis_time,
        distinct_languages
    )
    SELECT
        dt.time_id,
        du.user_id,
        COUNT(fsj.job_id),
        AVG(CASE WHEN fsj.success THEN 1.0 ELSE 0.0 END),
        AVG(fsj.synthesis_time_seconds),
        COUNT(DISTINCT fsj.language_id)
    FROM
        fact_synthesis_job fsj
    JOIN
        dim_time dt ON fsj.time_id = dt.time_id
    JOIN
        dim_user du ON fsj.user_id = du.user_id
    GROUP BY
        dt.time_id,
        du.user_id
    ON CONFLICT (time_id, user_id)
    DO UPDATE SET
        job_count = EXCLUDED.job_count,
        success_rate = EXCLUDED.success_rate,
        avg_synthesis_time = EXCLUDED.avg_synthesis_time,
        distinct_languages = EXCLUDED.distinct_languages;
END;
$$;

-- ETL master procedure
CREATE OR REPLACE PROCEDURE etl_master()
LANGUAGE plpgsql AS $$
BEGIN
    -- Run all ETL procedures in sequence
    CALL etl_synthesis_jobs();
    CALL etl_synthesis_performance();
    CALL etl_example_usage();
    CALL etl_user_activity();
END;
$$;

-- Analytical views

-- View for synthesis success rate trends
CREATE VIEW view_synthesis_success_trends AS
SELECT
    dt.year_actual,
    dt.quarter_actual,
    dt.month_actual,
    dt.month_name,
    dl.language_name,
    del.level_name,
    SUM(fsp.job_count) AS total_jobs,
    AVG(fsp.success_rate) AS avg_success_rate,
    AVG(fsp.avg_synthesis_time) AS avg_synthesis_time
FROM
    fact_synthesis_performance fsp
JOIN
    dim_time dt ON fsp.time_id = dt.time_id
JOIN
    dim_language dl ON fsp.language_id = dl.language_id
JOIN
    dim_explanation_level del ON fsp.explanation_level_id = del.level_id
GROUP BY
    dt.year_actual,
    dt.quarter_actual,
    dt.month_actual,
    dt.month_name,
    dl.language_name,
    del.level_name
ORDER BY
    dt.year_actual,
    dt.quarter_actual,
    dt.month_actual,
    dl.language_name,
    del.level_name;

-- View for language comparison
CREATE VIEW view_language_comparison AS
SELECT
    dl.language_name,
    dl.paradigm,
    COUNT(fsj.job_id) AS total_jobs,
    AVG(CASE WHEN fsj.success THEN 1.0 ELSE 0.0 END) AS success_rate,
    AVG(fsj.synthesis_time_seconds) AS avg_synthesis_time,
    AVG(fsj.program_complexity) AS avg_program_complexity,
    AVG(fsj.program_size_bytes) AS avg_program_size
FROM
    fact_synthesis_job fsj
JOIN
    dim_language dl ON fsj.language_id = dl.language_id
GROUP BY
    dl.language_name,
    dl.paradigm
ORDER BY
    total_jobs DESC;

-- View for explanation level effectiveness
CREATE VIEW view_explanation_level_effectiveness AS
SELECT
    del.level_name,
    dl.language_name,
    COUNT(fsj.job_id) AS total_jobs,
    AVG(CASE WHEN fsj.success THEN 1.0 ELSE 0.0 END) AS success_rate,
    AVG(fsj.synthesis_time_seconds) AS avg_synthesis_time,
    AVG(fsj.program_complexity) AS avg_program_complexity
FROM
    fact_synthesis_job fsj
JOIN
    dim_explanation_level del ON fsj.explanation_level_id = del.level_id
JOIN
    dim_language dl ON fsj.language_id = dl.language_id
GROUP BY
    del.level_name,
    dl.language_name
ORDER BY
    del.level_name,
    dl.language_name;

-- View for user engagement
CREATE VIEW view_user_engagement AS
SELECT
    du.username,
    du.user_type,
    COUNT(DISTINCT dt.year_actual || '-' || dt.month_actual) AS active_months,
    SUM(fua.job_count) AS total_jobs,
    AVG(fua.success_rate) AS avg_success_rate,
    AVG(fua.avg_synthesis_time) AS avg_synthesis_time,
    MAX(fua.distinct_languages) AS max_distinct_languages
FROM
    fact_user_activity fua
JOIN
    dim_user du ON fua.user_id = du.user_id
JOIN
    dim_time dt ON fua.time_id = dt.time_id
WHERE
    dt.date_actual >= CURRENT_DATE - INTERVAL '12 months'
GROUP BY
    du.username,
    du.user_type
ORDER BY
    total_jobs DESC;

-- View for pattern effectiveness
CREATE VIEW view_pattern_effectiveness AS
SELECT
    dpp.pattern_name,
    dpp.pattern_type,
    dl.language_name,
    SUM(feu.usage_count) AS total_usage,
    AVG(feu.success_rate) AS avg_success_rate,
    AVG(feu.avg_synthesis_time) AS avg_synthesis_time
FROM
    fact_example_usage feu
JOIN
    dim_program_pattern dpp ON feu.pattern_id = dpp.pattern_id
JOIN
    dim_language dl ON feu.language_id = dl.language_id
GROUP BY
    dpp.pattern_name,
    dpp.pattern_type,
    dl.language_name
ORDER BY
    total_usage DESC;
#+end_src

#+begin_src python :tangle doc/future_work/data_warehouse_etl.py
#!/usr/bin/env python3
"""
ETL process for the Reversible Meta-Synthesis Data Warehouse.
"""

import os
import sys
import logging
import psycopg2
from psycopg2.extras import DictCursor
import datetime
import yaml
import argparse
from typing import Dict, List, Any, Optional
import pandas as pd
import json

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('/var/log/synthesis-dw-etl.log')
    ]
)
logger = logging.getLogger("synthesis-dw-etl")

class DataWarehouseETL:
    """
    ETL process for the Reversible Meta-Synthesis Data Warehouse.
    """
    
    def __init__(self, config_file: str):
        """
        Initialize the ETL process
        
        Args:
            config_file: Path to the configuration file
        """
        # Load configuration
        with open(config_file, 'r') as f:
            self.config = yaml.safe_load(f)
        
        # Validate configuration
        self._validate_config()
    
    def _validate_config(self):
        """Validate the configuration"""
        required_keys = [
            'source_db.host',
            'source_db.port',
            'source_db.name',
            'source_db.user',
            'source_db.password',
            'data_warehouse.host',
            'data_warehouse.port',
            'data_warehouse.name',
            'data_warehouse.user',
            'data_warehouse.password',
            'etl.batch_size',
            'etl.history_days'
        ]
        
        for key in required_keys:
            parts = key.split('.')
            config = self.config
            for part in parts:
                if part not in config:
                    raise ValueError(f"Missing required configuration key: {key}")
                config = config[part]
    
    def connect_source(self):
        """Create a connection to the source database"""
        return psycopg2.connect(
            host=self.config['source_db']['host'],
            port=self.config['source_db']['port'],
            dbname=self.config['source_db']['name'],
            user=self.config['source_db']['user'],
            password=self.config['source_db']['password'],
            cursor_factory=DictCursor
        )
    
    def connect_warehouse(self):
        """Create a connection to the data warehouse"""
        return psycopg2.connect(
            host=self.config['data_warehouse']['host'],
            port=self.config['data_warehouse']['port'],
            dbname=self.config['data_warehouse']['name'],
            user=self.config['data_warehouse']['user'],
            password=self.config['data_warehouse']['password'],
            cursor_factory=DictCursor
        )
    
    def run_full_etl(self):
        """Run the full ETL process"""
        logger.info("Starting full ETL process")
        
        try:
            # Initialize time dimension
            self._initialize_time_dimension()
            
            # Run master ETL procedure
            with self.connect_warehouse() as conn:
                with conn.cursor() as cur:
                    cur.execute("CALL etl_master()")
                    conn.commit()
            
            logger.info("Full ETL process completed successfully")
            return True
        
        except Exception as e:
            logger.error(f"ETL process failed: {e}", exc_info=True)
            return False
    
    def _initialize_time_dimension(self):
        """Initialize the time dimension table"""
        logger.info("Initializing time dimension")
        
        try:
            # Calculate date range
            end_date = datetime.date.today() + datetime.timedelta(days=365)  # One year in the future
            start_date = datetime.date.today() - datetime.timedelta(days=365*5)  # Five years in the past
            
            # Populate time dimension
            with self.connect_warehouse() as conn:
                with conn.cursor() as cur:
                    cur.execute(
                        "CALL populate_dim_time(%s, %s)",
                        (start_date, end_date)
                    )
                    conn.commit()
            
            logger.info(f"Time dimension initialized from {start_date} to {end_date}")
            return True
        
        except Exception as e:
            logger.error(f"Failed to initialize time dimension: {e}", exc_info=True)
            return False
    
    def extract_transform_jobs(self):
        """Extract and transform synthesis jobs data"""
        logger.info("Extracting synthesis jobs data")
        
        batch_size = self.config['etl']['batch_size']
        history_days = self.config['etl']['history_days']
        
        try:
            # Extract data from source
            with self.connect_source() as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        SELECT 
                            sj.job_id,
                            sj.user_id,
                            sj.language,
                            sj.explanation_level,
                            sj.status,
                            sj.progress,
                            sj.created_at,
                            sj.completed_at,
                            sa.synthesis_time,
                            sa.cpu_usage,
                            sa.memory_usage,
                            sa.program_complexity,
                            sr.program,
                            sr.explanation_tree,
                            sr.metrics,
                            js.examples,
                            js.constraints,
                            ss.strategy_id,
                            ss.name as strategy_name
                        FROM 
                            synthesis_jobs sj
                        LEFT JOIN 
                            synthesis_analytics sa ON sj.job_id = sa.job_id
                        LEFT JOIN 
                            synthesis_results sr ON sj.job_id = sr.job_id
                        LEFT JOIN 
                            job_specifications js ON sj.job_id = js.job_id
                        LEFT JOIN 
                            synthesis_analytics sa2 ON sj.job_id = sa2.job_id
                        LEFT JOIN 
                            synthesis_strategies ss ON sa2.strategy_id = ss.strategy_id
                        WHERE 
                            sj.created_at >= CURRENT_TIMESTAMP - INTERVAL '%s days'
                        ORDER BY 
                            sj.created_at DESC
                        LIMIT %s
                    """, (history_days, batch_size))
                    
                    jobs = cur.fetchall()
            
            logger.info(f"Extracted {len(jobs)} synthesis jobs")
            
            # Transform data
            transformed_jobs = []
            for job in jobs:
                transformed_job = {
                    'job_id': job['job_id'],
                    'user_id': job['user_id'],
                    'language': job['language'],
                    'explanation_level': job['explanation_level'],
                    'status': job['status'],
                    'created_date': job['created_at'].date(),
                    'synthesis_time_seconds': job['synthesis_time'] / 1000 if job['synthesis_time'] else 0,
                    'cpu_usage_percent': job['cpu_usage'] or 0,
                    'memory_usage_mb': job['memory_usage'] / (1024*1024) if job['memory_usage'] else 0,
                    'program_complexity': job['program_complexity'] or 0,
                    'program_size_bytes': len(job['program']) if job['program'] else 0,
                    'success': job['status'] == 'completed',
                    'explanation_tree_nodes': self._count_explanation_nodes(job['explanation_tree']),
                    'example_count': len(job['examples']) if job['examples'] else 0,
                    'strategy_name': job['strategy_name']
                }
                transformed_jobs.append(transformed_job)
            
            return transformed_jobs
        
        except Exception as e:
            logger.error(f"Failed to extract and transform jobs data: {e}", exc_info=True)
            return []
    
    def _count_explanation_nodes(self, explanation_tree):
        """Count the number of nodes in an explanation tree"""
        if not explanation_tree:
            return 0
        
        try:
            return self._count_nodes_recursive(explanation_tree)
        except:
            return 0
    
    def _count_nodes_recursive(self, node):
        """Recursively count nodes in a tree"""
        if not node or not isinstance(node, dict):
            return 0
        
        count = 1  # Count this node
        
        # Count children
        children = node.get('children', [])
        for child in children:
            count += self._count_nodes_recursive(child)
        
        return count
    
    def extract_transform_patterns(self):
        """Extract and transform program patterns data"""
        logger.info("Extracting program patterns data")
        
        try:
            # Extract data from source
            with self.connect_source() as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        SELECT 
                            pattern_id,
                            language,
                            pattern_type,
                            pattern_structure,
                            description,
                            usage_count
                        FROM 
                            program_patterns
                        ORDER BY 
                            usage_count DESC
                    """)
                    
                    patterns = cur.fetchall()
            
            logger.info(f"Extracted {len(patterns)} program patterns")
            
            # Transform data
            transformed_patterns = []
            for pattern in patterns:
                # Calculate complexity level based on pattern structure
                complexity_level = self._calculate_pattern_complexity(pattern['pattern_structure'])
                
                transformed_pattern = {
                    'pattern_id': pattern['pattern_id'],
                    'pattern_type': pattern['pattern_type'],
                    'pattern_name': f"{pattern['language']}_{pattern['pattern_type']}",
                    'pattern_structure': pattern['pattern_structure'],
                    'complexity_level': complexity_level,
                    'description': pattern['description']
                }
                transformed_patterns.append(transformed_pattern)
            
            return transformed_patterns
        
        except Exception as e:
            logger.error(f"Failed to extract and transform patterns data: {e}", exc_info=True)
            return []
    
    def _calculate_pattern_complexity(self, pattern_structure):
        """Calculate complexity level of a pattern"""
        if not pattern_structure:
            return 1
        
        try:
            # Simple heuristic based on structure properties
            structure = pattern_structure
            
            # Count various complexity factors
            recursion = structure.get('recursion', 0)
            functions = structure.get('functions', 0)
            conditionals = structure.get('conditionals', 0)
            higher_order = structure.get('higher_order', 0)
            
            # Calculate weighted complexity
            complexity = (
                recursion * 2 +
                functions * 1 +
                conditionals * 1.5 +
                higher_order * 2
            )
            
            # Map to discrete levels
            if complexity < 2:
                return 1  # Simple
            elif complexity < 5:
                return 2  # Moderate
            elif complexity < 10:
                return 3  # Complex
            else:
                return 4  # Very complex
        
        except:
            return 1  # Default to simple
    
    def load_data(self, jobs, patterns):
        """Load transformed data into the data warehouse"""
        logger.info("Loading data into the data warehouse")
        
        try:
            # Load program patterns
            self._load_patterns(patterns)
            
            # Run ETL procedures
            with self.connect_warehouse() as conn:
                with conn.cursor() as cur:
                    cur.execute("CALL etl_master()")
                    conn.commit()
            
            logger.info("Data loaded successfully")
            return True
        
        except Exception as e:
            logger.error(f"Failed to load data: {e}", exc_info=True)
            return False
    
    def _load_patterns(self, patterns):
        """Load program patterns into the data warehouse"""
        if not patterns:
            return
        
        logger.info(f"Loading {len(patterns)} program patterns")
        
        try:
            with self.connect_warehouse() as conn:
                with conn.cursor() as cur:
                    for pattern in patterns:
                        # Check if pattern already exists
                        cur.execute("""
                            SELECT pattern_id 
                            FROM dim_program_pattern
                            WHERE pattern_name = %s AND pattern_type = %s
                        """, (pattern['pattern_name'], pattern['pattern_type']))
                        
                        existing = cur.fetchone()
                        
                        if existing:
                            # Update existing pattern
                            cur.execute("""
                                UPDATE dim_program_pattern
                                SET pattern_structure = %s,
                                    complexity_level = %s,
                                    description = %s
                                WHERE pattern_id = %s
                            """, (
                                json.dumps(pattern['pattern_structure']),
                                pattern['complexity_level'],
                                pattern['description'],
                                existing['pattern_id']
                            ))
                        else:
                            # Insert new pattern
                            cur.execute("""
                                INSERT INTO dim_program_pattern (
                                    pattern_type,
                                    pattern_name,
                                    pattern_structure,
                                    complexity_level,
                                    description
                                ) VALUES (%s, %s, %s, %s, %s)
                            """, (
                                pattern['pattern_type'],
                                pattern['pattern_name'],
                                json.dumps(pattern['pattern_structure']),
                                pattern['complexity_level'],
                                pattern['description']
                            ))
                
                conn.commit()
        
        except Exception as e:
            logger.error(f"Failed to load patterns: {e}", exc_info=True)
            raise
    
    def generate_warehouse_reports(self, output_dir: str):
        """Generate reports from the data warehouse"""
        logger.info("Generating data warehouse reports")
        
        try:
            os.makedirs(output_dir, exist_ok=True)
            
            # List of reports to generate
            reports = [
                ('synthesis_success_trends', 'view_synthesis_success_trends'),
                ('language_comparison', 'view_language_comparison'),
                ('explanation_level_effectiveness', 'view_explanation_level_effectiveness'),
                ('user_engagement', 'view_user_engagement'),
                ('pattern_effectiveness', 'view_pattern_effectiveness')
            ]
            
            # Generate each report
            for report_name, view_name in reports:
                self._generate_report(report_name, view_name, output_dir)
            
            logger.info(f"Reports generated successfully in {output_dir}")
            return True
        
        except Exception as e:
            logger.error(f"Failed to generate reports: {e}", exc_info=True)
            return False
    
    def _generate_report(self, report_name, view_name, output_dir):
        """Generate a single report from a view"""
        logger.info(f"Generating report: {report_name}")
        
        try:
            # Query the view
            with self.connect_warehouse() as conn:
                df = pd.read_sql(f"SELECT * FROM {view_name}", conn)
            
            # Save as CSV
            csv_path = os.path.join(output_dir, f"{report_name}.csv")
            df.to_csv(csv_path, index=False)
            
            # Save as JSON
            json_path = os.path.join(output_dir, f"{report_name}.json")
            df.to_json(json_path, orient='records', date_format='iso')
            
            logger.info(f"Report {report_name} generated with {len(df)} rows")
        
        except Exception as e:
            logger.error(f"Failed to generate report {report_name}: {e}", exc_info=True)
            raise

# Example usage
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='ETL process for Reversible Meta-Synthesis Data Warehouse')
    parser.add_argument('--config', type=str, default='dw_config.yaml', help='Path to configuration file')
    parser.add_argument('--action', type=str, choices=['full-etl', 'generate-reports'], default='full-etl', help='Action to perform')
    parser.add_argument('--output-dir', type=str, default='reports', help='Output directory for reports')
    args = parser.parse_args()
    
    etl = DataWarehouseETL(args.config)
    
    if args.action == 'full-etl':
        success = etl.run_full_etl()
        sys.exit(0 if success else 1)
    
    elif args.action == 'generate-reports':
        success = etl.generate_warehouse_reports(args.output_dir)
        sys.exit(0 if success else 1)
    
    else:
        parser.print_help()
        sys.exit(1)
#+end_src

#+begin_src yaml :tangle doc/future_work/dw_config.yaml
# Configuration for Data Warehouse ETL process

# Source Database (OLTP)
source_db:
  host: db.reversible-meta-synthesis.com
  port: 5432
  name: synthesis
  user: etl_user
  password: "YOUR_PASSWORD"

# Data Warehouse (OLAP)
data_warehouse:
  host: dw.reversible-meta-synthesis.com
  port: 5432
  name: synthesis_dw
  user: dw_admin
  password: "YOUR_DW_PASSWORD"

# ETL Configuration
etl:
  batch_size: 10000       # Number of records to process at once
  history_days: 365       # Number of days of history to process
  schedule: "0 2 * * *"   # Daily at 2 AM

# Reporting Configuration
reporting:
  output_dir: /var/data/synthesis/reports
  formats:
    - csv
    - json
  schedule: "0 6 * * *"   # Daily at 6 AM

# Logging Configuration
logging:
  level: INFO
  file: /var/log/synthesis-dw-etl.log
  retention_days: 30
#+end_src

*** Real-Time Performance Monitoring

Implement real-time monitoring and alerting for the cloud synthesis service.

#+begin_src yaml :tangle doc/future_work/monitoring_config.yaml
# Monitoring Configuration for Reversible Meta-Synthesis Service

# Prometheus Configuration
prometheus:
  scrape_interval: 15s   # How frequently to scrape targets
  evaluation_interval: 15s  # How frequently to evaluate rules
  
  # Alertmanager configuration
  alerting:
    alertmanagers:
      - static_configs:
          - targets: ['alertmanager:9093']
  
  # Rule files
  rule_files:
    - /etc/prometheus/rules/*.yml
  
  # Scrape configurations
  scrape_configs:
    # API Gateway monitoring
    - job_name: 'api-gateway'
      metrics_path: /metrics
      static_configs:
        - targets: ['api-gateway:8080']
    
    # Synthesis service monitoring
    - job_name: 'synthesis-service'
      metrics_path: /metrics
      static_configs:
        - targets: ['synthesis-service:8000']
      
    # Worker monitoring
    - job_name: 'synthesis-workers'
      metrics_path: /metrics
      dns_sd_configs:
        - names:
            - 'worker.synthesis.svc.cluster.local'
          type: 'A'
          port: 8000
    
    # Database monitoring
    - job_name: 'postgres-exporter'
      static_configs:
        - targets: ['postgres-exporter:9187']
    
    # Node monitoring
    - job_name: 'node-exporter'
      static_configs:
        - targets: ['node-exporter:9100']
    
    # Kubernetes monitoring
    - job_name: 'kubernetes-apiservers'
      kubernetes_sd_configs:
        - role: endpoints
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
        - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
          action: keep
          regex: default;kubernetes;https
    
    # Redis monitoring
    - job_name: 'redis-exporter'
      static_configs:
        - targets: ['redis-exporter:9121']

# Alertmanager Configuration
alertmanager:
  route:
    group_by: ['alertname', 'cluster', 'service']
    group_wait: 30s
    group_interval: 5m
    repeat_interval: 4h
    receiver: 'synthesis-team'
    routes:
      - match:
          severity: critical
        receiver: 'synthesis-team-pager'
        continue: true
  
  receivers:
    - name: 'synthesis-team'
      email_configs:
        - to: 'team@reversible-meta-synthesis.com'
          send_resolved: true
      slack_configs:
        - api_url: 'https://hooks.slack.com/services/YOUR_SLACK_WEBHOOK'
          channel: '#synthesis-alerts'
          send_resolved: true
          title: '{{ template "slack.default.title" . }}'
          text: '{{ template "slack.default.text" . }}'
    
    - name: 'synthesis-team-pager'
      pagerduty_configs:
        - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
          send_resolved: true

# Alert Rules
rules:
  # Synthesis Service Health
  - name: synthesis-service.rules
    rules:
      - alert: SynthesisServiceDown
        expr: up{job="synthesis-service"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Synthesis service is down"
          description: "The synthesis service instance {{ $labels.instance }} has been down for more than 2 minutes."
      
      - alert: HighSynthesisLatency
        expr: histogram_quantile(0.95, sum(rate(synthesis_job_duration_seconds_bucket[5m])) by (le)) > 300
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High synthesis latency"
          description: "95th percentile synthesis time is above 5 minutes (300s) for the last 5 minutes."
      
      - alert: LowSynthesisSuccessRate
        expr: (sum(rate(synthesis_success_total[1h])) / sum(rate(synthesis_attempts_total[1h]))) < 0.8
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Low synthesis success rate"
          description: "Synthesis success rate has dropped below 80% for the last 15 minutes."
  
  # Worker Health
  - name: synthesis-workers.rules
    rules:
      - alert: WorkerDown
        expr: up{job="synthesis-workers"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Synthesis worker is down"
          description: "The synthesis worker {{ $labels.instance }} has been down for more than 2 minutes."
      
      - alert: HighWorkerLoad
        expr: sum(rate(synthesis_job_duration_seconds_sum[5m])) by (instance) / sum(rate(synthesis_job_duration_seconds_count[5m])) by (instance) > 0.8
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High worker load"
          description: "The synthesis worker {{ $labels.instance }} has been under high load (>80% utilization) for the last 10 minutes."
  
  # Database Health
  - name: database.rules
    rules:
      - alert: DatabaseDown
        expr: pg_up{job="postgres-exporter"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Database is down"
          description: "The synthesis database has been down for more than 1 minute."
      
      - alert: HighDatabaseLoad
        expr: rate(pg_stat_database_xact_commit{datname="synthesis"}[1m]) + rate(pg_stat_database_xact_rollback{datname="synthesis"}[1m]) > 1000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High database load"
          description: "The synthesis database is experiencing high transaction rate (>1000 TPS) for the last 5 minutes."
      
      - alert: LowDatabaseConnections
        expr: sum(pg_stat_database_numbackends{datname="synthesis"}) < 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Low database connections"
          description: "The synthesis database has fewer than 5 connections for the last 5 minutes, indicating potential connectivity issues."
  
  # System Health
  - name: system.rules
    rules:
      - alert: HighCPULoad
        expr: 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "High CPU load"
          description: "CPU load on {{ $labels.instance }} is above 80% for the last 15 minutes."
      
      - alert: HighMemoryUsage
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 85
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Memory usage on {{ $labels.instance }} is above 85% for the last 15 minutes."
      
      - alert: DiskWillFillIn24Hours
        expr: predict_linear(node_filesystem_free_bytes{mountpoint="/"}[1h], 24 * 3600) < 0
        for: 30m
        labels:
          severity: critical
        annotations:
          summary: "Disk will fill in 24 hours"
          description: "Disk on {{ $labels.instance }} is predicted to fill up within 24 hours."

# Grafana Dashboard Configuration
grafana:
  dashboards:
    - name: "Synthesis Service Overview"
      uid: "synthesis-overview"
      panels:
        - title: "Synthesis Jobs"
          type: "graph"
          targets:
            - expr: "sum(rate(synthesis_attempts_total[5m])) by (language, explanation_level)"
          gridPos:
            h: 9
            w: 12
            x: 0
            y: 0
        
        - title: "Success Rate"
          type: "gauge"
          targets:
            - expr: "sum(rate(synthesis_success_total[1h])) / sum(rate(synthesis_attempts_total[1h]))"
          gridPos:
            h: 9
            w: 6
            x: 12
            y: 0
        
        - title: "Synthesis Time"
          type: "graph"
          targets:
            - expr: "histogram_quantile(0.95, sum(rate(synthesis_job_duration_seconds_bucket[5m])) by (le))"
            - expr: "histogram_quantile(0.50, sum(rate(synthesis_job_duration_seconds_bucket[5m])) by (le))"
          gridPos:
            h: 9
            w: 6
            x: 18
            y: 0
        
        - title: "Worker CPU Usage"
          type: "graph"
          targets:
            - expr: "sum(rate(process_cpu_seconds_total{job=\"synthesis-workers\"}[1m])) by (instance)"
          gridPos:
            h: 9
            w: 12
            x: 0
            y: 9
        
        - title: "Worker Memory Usage"
          type: "graph"
          targets:
            - expr: "process_resident_memory_bytes{job=\"synthesis-workers\"}"
          gridPos:
            h: 9
            w: 12
            x: 12
            y: 9
    
    - name: "Database Performance"
      uid: "synthesis-database"
      panels:
        - title: "Database Transactions"
          type: "graph"
          targets:
            - expr: "rate(pg_stat_database_xact_commit{datname=\"synthesis\"}[1m])"
            - expr: "rate(pg_stat_database_xact_rollback{datname=\"synthesis\"}[1m])"
          gridPos:
            h: 9
            w: 12
            x: 0
            y: 0
        
        - title: "Database Connections"
          type: "graph"
          targets:
            - expr: "pg_stat_database_numbackends{datname=\"synthesis\"}"
          gridPos:
            h: 9
            w: 12
            x: 12
            y: 0
        
        - title: "Cache Hit Ratio"
          type: "graph"
          targets:
            - expr: "pg_stat_database_blks_hit{datname=\"synthesis\"} / (pg_stat_database_blks_hit{datname=\"synthesis\"} + pg_stat_database_blks_read{datname=\"synthesis\"})"
          gridPos:
            h: 9
            w: 12
            x: 0
            y: 9
        
        - title: "Slow Queries"
          type: "graph"
          targets:
            - expr: "rate(synthesis_db_slow_queries_total[5m])"
          gridPos:
            h: 9
            w: 12
            x: 12
            y: 9

# Custom metrics for synthesis service
custom_metrics:
  # Synthesis job metrics
  - name: synthesis_attempts_total
    type: counter
    help: "Total number of synthesis attempts"
    labels:
      - language
      - explanation_level
  
  - name: synthesis_success_total
    type: counter
    help: "Total number of successful syntheses"
    labels:
      - language
      - explanation_level
  
  - name: synthesis_job_duration_seconds
    type: histogram
    help: "Duration of synthesis jobs in seconds"
    labels:
      - language
      - explanation_level
    buckets: [1, 5, 10, 30, 60, 120, 300, 600, 1800, 3600]
  
  # Resource usage metrics
  - name: synthesis_worker_memory_usage_bytes
    type: gauge
    help: "Memory usage of synthesis worker in bytes"
    labels:
      - worker_id
  
  - name: synthesis_worker_cpu_usage
    type: gauge
    help: "CPU usage of synthesis worker as a percentage"
    labels:
      - worker_id
  
  # Program complexity metrics
  - name: synthesis_program_complexity
    type: histogram
    help: "Complexity of synthesized programs"
    labels:
      - language
      - explanation_level
    buckets: [1, 2, 3, 5, 8, 13, 21, 34, 55]
  
  # Explanation tree metrics
  - name: synthesis_explanation_tree_size
    type: histogram
    help: "Size of explanation trees in nodes"
    labels:
      - language
      - explanation_level
    buckets: [10, 20, 50, 100, 200, 500, 1000, 2000, 5000]
  
  # Queue metrics
  - name: synthesis_queue_size
    type: gauge
    help: "Number of jobs in the synthesis queue"
  
  - name: synthesis_queue_wait_time_seconds
    type: histogram
    help: "Wait time in the synthesis queue in seconds"
    buckets: [1, 5, 15, 30, 60, 120, 300, 600]

# Log monitoring configuration
loki:
  scrape_configs:
    - job_name: synthesis-service-logs
      static_configs:
        - targets:
            - localhost
          labels:
            job: synthesis-service
            __path__: /var/log/synthesis-service/*.log
    
    - job_name: synthesis-worker-logs
      static_configs:
        - targets:
            - localhost
          labels:
            job: synthesis-worker
            __path__: /var/log/synthesis-worker/*.log
    
    - job_name: database-logs
      static_configs:
        - targets:
            - localhost
          labels:
            job: postgres
            __path__: /var/log/postgresql/*.log

# Log alerting rules
log_alerts:
  - name: synthesis-service-errors
    expr: |
      sum(count_over_time({job="synthesis-service"} |~ "(?i)error"[15m])) > 10
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High error rate in synthesis service logs"
      description: "More than 10 error log entries in the last 15 minutes"
  
  - name: synthesis-worker-critical
    expr: |
      sum(count_over_time({job="synthesis-worker"} |~ "(?i)critical"[15m])) > 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Critical error in synthesis worker"
      description: "Critical error detected in synthesis worker logs"
#+end_src

#+begin_src python :tangle doc/future_work/monitor_synthesis_service.py
#!/usr/bin/env python3
"""
Real-time monitoring for the Reversible Meta-Synthesis service.
"""

import time
import os
import logging
import json
import yaml
import argparse
import threading
import datetime
from typing import Dict, List, Any, Optional
import socket
import psutil
import requests
from prometheus_client import start_http_server, Counter, Gauge, Histogram, Summary

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('/var/log/synthesis-monitoring.log')
    ]
)
logger = logging.getLogger("synthesis-monitoring")

class SynthesisServiceMonitor:
    """
    Real-time monitoring for the Reversible Meta-Synthesis service.
    """
    
    def __init__(self, config_file: str):
        """
        Initialize the service monitor
        
        Args:
            config_file: Path to the configuration file
        """
        # Load configuration
        with open(config_file, 'r') as f:
            self.config = yaml.safe_load(f)
        
        # Initialize metrics
        self._init_metrics()
        
        # Get hostname and worker ID
        self.hostname = socket.gethostname()
        self.worker_id = os.environ.get('WORKER_ID', self.hostname)
        
        # Initialize monitoring state
        self.running = False
        self.monitor_thread = None
    
    def _init_metrics(self):
        """Initialize Prometheus metrics"""
        # Synthesis job metrics
        self.synthesis_attempts = Counter(
            'synthesis_attempts_total',
            'Total number of synthesis attempts',
            ['language', 'explanation_level']
        )
        
        self.synthesis_success = Counter(
            'synthesis_success_total',
            'Total number of successful syntheses',
            ['language', 'explanation_level']
        )
        
        self.synthesis_job_duration = Histogram(
            'synthesis_job_duration_seconds',
            'Duration of synthesis jobs in seconds',
            ['language', 'explanation_level'],
            buckets=[1, 5, 10, 30, 60, 120, 300, 600, 1800, 3600]
        )
        
        # Resource usage metrics
        self.worker_memory_usage = Gauge(
            'synthesis_worker_memory_usage_bytes',
            'Memory usage of synthesis worker in bytes',
            ['worker_id']
        )
        
        self.worker_cpu_usage = Gauge(
            'synthesis_worker_cpu_usage',
            'CPU usage of synthesis worker as a percentage',
            ['worker_id']
        )
        
        # Program complexity metrics
        self.program_complexity = Histogram(
            'synthesis_program_complexity',
            'Complexity of synthesized programs',
            ['language', 'explanation_level'],
            buckets=[1, 2, 3, 5, 8, 13, 21, 34, 55]
        )
        
        # Explanation tree metrics
        self.explanation_tree_size = Histogram(
            'synthesis_explanation_tree_size',
            'Size of explanation trees in nodes',
            ['language', 'explanation_level'],
            buckets=[10, 20, 50, 100, 200, 500, 1000, 2000, 5000]
        )
        
        # Queue metrics
        self.queue_size = Gauge(
            'synthesis_queue_size',
            'Number of jobs in the synthesis queue'
        )
        
        self.queue_wait_time = Histogram(
            'synthesis_queue_wait_time_seconds',
            'Wait time in the synthesis queue in seconds',
            buckets=[1, 5, 15, 30, 60, 120, 300, 600]
        )
    
    def start(self, port: int = 8000):
        """
        Start the monitoring service
        
        Args:
            port: Port to expose Prometheus metrics
        """
        if self.running:
            logger.warning("Monitoring service is already running")
            return
        
        # Start Prometheus metrics server
        start_http_server(port)
        logger.info(f"Prometheus metrics server started on port {port}")
        
        # Start monitoring thread
        self.running = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        
        logger.info("Synthesis service monitoring started")
    
    def stop(self):
        """Stop the monitoring service"""
        self.running = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=10)
        logger.info("Synthesis service monitoring stopped")
    
    def _monitor_loop(self):
        """Main monitoring loop"""
        while self.running:
            try:
                # Monitor system resources
                self._monitor_resources()
                
                # Monitor synthesis queue
                self._monitor_queue()
                
                # Monitor active jobs
                self._monitor_active_jobs()
                
                # Sleep before next iteration
                time.sleep(15)  # 15 seconds
            
            except Exception as e:
                logger.error(f"Error in monitoring loop: {e}", exc_info=True)
                time.sleep(60)  # Wait longer after error
    
    def _monitor_resources(self):
        """Monitor system resources"""
        try:
            # Get CPU and memory usage
            cpu_percent = psutil.cpu_percent(interval=1)
            memory_info = psutil.virtual_memory()
            
            # Update metrics
            self.worker_cpu_usage.labels(worker_id=self.worker_id).set(cpu_percent)
            self.worker_memory_usage.labels(worker_id=self.worker_id).set(memory_info.used)
            
            # Log if resources are constrained
            if cpu_percent > 80:
                logger.warning(f"High CPU usage: {cpu_percent}%")
            
            if memory_info.percent > 80:
                logger.warning(f"High memory usage: {memory_info.percent}%")
        
        except Exception as e:
            logger.error(f"Error monitoring resources: {e}")
    
    def _monitor_queue(self):
        """Monitor synthesis job queue"""
        try:
            # Get queue information from API
            api_url = self.config.get('api', {}).get('url', 'http://localhost:8080')
            response = requests.get(f"{api_url}/api/v1/queue/stats")
            
            if response.status_code == 200:
                queue_stats = response.json()
                
                # Update metrics
                self.queue_size.set(queue_stats.get('size', 0))
                
                # Track wait times for jobs in queue
                for job in queue_stats.get('jobs', []):
                    created_at = datetime.datetime.fromisoformat(job.get('created_at').replace('Z', '+00:00'))
                    wait_time = (datetime.datetime.now(datetime.timezone.utc) - created_at).total_seconds()
                    self.queue_wait_time.observe(wait_time)
                    
                    # Log long-waiting jobs
                    if wait_time > 300:  # 5 minutes
                        logger.warning(f"Job {job.get('job_id')} has been waiting for {wait_time:.1f} seconds")
        
        except Exception as e:
            logger.error(f"Error monitoring queue: {e}")
    
    def _monitor_active_jobs(self):
        """Monitor active synthesis jobs"""
        try:
            # Get active job information from API
            api_url = self.config.get('api', {}).get('url', 'http://localhost:8080')
            response = requests.get(f"{api_url}/api/v1/jobs/active")
            
            if response.status_code == 200:
                active_jobs = response.json()
                
                for job in active_jobs:
                    # Record job metrics
                    language = job.get('language', 'unknown')
                    explanation_level = str(job.get('explanation_level', 0))
                    
                    # Check if job has completed since last check
                    if job.get('status') == 'completed':
                        # Record success
                        self.synthesis_success.labels(
                            language=language,
                            explanation_level=explanation_level
                        ).inc()
                        
                        # Record duration
                        created_at = datetime.datetime.fromisoformat(job.get('created_at').replace('Z', '+00:00'))
                        completed_at = datetime.datetime.fromisoformat(job.get('completed_at').replace('Z', '+00:00'))
                        duration = (completed_at - created_at).total_seconds()
                        
                        self.synthesis_job_duration.labels(
                            language=language,
                            explanation_level=explanation_level
                        ).observe(duration)
                        
                        # Record program complexity
                        if 'program_complexity' in job:
                            self.program_complexity.labels(
                                language=language,
                                explanation_level=explanation_level
                            ).observe(job['program_complexity'])
                        
                        # Record explanation tree size
                        if 'explanation_tree_size' in job:
                            self.explanation_tree_size.labels(
                                language=language,
                                explanation_level=explanation_level
                            ).observe(job['explanation_tree_size'])
                    
                    # Record attempt regardless of outcome
                    if job.get('status') in ['completed', 'failed']:
                        self.synthesis_attempts.labels(
                            language=language,
                            explanation_level=explanation_level
                        ).inc()
        
        except Exception as e:
            logger.error(f"Error monitoring active jobs: {e}")
    
    def record_synthesis_job(self, job_data: Dict[str, Any]):
        """
        Record metrics for a synthesis job
        
        Args:
            job_data: Synthesis job data
        """
        try:
            # Extract job information
            language = job_data.get('language', 'unknown')
            explanation_level = str(job_data.get('explanation_level', 0))
            status = job_data.get('status')
            
            # Record attempt
            self.synthesis_attempts.labels(
                language=language,
                explanation_level=explanation_level
            ).inc()
            
            # Record success if applicable
            if status == 'completed':
                self.synthesis_success.labels(
                    language=language,
                    explanation_level=explanation_level
                ).inc()
            
            # Record duration
            if 'synthesis_time' in job_data:
                synthesis_time = job_data['synthesis_time'] / 1000  # Convert ms to seconds
                self.synthesis_job_duration.labels(
                    language=language,
                    explanation_level=explanation_level
                ).observe(synthesis_time)
            
            # Record program complexity
            if 'program_complexity' in job_data:
                self.program_complexity.labels(
                    language=language,
                    explanation_level=explanation_level
                ).observe(job_data['program_complexity'])
            
            # Record explanation tree size
            if 'explanation_tree_size' in job_data:
                self.explanation_tree_size.labels(
                    language=language,
                    explanation_level=explanation_level
                ).observe(job_data['explanation_tree_size'])
            
            logger.info(f"Recorded metrics for job {job_data.get('job_id')}")
        
        except Exception as e:
            logger.error(f"Error recording job metrics: {e}", exc_info=True)

# Example usage
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Real-time monitoring for Reversible Meta-Synthesis service')
    parser.add_argument('--config', type=str, default='monitoring_config.yaml', help='Path to configuration file')
    parser.add_argument('--port', type=int, default=8000, help='Port to expose Prometheus metrics')
    args = parser.parse_args()
    
    monitor = SynthesisServiceMonitor(args.config)
    
    try:
        # Start monitoring
        monitor.start(port=args.port)
        
        # Keep running until interrupted
        while True:
            time.sleep(1)
    
    except KeyboardInterrupt:
        monitor.stop()
#+end_src

#+begin_src python :tangle doc/future_work/alert_manager.py
#!/usr/bin/env python3
"""
Alert manager for the Reversible Meta-Synthesis service.
"""

import time
import logging
import json
import yaml
import argparse
import threading
import datetime
import requests
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from typing import Dict, List, Any, Optional

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('/var/log/synthesis-alerts.log')
    ]
)
logger = logging.getLogger("synthesis-alerts")

class AlertManager:
    """
    Alert manager for the Reversible Meta-Synthesis service.
    """
    
    def __init__(self, config_file: str):
        """
        Initialize the alert manager
        
        Args:
            config_file: Path to the configuration file
        """
        # Load configuration
        with open(config_file, 'r') as f:
            self.config = yaml.safe_load(f)
        
        # Initialize alert state
        self.active_alerts = {}
        self.resolved_alerts = {}
        
        # Initialize notification handlers
        self._init_notification_handlers()
        
        # Initialize monitoring state
        self.running = False
        self.monitor_thread = None
    
    def _init_notification_handlers(self):
        """Initialize notification handlers"""
        self.notification_handlers = {
            'email': self._send_email_notification,
            'slack': self._send_slack_notification,
            'pagerduty': self._send_pagerduty_notification
        }
    
    def start(self):
        """Start the alert manager"""
        if self.running:
            logger.warning("Alert manager is already running")
            return
        
        # Start monitoring thread
        self.running = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        
        logger.info("Alert manager started")
    
    def stop(self):
        """Stop the alert manager"""
        self.running = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=10)
        logger.info("Alert manager stopped")
    
    def _monitor_loop(self):
        """Main monitoring loop"""
        while self.running:
            try:
                # Check for new alerts
                self._check_alerts()
                
                # Sleep before next iteration
                time.sleep(30)  # 30 seconds
            
            except Exception as e:
                logger.error(f"Error in monitoring loop: {e}", exc_info=True)
                time.sleep(60)  # Wait longer after error
    
    def _check_alerts(self):
        """Check for new alerts from Prometheus"""
        try:
            # Get alerts from Prometheus Alertmanager API
            alertmanager_url = self.config.get('alertmanager', {}).get('url', 'http://localhost:9093')
            response = requests.get(f"{alertmanager_url}/api/v2/alerts")
            
            if response.status_code == 200:
                current_alerts = response.json()
                
                # Process active alerts
                for alert in current_alerts:
                    alert_id = alert.get('fingerprint')
                    if alert_id not in self.active_alerts:
                        # New alert
                        self._handle_new_alert(alert)
                    else:
                        # Update existing alert
                        self._update_alert(alert)
                
                # Check for resolved alerts
                current_alert_ids = {alert.get('fingerprint') for alert in current_alerts}
                for alert_id in list(self.active_alerts.keys()):
                    if alert_id not in current_alert_ids:
                        # Alert has been resolved
                        self._handle_resolved_alert(self.active_alerts[alert_id])
        
        except Exception as e:
            logger.error(f"Error checking alerts: {e}")
    
    def _handle_new_alert(self, alert: Dict[str, Any]):
        """
        Handle a new alert
        
        Args:
            alert: Alert data
        """
        alert_id = alert.get('fingerprint')
        logger.info(f"New alert: {alert.get('labels', {}).get('alertname')} ({alert_id})")
        
        # Store alert
        self.active_alerts[alert_id] = alert
        
        # Send notifications
        self._send_alert_notifications(alert)
    
    def _update_alert(self, alert: Dict[str, Any]):
        """
        Update an existing alert
        
        Args:
            alert: Alert data
        """
        alert_id = alert.get('fingerprint')
        previous_alert = self.active_alerts.get(alert_id)
        
        # Check for severity changes
        if previous_alert and (
            previous_alert.get('labels', {}).get('severity') != 
            alert.get('labels', {}).get('severity')
        ):
            logger.info(f"Alert severity changed for {alert.get('labels', {}).get('alertname')} ({alert_id})")
            
            # Update alert
            self.active_alerts[alert_id] = alert
            
            # Send notifications for severity change
            self._send_alert_notifications(
                alert, 
                subject_prefix="[UPDATED]", 
                message_prefix="Alert severity has changed:"
            )
        else:
            # Just update the alert data
            self.active_alerts[alert_id] = alert
    
    def _handle_resolved_alert(self, alert: Dict[str, Any]):
        """
        Handle a resolved alert
        
        Args:
            alert: Alert data
        """
        alert_id = alert.get('fingerprint')
        logger.info(f"Resolved alert: {alert.get('labels', {}).get('alertname')} ({alert_id})")
        
        # Move from active to resolved
        self.resolved_alerts[alert_id] = alert
        del self.active_alerts[alert_id]
        
        # Send resolved notifications
        self._send_resolved_notifications(alert)
    
    def _send_alert_notifications(self, alert: Dict[str, Any], subject_prefix: str = "", message_prefix: str = ""):
        """
        Send notifications for an alert
        
        Args:
            alert: Alert data
            subject_prefix: Prefix for notification subject
            message_prefix: Prefix for notification message
        """
        severity = alert.get('labels', {}).get('severity', 'unknown')
        
        # Get notification config based on severity
        notification_config = self.config.get('notifications', {}).get(severity, {})
        
        if not notification_config:
            # Use default if no specific config for this severity
            notification_config = self.config.get('notifications', {}).get('default', {})
        
        # Prepare notification content
        alert_name = alert.get('labels', {}).get('alertname', 'Unknown Alert')
        instance = alert.get('labels', {}).get('instance', 'unknown')
        summary = alert.get('annotations', {}).get('summary', 'No summary available')
        description = alert.get('annotations', {}).get('description', 'No description available')
        
        subject = f"{subject_prefix} {severity.upper()} ALERT: {alert_name} on {instance}"
        
        message = f"{message_prefix}\n\n" if message_prefix else ""
        message += f"Alert: {alert_name}\n"
        message += f"Severity: {severity.upper()}\n"
        message += f"Instance: {instance}\n"
        message += f"Summary: {summary}\n"
        message += f"Description: {description}\n\n"
        
        # Add other labels
        message += "Additional Information:\n"
        for label, value in alert.get('labels', {}).items():
            if label not in ['alertname', 'severity', 'instance']:
                message += f"- {label}: {value}\n"
        
        # Send notifications using configured methods
        for method in notification_config.get('methods', []):
            if method in self.notification_handlers:
                self.notification_handlers[method](subject, message, severity)
            else:
                logger.warning(f"Unknown notification method: {method}")
    
    def _send_resolved_notifications(self, alert: Dict[str, Any]):
        """
        Send notifications for a resolved alert
        
        Args:
            alert: Alert data
        """
        severity = alert.get('labels', {}).get('severity', 'unknown')
        
        # Get notification config based on severity
        notification_config = self.config.get('notifications', {}).get(severity, {})
        
        if not notification_config:
            # Use default if no specific config for this severity
            notification_config = self.config.get('notifications', {}).get('default', {})
        
        # Check if we should send resolved notifications
        if not notification_config.get('send_resolved', False):
            return
        
        # Prepare notification content
        alert_name = alert.get('labels', {}).get('alertname', 'Unknown Alert')
        instance = alert.get('labels', {}).get('instance', 'unknown')
        
        subject = f"RESOLVED: {alert_name} on {instance}"
        
        message = f"The following alert has been resolved:\n\n"
        message += f"Alert: {alert_name}\n"
        message += f"Severity: {severity.upper()}\n"
        message += f"Instance: {instance}\n"
        
        # Send notifications using configured methods
        for method in notification_config.get('methods', []):
            if method in self.notification_handlers:
                self.notification_handlers[method](subject, message, severity, resolved=True)
            else:
                logger.warning(f"Unknown notification method: {method}")
    
    def _send_email_notification(self, subject: str, message: str, severity: str, resolved: bool = False):
        """
        Send email notification
        
        Args:
            subject: Email subject
            message: Email message
            severity: Alert severity
            resolved: Whether this is a resolved notification
        """
        try:
            email_config = self.config.get('email', {})
            if not email_config:
                logger.warning("Email notification requested but no email configuration found")
                return
            
            smtp_host = email_config.get('smtp_host')
            smtp_port = email_config.get('smtp_port', 587)
            smtp_user = email_config.get('smtp_user')
            smtp_password = email_config.get('smtp_password')
            
            from_email = email_config.get('from_email')
            
            # Get recipients based on severity
            recipients = email_config.get('recipients', {}).get(severity, [])
            if not recipients:
                # Use default recipients if no specific ones for this severity
                recipients = email_config.get('recipients', {}).get('default', [])
            
            if not recipients:
                logger.warning(f"No email recipients configured for severity: {severity}")
                return
            
            # Create email
            email = MIMEMultipart()
            email['Subject'] = subject
            email['From'] = from_email
            email['To'] = ', '.join(recipients)
            
            # Add body
            email.attach(MIMEText(message, 'plain'))
            
            # Send email
            with smtplib.SMTP(smtp_host, smtp_port) as server:
                if email_config.get('use_tls', True):
                    server.starttls()
                
                if smtp_user and smtp_password:
                    server.login(smtp_user, smtp_password)
                
                server.send_message(email)
            
            logger.info(f"Sent email notification to {len(recipients)} recipients")
        
        except Exception as e:
            logger.error(f"Error sending email notification: {e}", exc_info=True)
    
    def _send_slack_notification(self, subject: str, message: str, severity: str, resolved: bool = False):
        """
        Send Slack notification
        
        Args:
            subject: Notification subject
            message: Notification message
            severity: Alert severity
            resolved: Whether this is a resolved notification
        """
        try:
            slack_config = self.config.get('slack', {})
            if not slack_config:
                logger.warning("Slack notification requested but no Slack configuration found")
                return
            
            webhook_url = slack_config.get('webhook_url')
            if not webhook_url:
                logger.warning("No Slack webhook URL configured")
                return
            
            # Determine channel based on severity
            channel = slack_config.get('channels', {}).get(severity)
            if not channel:
                # Use default channel if no specific one for this severity
                channel = slack_config.get('channels', {}).get('default')
            
            # Determine color based on severity and resolved status
            color = '#00FF00' if resolved else {
                'critical': '#FF0000',
                'warning': '#FFFF00',
                'info': '#0000FF'
            }.get(severity, '#808080')
            
            # Create Slack message
            slack_message = {
                'channel': channel,
                'username': slack_config.get('username', 'Synthesis Alert Bot'),
                'icon_emoji': slack_config.get('icon_emoji', ':robot_face:'),
                'attachments': [
                    {
                        'fallback': subject,
                        'color': color,
                        'title': subject,
                        'text': message,
                        'fields': [
                            {
                                'title': 'Severity',
                                'value': severity.upper(),
                                'short': True
                            },
                            {
                                'title': 'Status',
                                'value': 'Resolved' if resolved else 'Active',
                                'short': True
                            }
                        ],
                        'footer': f"Synthesis Alert Manager • {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
                    }
                ]
            }
            
            # Send to Slack
            response = requests.post(
                webhook_url,
                json=slack_message,
                headers={'Content-Type': 'application/json'}
            )
            
            if response.status_code != 200:
                logger.warning(f"Failed to send Slack notification: {response.text}")
            else:
                logger.info("Sent Slack notification")
        
        except Exception as e:
            logger.error(f"Error sending Slack notification: {e}", exc_info=True)
    
    def _send_pagerduty_notification(self, subject: str, message: str, severity: str, resolved: bool = False):
        """
        Send PagerDuty notification
        
        Args:
            subject: Notification subject
            message: Notification message
            severity: Alert severity
            resolved: Whether this is a resolved notification
        """
        try:
            pagerduty_config = self.config.get('pagerduty', {})
            if not pagerduty_config:
                logger.warning("PagerDuty notification requested but no PagerDuty configuration found")
                return
            
            integration_key = pagerduty_config.get('integration_key')
            if not integration_key:
                logger.warning("No PagerDuty integration key configured")
                return
            
            # Only send critical alerts to PagerDuty by default
            if severity != 'critical' and not pagerduty_config.get('send_non_critical', False):
                logger.info(f"Skipping PagerDuty notification for non-critical alert: {severity}")
                return
            
            # Map severity to PagerDuty severity
            pd_severity = {
                'critical': 'critical',
                'warning': 'warning',
                'info': 'info'
            }.get(severity, 'warning')
            
            # Create unique incident key from alert name and instance
            alert_name = subject.split(':')[1].strip() if ':' in subject else subject
            incident_key = f"synthesis-alert-{alert_name}"
            
            # Create PagerDuty event
            pd_event = {
                'routing_key': integration_key,
                'event_action': 'resolve' if resolved else 'trigger',
                'dedup_key': incident_key,
                'payload': {
                    'summary': subject,
                    'severity': pd_severity,
                    'source': 'Synthesis Alert Manager',
                    'custom_details': {
                        'message': message
                    }
                }
            }
            
            # Send to PagerDuty
            response = requests.post(
                'https://events.pagerduty.com/v2/enqueue',
                json=pd_event,
                headers={'Content-Type': 'application/json'}
            )
            
            if response.status_code != 202:
                logger.warning(f"Failed to send PagerDuty notification: {response.text}")
            else:
                logger.info(f"Sent PagerDuty {'resolution' if resolved else 'alert'}")
        
        except Exception as e:
            logger.error(f"Error sending PagerDuty notification: {e}", exc_info=True)
    
    def send_custom_alert(self, alert_name: str, severity: str, instance: str, summary: str, description: str, labels: Dict[str, str] = None):
        """
        Send a custom alert
        
        Args:
            alert_name: Name of the alert
            severity: Alert severity (critical, warning, info)
            instance: Instance affected
            summary: Alert summary
            description: Alert description
            labels: Additional labels
        """
        # Create alert data
        alert = {
            'fingerprint': f"custom-{alert_name}-{instance}-{int(time.time())}",
            'labels': {
                'alertname': alert_name,
                'severity': severity,
                'instance': instance,
                **(labels or {})
            },
            'annotations': {
                'summary': summary,
                'description': description
            },
            'startsAt': datetime.datetime.now(datetime.timezone.utc).isoformat()
        }
        
        # Handle like a new alert
        self._handle_new_alert(alert)
        
        return alert['fingerprint']
    
    def resolve_custom_alert(self, alert_id: str):
        """
        Resolve a custom alert
        
        Args:
            alert_id: ID of the alert to resolve
        """
        if alert_id in self.active_alerts:
            alert = self.active_alerts[alert_id]
            self._handle_resolved_alert(alert)
            return True
        else:
            logger.warning(f"Attempted to resolve non-existent alert: {alert_id}")
            return False

# Example usage
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Alert manager for Reversible Meta-Synthesis service')
    parser.add_argument('--config', type=str, default='alerts_config.yaml', help='Path to configuration file')
    args = parser.parse_args()
    
    alert_manager = AlertManager(args.config)
    
    try:
        # Start alert manager
        alert_manager.start()
        
        # Example of sending a custom alert
        if False:  # Set to True to test
            alert_id = alert_manager.send_custom_alert(
                alert_name="TestAlert",
                severity="warning",
                instance="test-instance",
                summary="Test alert for demonstration",
                description="This is a test alert to demonstrate the alert manager's functionality."
            )
            
            print(f"Sent test alert with ID: {alert_id}")
            
            # Resolve after 30 seconds
            time.sleep(30)
            alert_manager.resolve_custom_alert(alert_id)
            print(f"Resolved test alert with ID: {alert_id}")
        
        # Keep running until interrupted
        while True:
            time.sleep(1)
    
    except KeyboardInterrupt:
        alert_manager.stop()
#+end_src

#+begin_src yaml :tangle doc/future_work/alerts_config.yaml
# Alert Manager Configuration for Reversible Meta-Synthesis Service

# Alertmanager URL
alertmanager:
  url: http://localhost:9093

# Email Notification Configuration
email:
  smtp_host: smtp.example.com
  smtp_port: 587
  smtp_user: alerts@reversible-meta-synthesis.com
  smtp_password: "YOUR_SMTP_PASSWORD"
  from_email: alerts@reversible-meta-synthesis.com
  use_tls: true
  recipients:
    critical:
      - team-oncall@reversible-meta-synthesis.com
      - admin@reversible-meta-synthesis.com
    warning:
      - team@reversible-meta-synthesis.com
    default:
      - monitoring@reversible-meta-synthesis.com

# Slack Notification Configuration
slack:
  webhook_url: https://hooks.slack.com/services/YOUR_SLACK_WEBHOOK
  username: Synthesis Alert Bot
  icon_emoji: :robot_face:
  channels:
    critical: "#alerts-critical"
    warning: "#alerts-warning"
    default: "#alerts"

# PagerDuty Notification Configuration
pagerduty:
  integration_key: YOUR_PAGERDUTY_INTEGRATION_KEY
  send_non_critical: false

# Notification Configuration by Severity
notifications:
  critical:
    methods:
      - email
      - slack
      - pagerduty
    send_resolved: true
  
  warning:
    methods:
      - email
      - slack
    send_resolved: true
  
  info:
    methods:
      - slack
    send_resolved: false
  
  default:
    methods:
      - email
    send_resolved: false

# Alert Thresholds
thresholds:
  synthesis_time:
    critical: 600  # 10 minutes
    warning: 300   # 5 minutes
  
  success_rate:
    critical: 0.7  # 70%
    warning: 0.8   # 80%
  
  queue_size:
    critical: 50
    warning: 20
  
  queue_wait_time:
    critical: 300  # 5 minutes
    warning: 120   # 2 minutes
  
  cpu_usage:
    critical: 90   # 90%
    warning: 75    # 75%
  
  memory_usage:
    critical: 90   # 90%
    warning: 80    # 80%

# Custom Alert Rules
custom_rules:
  - name: LongRunningSynthesisJob
    expr: "synthesis_job_duration_seconds > 600"
    severity: warning
    summary: "Long-running synthesis job"
    description: "A synthesis job has been running for more than 10 minutes"
  
  - name: SynthesisFailureStreak
    expr: "count(synthesis_attempts_total) - count(synthesis_success_total) > 5"
    severity: critical
    summary: "Multiple synthesis failures in a row"
    description: "More than 5 consecutive synthesis jobs have failed"
  
  - name: DailySuccessRateDrop
    expr: "avg_over_time(synthesis_success_rate[1d]) < 0.8"
    severity: warning
    summary: "Daily synthesis success rate below target"
    description: "The average synthesis success rate over the past day has dropped below 80%"
#+end_src

#+begin_src python :tangle doc/future_work/dashboard_generator.py
#!/usr/bin/env python3
"""
Generate monitoring dashboards for the Reversible Meta-Synthesis service.
"""

import os
import logging
import json
import yaml
import argparse
import requests
from typing import Dict, List, Any, Optional

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('/var/log/dashboard-generator.log')
    ]
)
logger = logging.getLogger("dashboard-generator")

class DashboardGenerator:
    """
    Generate Grafana dashboards for the Reversible Meta-Synthesis service.
    """
    
    def __init__(self, config_file: str):
        """
        Initialize the dashboard generator
        
        Args:
            config_file: Path to the configuration file
        """
        # Load configuration
        with open(config_file, 'r') as f:
            self.config = yaml.safe_load(f)
        
        # Validate configuration
        self._validate_config()
    
    def _validate_config(self):
        """Validate the configuration"""
        required_keys = [
            'grafana.url',
            'grafana.api_key',
            'dashboards.output_dir'
        ]
        
        for key in required_keys:
            parts = key.split('.')
            config = self.config
            for part in parts:
                if part not in config:
                    raise ValueError(f"Missing required configuration key: {key}")
                config = config[part]
    
    def generate_dashboards(self):
        """Generate all dashboards"""
        logger.info("Generating dashboards")
        
        # Create output directory if it doesn't exist
        output_dir = self.config['dashboards']['output_dir']
        os.makedirs(output_dir, exist_ok=True)
        
        # Generate each dashboard
        for dashboard_config in self.config['dashboards'].get('definitions', []):
            self._generate_dashboard(dashboard_config, output_dir)
        
        logger.info(f"Generated {len(self.config['dashboards'].get('definitions', []))} dashboards in {output_dir}")
    
    def _generate_dashboard(self, dashboard_config: Dict[str, Any], output_dir: str):
        """
        Generate a single dashboard
        
        Args:
            dashboard_config: Dashboard configuration
            output_dir: Output directory
        """
        title = dashboard_config.get('title')
        if not title:
            logger.warning("Dashboard configuration missing title, skipping")
            return
        
        logger.info(f"Generating dashboard: {title}")
        
        # Create dashboard JSON
        dashboard = {
            "dashboard": {
                "id": None,
                "uid": dashboard_config.get('uid', None),
                "title": title,
                "description": dashboard_config.get('description', ''),
                "tags": dashboard_config.get('tags', []),
                "timezone": "browser",
                "editable": True,
                "panels": self._generate_panels(dashboard_config.get('panels', [])),
                "time": {
                    "from": "now-6h",
                    "to": "now"
                },
                "refresh": "1m",
                "schemaVersion": 30,
                "version": 1
            },
            "overwrite": True,
            "message": "Dashboard generated by dashboard-generator.py"
        }
        
        # Add folder if specified
        folder = dashboard_config.get('folder')
        if folder:
            dashboard["folderTitle"] = folder
        
        # Save to file
        safe_title = title.lower().replace(' ', '_').replace('-', '_')
        filename = f"{safe_title}.json"
        file_path = os.path.join(output_dir, filename)
        
        with open(file_path, 'w') as f:
            json.dump(dashboard, f, indent=2)
        
        logger.info(f"Saved dashboard to {file_path}")
        
        # Upload to Grafana if configured
        if self.config['grafana'].get('auto_upload', False):
            self._upload_dashboard(dashboard)
    
    def _generate_panels(self, panel_configs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Generate panels for a dashboard
        
        Args:
            panel_configs: Panel configurations
        
        Returns:
            List of panel definitions
        """
        panels = []
        panel_id = 1
        
        for panel_config in panel_configs:
            panel_type = panel_config.get('type', 'graph')
            title = panel_config.get('title', f'Panel {panel_id}')
            
            # Basic panel properties
            panel = {
                "id": panel_id,
                "title": title,
                "type": panel_type,
                "gridPos": panel_config.get('gridPos', {
                    "h": 8,
                    "w": 12,
                    "x": 0,
                    "y": 0
                })
            }
            
            # Add panel-type specific properties
            if panel_type == 'graph':
                panel.update(self._generate_graph_panel(panel_config))
            elif panel_type == 'gauge':
                panel.update(self._generate_gauge_panel(panel_config))
            elif panel_type == 'stat':
                panel.update(self._generate_stat_panel(panel_config))
            elif panel_type == 'table':
                panel.update(self._generate_table_panel(panel_config))
            elif panel_type == 'heatmap':
                panel.update(self._generate_heatmap_panel(panel_config))
            elif panel_type == 'text':
                panel.update(self._generate_text_panel(panel_config))
            else:
                logger.warning(f"Unknown panel type: {panel_type}, using default graph panel")
                panel.update(self._generate_graph_panel(panel_config))
            
            panels.append(panel)
            panel_id += 1
        
        return panels
    
    def _generate_graph_panel(self, panel_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate a graph panel
        
        Args:
            panel_config: Panel configuration
        
        Returns:
            Graph panel definition
        """
        return {
            "datasource": panel_config.get('datasource', 'Prometheus'),
            "targets": self._generate_targets(panel_config.get('targets', [])),
            "fieldConfig": {
                "defaults": {
                    "color": {
                        "mode": "palette-classic"
                    },
                    "custom": {
                        "axisLabel": panel_config.get('yAxis', {}).get('label', ''),
                        "axisPlacement": "auto",
                        "barAlignment": 0,
                        "drawStyle": "line",
                        "fillOpacity": panel_config.get('fillOpacity', 10),
                        "gradientMode": "none",
                        "hideFrom": {
                            "legend": False,
                            "tooltip": False,
                            "viz": False
                        },
                        "lineInterpolation": "linear",
                        "lineWidth": panel_config.get('lineWidth', 1),
                        "pointSize": panel_config.get('pointSize', 5),
                        "scaleDistribution": {
                            "type": "linear"
                        },
                        "showPoints": "never",
                        "spanNulls": True,
                        "stacking": {
                            "group": "A",
                            "mode": panel_config.get('stacking', 'none')
                        },
                        "thresholdsStyle": {
                            "mode": "off"
                        }
                    },
                    "mappings": [],
                    "thresholds": {
                        "mode": "absolute",
                        "steps": [
                            {
                                "color": "green",
                                "value": None
                            }
                        ]
                    },
                    "unit": panel_config.get('unit', 'none')
                },
                "overrides": []
            },
            "options": {
                "legend": {
                    "calcs": [],
                    "displayMode": "list",
                    "placement": "bottom",
                    "showLegend": True
                },
                "tooltip": {
                    "mode": "single",
                    "sort": "none"
                }
            }
        }
    
    def _generate_gauge_panel(self, panel_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate a gauge panel
        
        Args:
            panel_config: Panel configuration
        
        Returns:
            Gauge panel definition
        """
        thresholds = panel_config.get('thresholds', [
            {"color": "green", "value": None},
            {"color": "orange", "value": 70},
            {"color": "red", "value": 90}
        ])
        
        return {
            "datasource": panel_config.get('datasource', 'Prometheus'),
            "targets": self._generate_targets(panel_config.get('targets', [])),
            "fieldConfig": {
                "defaults": {
                    "mappings": [],
                    "thresholds": {
                        "mode": "absolute",
                        "steps": thresholds
                    },
                    "color": {
                        "mode": "thresholds"
                    },
                    "min": panel_config.get('min', 0),
                    "max": panel_config.get('max', 100),
                    "unit": panel_config.get('unit', 'none')
                },
                "overrides": []
            },
            "options": {
                "reduceOptions": {
                    "values": False,
                    "calcs": ["lastNotNull"],
                    "fields": ""
                },
                "orientation": "auto",
                "showThresholdLabels": panel_config.get('showThresholdLabels', False),
                "showThresholdMarkers": panel_config.get('showThresholdMarkers', True)
            }
        }
    
    def _generate_stat_panel(self, panel_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate a stat panel
        
        Args:
            panel_config: Panel configuration
        
        Returns:
            Stat panel definition
        """
        return {
            "datasource": panel_config.get('datasource', 'Prometheus'),
            "targets": self._generate_targets(panel_config.get('targets', [])),
            "fieldConfig": {
                "defaults": {
                    "mappings": [],
                    "thresholds": {
                        "mode": "absolute",
                        "steps": panel_config.get('thresholds', [
                            {"color": "green", "value": None}
                        ])
                    },
                    "color": {
                        "mode": "thresholds"
                    },
                    "unit": panel_config.get('unit', 'none')
                },
                "overrides": []
            },
            "options": {
                "reduceOptions": {
                    "values": False,
                    "calcs": [panel_config.get('calc', 'lastNotNull')],
                    "fields": ""
                },
                "orientation": "auto",
                "textMode": panel_config.get('textMode', 'auto'),
                "colorMode": panel_config.get('colorMode', 'value'),
                "graphMode": panel_config.get('graphMode', 'area'),
                "justifyMode": "auto"
            }
        }
    
    def _generate_table_panel(self, panel_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate a table panel
        
        Args:
            panel_config: Panel configuration
        
        Returns:
            Table panel definition
        """
        return {
            "datasource": panel_config.get('datasource', 'Prometheus'),
            "targets": self._generate_targets(panel_config.get('targets', [])),
            "fieldConfig": {
                "defaults": {
                    "mappings": [],
                    "thresholds": {
                        "mode": "absolute",
                        "steps": [
                            {"color": "green", "value": None}
                        ]
                    },
                    "color": {
                        "mode": "thresholds"
                    }
                },
                "overrides": []
            },
            "options": {
                "showHeader": True,
                "footer": {
                    "show": False,
                    "reducer": ["sum"],
                    "countRows": False
                }
            }
        }


def _generate_heatmap_panel(self, panel_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate a heatmap panel
        
        Args:
            panel_config: Panel configuration
        
        Returns:
            Heatmap panel definition
        """
        return {
            "datasource": panel_config.get('datasource', 'Prometheus'),
            "targets": self._generate_targets(panel_config.get('targets', [])),
            "fieldConfig": {
                "defaults": {
                    "custom": {
                        "hideFrom": {
                            "legend": False,
                            "tooltip": False,
                            "viz": False
                        }
                    }
                },
                "overrides": []
            },
            "options": {
                "calculate": True,
                "calculation": {
                    "xBuckets": panel_config.get('xBuckets', None),
                    "yBuckets": panel_config.get('yBuckets', None)
                },
                "cellGap": 1,
                "cellSize": 10,
                "cellValues": {},
                "color": {
                    "exponent": 0.5,
                    "fill": "dark-orange",
                    "mode": "scheme",
                    "reverse": False,
                    "scale": "exponential",
                    "scheme": panel_config.get('colorScheme', 'Oranges'),
                    "steps": 64
                },
                "yAxis": {
                    "axisPlacement": "left",
                    "reverse": False,
                    "unit": panel_config.get('yAxisUnit', 'none')
                },
                "tooltip": {
                    "show": True
                }
            }
        }
    
    def _generate_text_panel(self, panel_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate a text panel
        
        Args:
            panel_config: Panel configuration
        
        Returns:
            Text panel definition
        """
        return {
            "options": {
                "content": panel_config.get('content', ''),
                "mode": panel_config.get('mode', 'markdown')
            }
        }
    
    def _generate_targets(self, target_configs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Generate targets for a panel
        
        Args:
            target_configs: Target configurations
        
        Returns:
            List of target definitions
        """
        targets = []
        
        for i, target_config in enumerate(target_configs):
            if 'expr' not in target_config:
                logger.warning(f"Target configuration missing expr, skipping")
                continue
            
            target = {
                "expr": target_config['expr'],
                "refId": target_config.get('refId', chr(65 + i)),
                "instant": target_config.get('instant', False),
                "legendFormat": target_config.get('legend', ''),
                "interval": target_config.get('interval', '')
            }
            
            targets.append(target)
        
        return targets
    
    def upload_dashboards(self):
        """Upload all generated dashboards to Grafana"""
        logger.info("Uploading dashboards to Grafana")
        
        output_dir = self.config['dashboards']['output_dir']
        
        # Get list of dashboard files
        dashboard_files = [f for f in os.listdir(output_dir) if f.endswith('.json')]
        
        # Upload each dashboard
        for filename in dashboard_files:
            file_path = os.path.join(output_dir, filename)
            self._upload_dashboard_file(file_path)
        
        logger.info(f"Uploaded {len(dashboard_files)} dashboards to Grafana")
    
    def _upload_dashboard(self, dashboard: Dict[str, Any]):
        """
        Upload a dashboard to Grafana
        
        Args:
            dashboard: Dashboard definition
        """
        try:
            grafana_url = self.config['grafana']['url']
            api_key = self.config['grafana']['api_key']
            
            url = f"{grafana_url}/api/dashboards/db"
            
            headers = {
                'Authorization': f"Bearer {api_key}",
                'Content-Type': 'application/json'
            }
            
            response = requests.post(url, json=dashboard, headers=headers)
            
            if response.status_code not in [200, 201]:
                logger.warning(f"Failed to upload dashboard: {response.text}")
            else:
                result = response.json()
                dashboard_url = f"{grafana_url}{result['url']}"
                logger.info(f"Dashboard uploaded successfully: {dashboard_url}")
        
        except Exception as e:
            logger.error(f"Error uploading dashboard: {e}", exc_info=True)
    
    def _upload_dashboard_file(self, file_path: str):
        """
        Upload a dashboard file to Grafana
        
        Args:
            file_path: Path to dashboard file
        """
        try:
            with open(file_path, 'r') as f:
                dashboard = json.load(f)
            
            self._upload_dashboard(dashboard)
        
        except Exception as e:
            logger.error(f"Error uploading dashboard file {file_path}: {e}", exc_info=True)

# Example usage
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Generate dashboards for Reversible Meta-Synthesis service')
    parser.add_argument('--config', type=str, default='dashboard_config.yaml', help='Path to configuration file')
    parser.add_argument('--action', type=str, choices=['generate', 'upload', 'both'], default='both', help='Action to perform')
    args = parser.parse_args()
    
    generator = DashboardGenerator(args.config)
    
    if args.action in ['generate', 'both']:
        generator.generate_dashboards()
    
    if args.action in ['upload', 'both']:
        generator.upload_dashboards()
#+end_src

#+begin_src yaml :tangle doc/future_work/dashboard_config.yaml
# Dashboard Generator Configuration for Reversible Meta-Synthesis Service

# Grafana Configuration
grafana:
  url: http://grafana.reversible-meta-synthesis.com
  api_key: YOUR_GRAFANA_API_KEY
  auto_upload: false

# Dashboard Configuration
dashboards:
  output_dir: /var/data/synthesis/dashboards
  
  definitions:
    # Synthesis Service Overview Dashboard
    - title: Synthesis Service Overview
      uid: synthesis-overview
      description: Overview of the Reversible Meta-Synthesis service
      tags:
        - synthesis
        - overview
      folder: Synthesis Monitoring
      panels:
        # Synthesis Jobs
        - title: Synthesis Jobs
          type: graph
          gridPos:
            h: 8
            w: 12
            x: 0
            y: 0
          targets:
            - expr: sum(rate(synthesis_attempts_total[5m])) by (language)
              legend: "{{language}}"
          unit: jobs/s
          yAxis:
            label: Jobs per second
        
        # Success Rate
        - title: Synthesis Success Rate
          type: gauge
          gridPos:
            h: 8
            w: 6
            x: 12
            y: 0
          targets:
            - expr: sum(rate(synthesis_success_total[1h])) / sum(rate(synthesis_attempts_total[1h]))
          min: 0
          max: 1
          unit: percentunit
          thresholds:
            - value: null
              color: red
            - value: 0.7
              color: orange
            - value: 0.9
              color: green
        
        # Synthesis Time
        - title: Synthesis Time (95th Percentile)
          type: graph
          gridPos:
            h: 8
            w: 6
            x: 18
            y: 0
          targets:
            - expr: histogram_quantile(0.95, sum(rate(synthesis_job_duration_seconds_bucket[5m])) by (le, language, explanation_level))
              legend: "{{language}} (level {{explanation_level}})"
          unit: s
          yAxis:
            label: Seconds
        
        # Queue Size
        - title: Queue Size
          type: stat
          gridPos:
            h: 4
            w: 6
            x: 0
            y: 8
          targets:
            - expr: synthesis_queue_size
          colorMode: value
          graphMode: area
          textMode: value
        
        # Active Jobs
        - title: Active Jobs
          type: stat
          gridPos:
            h: 4
            w: 6
            x: 6
            y: 8
          targets:
            - expr: count(synthesis_worker_cpu_usage)
          colorMode: value
          graphMode: area
          textMode: value
        
        # Program Complexity
        - title: Program Complexity
          type: graph
          gridPos:
            h: 8
            w: 12
            x: 12
            y: 8
          targets:
            - expr: histogram_quantile(0.5, sum(rate(synthesis_program_complexity_bucket[1h])) by (le, language))
              legend: "{{language}} (median)"
          unit: none
          yAxis:
            label: Complexity Score
        
        # Worker CPU Usage
        - title: Worker CPU Usage
          type: graph
          gridPos:
            h: 8
            w: 12
            x: 0
            y: 12
          targets:
            - expr: synthesis_worker_cpu_usage
              legend: "{{worker_id}}"
          unit: percent
          yAxis:
            label: CPU Usage
        
        # Worker Memory Usage
        - title: Worker Memory Usage
          type: graph
          gridPos:
            h: 8
            w: 12
            x: 12
            y: 12
          targets:
            - expr: synthesis_worker_memory_usage_bytes / 1024 / 1024
              legend: "{{worker_id}}"
          unit: megabytes
          yAxis:
            label: Memory Usage (MB)
        
        # Success Rate by Language
        - title: Success Rate by Language
          type: table
          gridPos:
            h: 8
            w: 12
            x: 0
            y: 20
          targets:
            - expr: sum by (language) (rate(synthesis_success_total[24h])) / sum by (language) (rate(synthesis_attempts_total[24h]))
              instant: true
        
        # Average Synthesis Time by Language and Level
        - title: Average Synthesis Time by Language and Level
          type: table
          gridPos:
            h: 8
            w: 12
            x: 12
            y: 20
          targets:
            - expr: avg by (language, explanation_level) (rate(synthesis_job_duration_seconds_sum[24h]) / rate(synthesis_job_duration_seconds_count[24h]))
              instant: true
        
        # Explanation Tree Size Heatmap
        - title: Explanation Tree Size Distribution
          type: heatmap
          gridPos:
            h: 8
            w: 24
            x: 0
            y: 28
          targets:
            - expr: sum(rate(synthesis_explanation_tree_size_bucket[1h])) by (le, explanation_level)
              legend: "Level {{explanation_level}}"
          yAxisUnit: none
          colorScheme: Blues
    
    # Database Performance Dashboard
    - title: Database Performance
      uid: synthesis-database
      description: Performance metrics for the synthesis database
      tags:
        - synthesis
        - database
      folder: Synthesis Monitoring
      panels:
        # Database Transactions
        - title: Database Transactions
          type: graph
          gridPos:
            h: 8
            w: 12
            x: 0
            y: 0
          targets:
            - expr: rate(pg_stat_database_xact_commit{datname="synthesis"}[1m])
              legend: "Commits"
            - expr: rate(pg_stat_database_xact_rollback{datname="synthesis"}[1m])
              legend: "Rollbacks"
          unit: ops/s
          yAxis:
            label: Transactions per second
        
        # Database Connections
        - title: Database Connections
          type: graph
          gridPos:
            h: 8
            w: 12
            x: 12
            y: 0
          targets:
            - expr: pg_stat_database_numbackends{datname="synthesis"}
          unit: conns
          yAxis:
            label: Active Connections
        
        # Cache Hit Ratio
        - title: Cache Hit Ratio
          type: gauge
          gridPos:
            h: 8
            w: 8
            x: 0
            y: 8
          targets:
            - expr: pg_stat_database_blks_hit{datname="synthesis"} / (pg_stat_database_blks_hit{datname="synthesis"} + pg_stat_database_blks_read{datname="synthesis"})
          min: 0
          max: 1
          unit: percentunit
          showThresholdLabels: false
          showThresholdMarkers: true
          thresholds:
            - value: null
              color: red
            - value: 0.8
              color: orange
            - value: 0.95
              color: green
        
        # Slow Queries
        - title: Slow Queries
          type: graph
          gridPos:
            h: 8
            w: 8
            x: 8
            y: 8
          targets:
            - expr: rate(synthesis_db_slow_queries_total[5m])
          unit: qps
          yAxis:
            label: Queries per second
        
        # Database Size
        - title: Database Size
          type: graph
          gridPos:
            h: 8
            w: 8
            x: 16
            y: 8
          targets:
            - expr: pg_database_size_bytes{datname="synthesis"} / 1024 / 1024 / 1024
              legend: "Database Size"
          unit: GB
          yAxis:
            label: Size (GB)
        
        # Table Sizes
        - title: Table Sizes
          type: graph
          gridPos:
            h: 8
            w: 12
            x: 0
            y: 16
          targets:
            - expr: pg_tables_size / 1024 / 1024
              legend: "{{table}}"
          unit: MiB
          yAxis:
            label: Size (MiB)
        
        # Index Sizes
        - title: Index Sizes
          type: graph
          gridPos:
            h: 8
            w: 12
            x: 12
            y: 16
          targets:
            - expr: pg_indexes_size / 1024 / 1024
              legend: "{{index}}"
          unit: MiB
          yAxis:
            label: Size (MiB)
        
        # Query Performance
        - title: Query Performance
          type: table
          gridPos:
            h: 10
            w: 24
            x: 0
            y: 24
          targets:
            - expr: topk(10, avg by (query_type) (rate(pg_stat_statements_total_time[5m]) / rate(pg_stat_statements_calls[5m])))
              instant: true
    
    # Synthesis Process Dashboard
    - title: Synthesis Process Details
      uid: synthesis-process
      description: Detailed metrics about the synthesis process
      tags:
        - synthesis
        - process
      folder: Synthesis Monitoring
      panels:
        # Explanation Tree Size by Level
        - title: Explanation Tree Size by Level
          type: graph
          gridPos:
            h: 8
            w: 12
            x: 0
            y: 0
          targets:
            - expr: avg by (explanation_level) (rate(synthesis_explanation_tree_size_sum[1h]) / rate(synthesis_explanation_tree_size_count[1h]))
              legend: "Level {{explanation_level}}"
          unit: nodes
          yAxis:
            label: Average Tree Size (nodes)
        
        # Decomposition Usage
        - title: Decomposition Level Distribution
          type: graph
          gridPos:
            h: 8
            w: 12
            x: 12
            y: 0
          targets:
            - expr: count by (explanation_level) (synthesis_job_duration_seconds_count)
              legend: "Level {{explanation_level}}"
          unit: none
          yAxis:
            label: Job Count
        
        # Success Rate by Explanation Level
        - title: Success Rate by Explanation Level
          type: graph
          gridPos:
            h: 8
            w: 12
            x: 0
            y: 8
          targets:
            - expr: sum by (explanation_level) (rate(synthesis_success_total[24h])) / sum by (explanation_level) (rate(synthesis_attempts_total[24h]))
              legend: "Level {{explanation_level}}"
          unit: percentunit
          yAxis:
            label: Success Rate
        
        # Synthesis Time Distribution
        - title: Synthesis Time Distribution
          type: heatmap
          gridPos:
            h: 8
            w: 12
            x: 12
            y: 8
          targets:
            - expr: sum(rate(synthesis_job_duration_seconds_bucket[1h])) by (le, language)
          yAxisUnit: s
        
        # Process Info
        - title: Process Information
          type: text
          gridPos:
            h: 4
            w: 24
            x: 0
            y: 16
          mode: markdown
          content: |
            ## Synthesis Process
            
            The synthesis process involves the following steps:
            
            1. **Parsing examples and constraints**
            2. **Selecting appropriate explanation level**
            3. **Building explanation trees**
            4. **Applying decomposition based on composability**
            5. **Synthesizing program structures**
            6. **Refining and optimizing the synthesized program**
        
        # Program Complexity by Language
        - title: Program Complexity by Language
          type: graph
          gridPos:
            h: 8
            w: 12
            x: 0
            y: 20
          targets:
            - expr: avg by (language) (rate(synthesis_program_complexity_sum[24h]) / rate(synthesis_program_complexity_count[24h]))
              legend: "{{language}}"
          unit: none
          yAxis:
            label: Average Complexity
        
        # Queue Wait Time
        - title: Queue Wait Time
          type: graph
          gridPos:
            h: 8
            w: 12
            x: 12
            y: 20
          targets:
            - expr: histogram_quantile(0.95, sum(rate(synthesis_queue_wait_time_seconds_bucket[1h])) by (le))
              legend: "95th Percentile"
            - expr: histogram_quantile(0.50, sum(rate(synthesis_queue_wait_time_seconds_bucket[1h])) by (le))
              legend: "Median"
          unit: s
          yAxis:
            label: Wait Time (seconds)
#+end_src

#+begin_src shell :tangle doc/future_work/run_monitoring.sh
#!/bin/bash
# Script to set up and run monitoring for the Reversible Meta-Synthesis service

# Set environment variables
export PYTHONPATH=/opt/reversible-meta-synthesis
export WORKER_ID=${HOSTNAME}

# Configuration files
MONITOR_CONFIG=/etc/synthesis/monitoring_config.yaml
ALERTS_CONFIG=/etc/synthesis/alerts_config.yaml
DASHBOARD_CONFIG=/etc/synthesis/dashboard_config.yaml

# Create log directory
mkdir -p /var/log/synthesis

# Function to start a component
start_component() {
    echo "Starting $1..."
    if pgrep -f "python3 $2" > /dev/null; then
        echo "$1 is already running"
    else
        python3 $2 --config $3 $4 >> /var/log/synthesis/$1.log 2>&1 &
        echo "$1 started with PID $!"
    fi
}

# Function to stop a component
stop_component() {
    echo "Stopping $1..."
    pkill -f "python3 $2" || true
    echo "$1 stopped"
}

# Function to check component status
check_component() {
    if pgrep -f "python3 $1" > /dev/null; then
        echo "$2 is running"
    else
        echo "$2 is not running"
    fi
}

# Function to display help
display_help() {
    echo "Usage: $0 [start|stop|restart|status] [all|monitor|alert|dashboard]"
    echo ""
    echo "Commands:"
    echo "  start       Start monitoring components"
    echo "  stop        Stop monitoring components"
    echo "  restart     Restart monitoring components"
    echo "  status      Check status of monitoring components"
    echo ""
    echo "Components:"
    echo "  all         All monitoring components"
    echo "  monitor     Service monitor"
    echo "  alert       Alert manager"
    echo "  dashboard   Dashboard generator"
    echo ""
    echo "Examples:"
    echo "  $0 start all             # Start all monitoring components"
    echo "  $0 stop monitor          # Stop the service monitor"
    echo "  $0 restart alert         # Restart the alert manager"
    echo "  $0 status                # Check status of all components"
}

# Main script logic
case $1 in
    start)
        case $2 in
            all|"")
                start_component "service-monitor" "/opt/reversible-meta-synthesis/doc/future_work/monitor_synthesis_service.py" "$MONITOR_CONFIG" "--port 8000"
                start_component "alert-manager" "/opt/reversible-meta-synthesis/doc/future_work/alert_manager.py" "$ALERTS_CONFIG"
                start_component "dashboard-generator" "/opt/reversible-meta-synthesis/doc/future_work/dashboard_generator.py" "$DASHBOARD_CONFIG" "--action generate"
                ;;
            monitor)
                start_component "service-monitor" "/opt/reversible-meta-synthesis/doc/future_work/monitor_synthesis_service.py" "$MONITOR_CONFIG" "--port 8000"
                ;;
            alert)
                start_component "alert-manager" "/opt/reversible-meta-synthesis/doc/future_work/alert_manager.py" "$ALERTS_CONFIG"
                ;;
            dashboard)
                start_component "dashboard-generator" "/opt/reversible-meta-synthesis/doc/future_work/dashboard_generator.py" "$DASHBOARD_CONFIG" "--action generate"
                ;;
            *)
                echo "Unknown component: $2"
                display_help
                exit 1
                ;;
        esac
        ;;
    
    stop)
        case $2 in
            all|"")
                stop_component "service-monitor" "/opt/reversible-meta-synthesis/doc/future_work/monitor_synthesis_service.py"
                stop_component "alert-manager" "/opt/reversible-meta-synthesis/doc/future_work/alert_manager.py"
                stop_component "dashboard-generator" "/opt/reversible-meta-synthesis/doc/future_work/dashboard_generator.py"
                ;;
            monitor)
                stop_component "service-monitor" "/opt/reversible-meta-synthesis/doc/future_work/monitor_synthesis_service.py"
                ;;
            alert)
                stop_component "alert-manager" "/opt/reversible-meta-synthesis/doc/future_work/alert_manager.py"
                ;;
            dashboard)
                stop_component "dashboard-generator" "/opt/reversible-meta-synthesis/doc/future_work/dashboard_generator.py"
                ;;
            *)
                echo "Unknown component: $2"
                display_help
                exit 1
                ;;
        esac
        ;;
    
    restart)
        case $2 in
            all|"")
                $0 stop all
                sleep 2
                $0 start all
                ;;
            monitor|alert|dashboard)
                $0 stop $2
                sleep 2
                $0 start $2
                ;;
            *)
                echo "Unknown component: $2"
                display_help
                exit 1
                ;;
        esac
        ;;
    
    status)
        echo "Checking monitoring component status..."
        check_component "/opt/reversible-meta-synthesis/doc/future_work/monitor_synthesis_service.py" "Service monitor"
        check_component "/opt/reversible-meta-synthesis/doc/future_work/alert_manager.py" "Alert manager"
        check_component "/opt/reversible-meta-synthesis/doc/future_work/dashboard_generator.py" "Dashboard generator"
        ;;
    
    *)
        display_help
        exit 1
        ;;
esac

exit 0
#+end_src

*** Multi-Region Deployment

Design a multi-region deployment architecture for high availability and disaster recovery.

#+begin_src yaml :tangle doc/future_work/multi_region_architecture.yaml
# Multi-Region Deployment Architecture for Reversible Meta-Synthesis Service

regions:
  - name: us-west
    primary: true
    zones:
      - us-west-1a
      - us-west-1b
      - us-west-1c
    resources:
      compute: 24
      memory: 96
      storage: 1000
  
  - name: eu-central
    primary: false
    zones:
      - eu-central-1a
      - eu-central-1b
      - eu-central-1c
    resources:
      compute: 16
      memory: 64
      storage: 500
  
  - name: ap-southeast
    primary: false
    zones:
      - ap-southeast-1a
      - ap-southeast-1b
      - ap-southeast-1c
    resources:
      compute: 16
      memory: 64
      storage: 500

global_services:
  - name: dns
    provider: route53
    configuration:
      domain: reversible-meta-synthesis.com
      routing_policy: latency
      health_checks:
        - endpoint: /api/v1/health
          interval: 30
          failure_threshold: 3
  
  - name: cdn
    provider: cloudfront
    configuration:
      origins:
        - region: us-west
          path: /static/*
        - region: eu-central
          path: /static/*
        - region: ap-southeast
          path: /static/*
      ttl: 86400
      compression: true
  
  - name: object_storage
    provider: s3
    configuration:
      buckets:
        - name: synthesis-results
          replication: cross-region
          versioning: enabled
          lifecycle:
            transition_to_ia: 30
            transition_to_glacier: 90
            expiration: 365

regional_services:
  - name: api_gateway
    configuration:
      path: /api/v1
      rate_limit: 1000
      timeout: 30
  
  - name: load_balancer
    configuration:
      type: application
      listeners:
        - port: 80
          protocol: HTTP
          redirect: HTTPS
        - port: 443
          protocol: HTTPS
          certificate: arn:aws:acm:us-west-1:123456789012:certificate/abcdef
      target_groups:
        - name: synthesis-service
          port: 8080
          health_check:
            path: /health
            interval: 15
            timeout: 5
            healthy_threshold: 2
            unhealthy_threshold: 3
  
  - name: kubernetes_cluster
    configuration:
      version: 1.28
      node_groups:
        - name: synthesis-workers
          instance_type: c5.2xlarge
          min_size: 3
          max_size: 20
          spot: false
        - name: synthesis-batch
          instance_type: c5.4xlarge
          min_size: 0
          max_size: 10
          spot: true
  
  - name: database
    configuration:
      engine: postgres
      version: 15.3
      instance_type: db.r6g.2xlarge
      multi_az: true
      storage: 200
      backups:
        automated:
          retention: 7
        snapshots:
          frequency: daily
          retention: 30
      replication:
        type: read_replica
        regions:
          - us-west
          - eu-central
          - ap-southeast

data_synchronization:
  - type: database
    strategy: active-passive
    replication:
      primary: us-west
      replicas:
        - eu-central
        - ap-southeast
      lag_limit: 10
  
  - type: object_storage
    strategy: active-active
    replication:
      enabled: true
      regions:
        - us-west
        - eu-central
        - ap-southeast
  
  - type: knowledge_base
    strategy: active-active
    replication:
      enabled: true
      frequency: 5
      regions:
        - us-west
        - eu-central
        - ap-southeast

deployment:
  strategy: blue-green
  stages:
    - name: us-west
      order: 1
      canary: true
      canary_percentage: 10
    - name: eu-central
      order: 2
      canary: false
    - name: ap-southeast
      order: 3
      canary: false
  rollback:
    automatic: true
    metrics:
      - name: error_rate
        threshold: 2.0
      - name: latency_p95
        threshold: 1000

disaster_recovery:
  rto: 60  # Recovery Time Objective (minutes)
  rpo: 15  # Recovery Point Objective (minutes)
  
  failover:
    automatic: true
    conditions:
      - type: region_outage
        detection:
          timeout: 300
          checks: 5
      - type: database_failure
        detection:
          timeout: 180
          checks: 3
  
  failback:
    automatic: false
    validation_steps:
      - name: data_integrity
        script: /scripts/verify_data_integrity.sh
      - name: service_health
        script: /scripts/verify_service_health.sh

cost_optimization:
  - strategy: scaling
    implementation:
      min_resources:
        compute: 6
        memory: 24
      max_resources:
        compute: 48
        memory: 192
      scaling_policy:
        metric: queue_size
        target: 10
  
  - strategy: spot_instances
    implementation:
      eligible_workloads:
        - batch_processing
        - non_critical_workers
      max_percentage: 70
  
  - strategy: reserved_instances
    implementation:
      commitment: 1
      coverage_target: 60
      instance_types:
        - c5.2xlarge
        - r6g.2xlarge

security:
  - component: network
    implementation:
      vpc_isolation: true
      traffic_flow:
        ingress:
          - source: public
            destination: load_balancer
            ports: [80, 443]
          - source: vpc
            destination: api_gateway
            ports: [8080]
          - source: vpc
            destination: database
            ports: [5432]
        egress:
          - source: api_gateway
            destination: synthesis_service
            ports: [8080]
          - source: synthesis_service
            destination: database
            ports: [5432]
  
  - component: encryption
    implementation:
